#!/usr/bin/env python3
"""
DailyNews Main Entry Point

å…¨è‡ªåŠ¨è¿è¡Œå…¥å£ï¼šæ¯å¤©æ™šä¸Š11ç‚¹é€šè¿‡ cron è°ƒç”¨ï¼Œå®Œæˆçˆ¬å–ã€æ€»ç»“ã€å‘å¸ƒå…¨æµç¨‹ã€‚

é»˜è®¤æ—¥æœŸç­–ç•¥ï¼š
- å…¬ä¼—å·æ–‡ç« ï¼šä»Šå¤©
- GitHub Trendingï¼šä»Šå¤©
- HuggingFace è®ºæ–‡ï¼šæ˜¨å¤©

Usage:
    python main.py [date]

    date: Optional date string in YYYY-MM-DD format
          å½“æŒ‡å®šæ—¥æœŸæ—¶ï¼šå…¬ä¼—å·å’Œ GitHub ä½¿ç”¨è¯¥æ—¥æœŸï¼Œè®ºæ–‡ä½¿ç”¨è¯¥æ—¥æœŸçš„å‰ä¸€å¤©
          ï¼ˆå› ä¸º HuggingFace è®ºæ–‡æ¦œå•åªæ˜¾ç¤º"æ˜¨å¤©"çš„è®ºæ–‡ï¼‰
"""
import sys
import argparse
from datetime import datetime, timedelta
from pathlib import Path

# Add project root to path
PROJECT_ROOT = Path(__file__).parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.fetchers import WechatFetcher, GithubTrendingFetcher, PapersFetcher
from src.summarizers import ArticleSummarizer, GithubSummarizer, PaperSummarizer, GeminiClient
from src.processors import LLMDeduplicator, MarkdownFormatter
from src.publishers import WechatPublisher
from src.utils.paper_ranker import PaperRanker
import config


def run_pipeline(date: str = None, dry_run: bool = False):
    """
    è¿è¡Œå®Œæ•´æµç¨‹ï¼šçˆ¬å– â†’ æ€»ç»“ â†’ æ¸…ç† â†’ æ ¼å¼åŒ– â†’ å‘å¸ƒ

    Args:
        date: ç›®æ ‡æ—¥æœŸ (YYYY-MM-DD)
              å½“ä¸º None æ—¶ï¼šå…¬ä¼—å·å’Œ GitHub ç”¨ä»Šå¤©ï¼Œè®ºæ–‡ç”¨æ˜¨å¤©
              å½“æŒ‡å®šæ—¥æœŸæ—¶ï¼šå…¬ä¼—å·å’Œ GitHub ç”¨è¯¥æ—¥æœŸï¼Œè®ºæ–‡ç”¨è¯¥æ—¥æœŸçš„å‰ä¸€å¤©
        dry_run: åªè¿è¡Œåˆ°æ ¼å¼åŒ–ï¼Œä¸å®é™…å‘å¸ƒ
    """
    # ç¡®å®šæ—¥æœŸ
    today = datetime.now().strftime('%Y-%m-%d')
    yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')

    if date is None:
        # ä½¿ç”¨æ··åˆæ—¥æœŸç­–ç•¥
        wechat_date = today
        github_date = today
        papers_date = yesterday
        output_date = today  # è¾“å‡ºç›®å½•ç”¨ä»Šå¤©
    else:
        # ç”¨æˆ·æŒ‡å®šæ—¥æœŸï¼šå…¬ä¼—å·å’Œ GitHub ç”¨æŒ‡å®šæ—¥æœŸï¼Œè®ºæ–‡ç”¨å‰ä¸€å¤©
        # ï¼ˆå› ä¸º HuggingFace è®ºæ–‡æ¦œå•åªæ˜¾ç¤º"æ˜¨å¤©"çš„è®ºæ–‡ï¼‰
        specified_date = datetime.strptime(date, '%Y-%m-%d')
        papers_date = (specified_date - timedelta(days=1)).strftime('%Y-%m-%d')
        wechat_date = github_date = output_date = date

    print("=" * 60)
    print(f"ğŸš€ DailyNews Pipeline - {output_date}")
    print(f"   å…¬ä¼—å·: {wechat_date} | GitHub: {github_date} | è®ºæ–‡: {papers_date}")
    print("=" * 60)

    # åˆå§‹åŒ–ç»„ä»¶
    client = GeminiClient()
    formatter = MarkdownFormatter()
    llm_deduplicator = LLMDeduplicator(client)
    publisher = WechatPublisher()

    # æ¥æºä¼˜å…ˆçº§æ˜ å°„
    priority_map = {"æ–°æ™ºå…ƒ": 3, "æœºå™¨ä¹‹å¿ƒ": 2, "é‡å­ä½": 1}

    # è¾“å‡ºç›®å½•
    output_dir = PROJECT_ROOT / "output" / output_date
    summaries_dir = PROJECT_ROOT / "data" / "summaries" / output_date
    papers_summaries_dir = PROJECT_ROOT / "data" / "summaries" / papers_date

    # ========== Phase 1: çˆ¬å– ==========
    print("\n" + "=" * 60)
    print("ğŸ“¡ Phase 1: Fetching")
    print("=" * 60)

    # 1.1 å…¬ä¼—å·æ–‡ç«  (é»˜è®¤ä»Šå¤©)
    print(f"\n[1/3] å…¬ä¼—å·æ–‡ç«  ({wechat_date})...")
    wechat_fetcher = WechatFetcher()
    try:
        articles = wechat_fetcher.fetch(wechat_date)
        print(f"  âœ… çˆ¬å–å®Œæˆ: {len(articles)} ç¯‡")
    except Exception as e:
        print(f"  âŒ çˆ¬å–å¤±è´¥: {e}")
        articles = []

    # 1.2 GitHub Trending (é»˜è®¤ä»Šå¤©)
    print(f"\n[2/3] GitHub Trending ({github_date})...")
    github_fetcher = GithubTrendingFetcher()
    try:
        repos = github_fetcher.fetch(github_date)
        # Save raw GitHub data
        if repos:
            github_fetcher.save_raw_data(repos, github_date)
            # Download README files
            github_fetcher.download_readmes(repos, date=github_date)
        print(f"  âœ… çˆ¬å–å®Œæˆ: {len(repos)} ä¸ªé¡¹ç›®")
    except Exception as e:
        print(f"  âŒ çˆ¬å–å¤±è´¥: {e}")
        repos = []

    # 1.3 è®ºæ–‡æ¦œå• (é»˜è®¤æ˜¨å¤©)
    print(f"\n[3/3] HuggingFace è®ºæ–‡ ({papers_date})...")
    papers_fetcher = PapersFetcher()
    try:
        papers = papers_fetcher.fetch(papers_date, max_papers=20)
        # Save raw papers data
        if papers:
            papers_fetcher.save_raw_data(papers, papers_date)
        print(f"  âœ… è·å–å®Œæˆ: {len(papers)} ç¯‡")
    except Exception as e:
        print(f"  âŒ è·å–å¤±è´¥: {e}")
        papers = []

    # ========== Phase 2: æ€»ç»“ ==========
    print("\n" + "=" * 60)
    print("ğŸ¤– Phase 2: Summarizing")
    print("=" * 60)

    summaries_dir.mkdir(parents=True, exist_ok=True)
    papers_summaries_dir.mkdir(parents=True, exist_ok=True)

    # 2.1 å…¬ä¼—å·æ–‡ç« æ€»ç»“
    article_summaries = []
    if not articles:
        # å¦‚æœå†…å­˜ä¸­æ²¡æœ‰æ•°æ®ï¼Œå°è¯•ä» JSON åŠ è½½
        print("\n[1/3] ä» JSON åŠ è½½å…¬ä¼—å·æ–‡ç« ...")
        articles = wechat_fetcher.load_from_json(wechat_date)

    if articles:
        print("\n[1/3] å…¬ä¼—å·æ–‡ç« æ€»ç»“...")
        article_summarizer = ArticleSummarizer(client)
        articles_json_path = summaries_dir / "articles.json"
        article_summaries = article_summarizer.summarize_batch(articles, delay=1.0, output_path=str(articles_json_path))

        # æ‰“åˆ†å’Œ is_ad ç°å·²é›†æˆåˆ° ArticleSummarizer ä¸­

    # 2.2 GitHub é¡¹ç›®æ€»ç»“
    github_summaries = []
    if repos:
        print("\n[2/3] GitHub é¡¹ç›®æ€»ç»“...")
        github_summarizer = GithubSummarizer(client, date=github_date)
        github_summaries = github_summarizer.summarize_batch(
            repos,
            delay=0.5,
            output_path=str(summaries_dir / "trending.json")
        )

    # 2.3 è®ºæ–‡æ€»ç»“ (ä¼˜å…ˆä»å†…å­˜ï¼Œå…¶æ¬¡ä» JSON åŠ è½½)
    print("\n[3/3] è®ºæ–‡æ¦œå•åŠ è½½...")
    if not papers:
        papers = papers_fetcher.load_from_json(papers_date)

    paper_summaries = []
    if papers:
        # ä½¿ç”¨ LLM ç”Ÿæˆä¸­æ–‡æ‘˜è¦
        paper_summarizer = PaperSummarizer(client)
        paper_summaries = paper_summarizer.summarize_batch_from_summary(
            papers,
            delay=1.0,
            output_path=str(papers_summaries_dir / "papers.json")
        )

        print(f"  âœ… è®ºæ–‡æ•°æ®å·²ä¿å­˜ ({len(paper_summaries)} ç¯‡)")

    # ========== Phase 3: LLM å»é‡ ==========
    print("\n" + "=" * 60)
    print("ğŸ” Phase 3: LLM Deduplication")
    print("=" * 60)

    # 3.1 å…¬ä¼—å·æ–‡ç«  LLM å»é‡
    print("\n[1/2] å…¬ä¼—å·æ–‡ç«  LLM å»é‡...")
    before_count = len(article_summaries)
    cleaned_articles = llm_deduplicator.deduplicate(article_summaries, output_path=str(articles_json_path))
    after_count = len(cleaned_articles)
    print(f"  âœ… å»é‡å®Œæˆ: {before_count} â†’ {after_count}")

    # 3.2 GitHub é¡¹ç›®é€šå¸¸ä¸éœ€è¦å»é‡
    cleaned_repos = github_summaries

    # ========== Phase 4: æ ¼å¼åŒ–ï¼ˆJSON â†’ Markdownï¼‰==========
    print("\n" + "=" * 60)
    print("ğŸ“ Phase 4: Formatting")
    print("=" * 60)

    output_dir.mkdir(parents=True, exist_ok=True)
    papers_output_dir = PROJECT_ROOT / "output" / papers_date
    papers_output_dir.mkdir(parents=True, exist_ok=True)

    # 4.1 å…¬ä¼—å·æ—¥æŠ¥
    if cleaned_articles:
        print("\n[1/3] å…¬ä¼—å·æ—¥æŠ¥...")
        daily_report = formatter.format_articles(cleaned_articles, output_date)
        formatter.save(daily_report, output_dir / "daily_report.md")

    # 4.2 GitHub Trending æŠ¥å‘Š
    if cleaned_repos:
        print("\n[2/3] GitHub Trending æŠ¥å‘Š...")
        trending_report = formatter.format_github(cleaned_repos, output_date)
        formatter.save(trending_report, output_dir / "github_trending.md")

    # 4.3 è®ºæ–‡æ±‡æ€»
    if paper_summaries:
        print("\n[3/3] è®ºæ–‡æ±‡æ€»...")
        papers_report = formatter.format_papers_summary(paper_summaries, papers_date)
        formatter.save(papers_report, papers_output_dir / "papers_summary.md")

    # ========== Phase 5: å‘å¸ƒ ==========
    if dry_run:
        print("\n" + "=" * 60)
        print("ğŸ” DRY RUN - è·³è¿‡å‘å¸ƒé˜¶æ®µ")
        print("=" * 60)
        print(f"\nğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        print(f"ğŸ“ è®ºæ–‡è¾“å‡ºç›®å½•: {papers_output_dir}")
        print(f"ğŸ“ æ€»ç»“ç›®å½•: {summaries_dir}")
        return

    print("\n" + "=" * 60)
    print("ğŸ“¤ Phase 5: Publishing")
    print("=" * 60)

    publish_errors = []

    # 5.1 å‘å¸ƒå…¬ä¼—å·æ—¥æŠ¥
    if cleaned_articles and (output_dir / "daily_report.md").exists():
        print("\n[1/3] å‘å¸ƒå…¬ä¼—å·æ—¥æŠ¥...")
        try:
            result = publisher.publish_daily_report(
                str(output_dir / "daily_report.md"),
                target_date=output_date
            )
            print(f"  âœ… è‰ç¨¿å·²åˆ›å»º: {result['draft_id']}")
        except Exception as e:
            print(f"  âŒ å‘å¸ƒå¤±è´¥: {e}")
            publish_errors.append(("daily_report", str(e)))

    # 5.2 å‘å¸ƒ GitHub Trending
    if cleaned_repos and (output_dir / "github_trending.md").exists():
        print("\n[2/3] å‘å¸ƒ GitHub Trending...")
        try:
            result = publisher.publish_github_trending(
                str(output_dir / "github_trending.md"),
                target_date=github_date
            )
            print(f"  âœ… è‰ç¨¿å·²åˆ›å»º: {result['draft_id']}")
        except Exception as e:
            print(f"  âŒ å‘å¸ƒå¤±è´¥: {e}")
            publish_errors.append(("github_trending", str(e)))

    # 5.3 å‘å¸ƒè®ºæ–‡æ±‡æ€»
    if paper_summaries and (papers_output_dir / "papers_summary.md").exists():
        print("\n[3/3] å‘å¸ƒè®ºæ–‡æ±‡æ€»...")
        try:
            result = publisher.publish_papers_summary(
                str(papers_output_dir / "papers_summary.md"),
                target_date=papers_date
            )
            print(f"  âœ… è‰ç¨¿å·²åˆ›å»º: {result['draft_id']}")
        except Exception as e:
            print(f"  âŒ å‘å¸ƒå¤±è´¥: {e}")
            publish_errors.append(("papers_summary", str(e)))

    # ========== å®Œæˆ ==========
    print("\n" + "=" * 60)
    print("âœ… Pipeline Completed!")
    print("=" * 60)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ“ æ€»ç»“ç›®å½•: {summaries_dir}")

    if publish_errors:
        print("\nâš ï¸ å‘å¸ƒé”™è¯¯:")
        for name, error in publish_errors:
            print(f"  - {name}: {error}")


def run_paper_analysis_pipeline(
    target_date: str = None,
    min_papers: int = 3,
    max_papers: int = 20,
    enable_topic_bonus: bool = False,
    dry_run: bool = False
):
    """
    è®ºæ–‡æ·±åº¦åˆ†ææµç¨‹ - è·å–ã€æ’åºã€ä¸‹è½½ã€åˆ†æ

    Args:
        target_date: ç›®æ ‡æ—¥æœŸ (YYYY-MM-DD)ï¼Œé»˜è®¤æ˜¨å¤©
        min_papers: æœ€å°‘ä¸‹è½½æ•°
        max_papers: æœ€å¤šä¸‹è½½æ•°
        enable_topic_bonus: æ˜¯å¦å¯ç”¨å…´è¶£åŠ æˆ
        dry_run: åªæ˜¾ç¤ºä¸å®é™…æ‰§è¡Œ
    """
    # é»˜è®¤æ˜¨å¤©
    if target_date is None:
        target_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')

    print("=" * 80)
    print(f"Paper Analysis Pipeline - {target_date}")
    print("=" * 80)

    # åˆå§‹åŒ–ç»„ä»¶
    client = GeminiClient()

    # 1. è·å–å¹¶æ’åºè®ºæ–‡
    print(f"\n[1/5] è·å–è®ºæ–‡åˆ—è¡¨...")
    import requests
    url = f"https://huggingface.co/api/daily_papers?date={target_date}"
    resp = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})

    if resp.status_code != 200:
        print(f"é”™è¯¯: API è¯·æ±‚å¤±è´¥ ({resp.status_code})")
        return

    papers = resp.json()
    if not papers:
        print(f"æ— æ•°æ®: {target_date}")
        return

    print(f"  è·å–åˆ° {len(papers)} ç¯‡è®ºæ–‡")

    # 2. æ’åº
    print(f"\n[2/5] è®ºæ–‡æ’åº...")
    ranker = PaperRanker(enable_topic_bonus=enable_topic_bonus)
    ranked = ranker.rank_papers(papers)

    # æ‰¾åˆ°æœ€åä¸€ç¯‡ Frontier Lab è®ºæ–‡çš„ä½ç½®
    last_frontier_idx = 0
    for i, p in enumerate(ranked):
        reasons = p.get("rank_reasons", "")
        if "Super Lab" in reasons or "Frontier Lab" in reasons:
            last_frontier_idx = i

    # ç»Ÿè®¡ Frontier Lab æ•°é‡
    frontier_count = sum(1 for p in ranked[:last_frontier_idx+1]
                          if "Super Lab" in p.get("rank_reasons", "") or "Frontier Lab" in p.get("rank_reasons", ""))

    # ç¡®å®šä¸‹è½½æ•°é‡: ä»ç¬¬1ç¯‡åˆ°æœ€åä¸€ç¯‡ Frontier Lab
    download_count = max(min_papers, last_frontier_idx + 1)
    download_count = min(download_count, max_papers)

    papers_to_download = ranked[:download_count]
    print(f"  å°†ä¸‹è½½: ç¬¬1ç¯‡ â†’ ç¬¬{last_frontier_idx+1}ç¯‡ (å…± {download_count} ç¯‡)")
    print(f"  å…¶ä¸­ Frontier Labs: {frontier_count} ç¯‡")

    # æ˜¾ç¤ºå°†è¦ä¸‹è½½çš„è®ºæ–‡åˆ—è¡¨
    print(f"\n  å°†åˆ†æçš„è®ºæ–‡:")
    for i, p in enumerate(papers_to_download, 1):
        title = p.get("title", "")[:55]
        score = p.get("rank_score", 0)
        reasons = p.get("rank_reasons", "")
        marker = "ğŸ”¥" if ("Super Lab" in reasons or "Frontier Lab" in reasons) else "  "
        print(f"    {i:2d}. [{marker}] {score:6.2f} | {title}... | {reasons}")

    if dry_run:
        print("\n[DRY RUN] è·³è¿‡å®é™…ä¸‹è½½å’Œåˆ†æ")
        return

    # 3. å‡†å¤‡ç›®å½•
    download_dir = PROJECT_ROOT / "data" / target_date / "papers" / "pdf_downloads"
    download_dir.mkdir(parents=True, exist_ok=True)

    output_dir = PROJECT_ROOT / "output" / target_date
    output_dir.mkdir(parents=True, exist_ok=True)

    # 4. ä¸‹è½½ PDF
    print(f"\n[3/5] ä¸‹è½½ PDF...")
    downloaded_files = []

    import re
    import time

    for i, p in enumerate(papers_to_download, 1):
        paper = p.get("paper", {})
        arxiv_id = paper.get("id", "")
        title = p.get("title", "")
        score = p.get("rank_score", 0)
        reasons = p.get("rank_reasons", "")

        print(f"\n  [{i}/{download_count}] Score: {score} | {title[:60]}...")
        print(f"     Tags: {reasons}")

        # ä¸‹è½½ PDF
        pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
        safe_title = re.sub(r'[\\/*?:"<>|]', "", title).strip()
        filename = f"{arxiv_id}_{safe_title[:80]}.pdf"
        file_path = download_dir / filename

        if file_path.exists():
            print(f"    [å·²å­˜åœ¨] {filename}")
        else:
            try:
                print(f"    [ä¸‹è½½ä¸­] {filename}...")
                r = requests.get(
                    pdf_url,
                    headers={"User-Agent": "Mozilla/5.0"},
                    stream=True,
                    timeout=60
                )
                if r.status_code == 200:
                    with open(file_path, 'wb') as f:
                        for chunk in r.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                    file_size = file_path.stat().st_size
                    print(f"    [å®Œæˆ] {file_size:,} bytes")
                    time.sleep(3)  # ArXiv é™åˆ¶
                else:
                    print(f"    [å¤±è´¥] HTTP {r.status_code}")
                    continue
            except Exception as e:
                print(f"    [é”™è¯¯] {e}")
                continue

        downloaded_files.append({
            "pdf_path": file_path,
            "arxiv_id": arxiv_id,
            "title": title,
            "org": p.get("organization", {}).get("fullname", ""),
            "stars": paper.get("githubStars", 0),
            "upvotes": paper.get("upvotes", 0),
            "score": score,
            "reasons": reasons
        })

    if not downloaded_files:
        print("  æ²¡æœ‰æˆåŠŸä¸‹è½½ä»»ä½•è®ºæ–‡")
        return

    # 5. åŠ è½½åˆ†ææç¤ºè¯
    prompt_path = PROJECT_ROOT / "prompt.md"
    if not prompt_path.exists():
        print("  âš ï¸  æœªæ‰¾åˆ° prompt.mdï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯")
        prompt = "è¯·ç”¨ä¸­æ–‡è¯¦ç»†åˆ†æè¿™ç¯‡è®ºæ–‡ï¼ŒåŒ…æ‹¬ï¼šæ ¸å¿ƒè´¡çŒ®ã€æ–¹æ³•è®ºã€åˆ›æ–°ç‚¹ã€å®éªŒç»“æœã€å±€é™æ€§ç­‰ã€‚"
    else:
        prompt = prompt_path.read_text(encoding='utf-8')

    # 6. åˆ†æè®ºæ–‡
    print(f"\n[4/5] åˆ†æè®ºæ–‡...")

    analysis_files = []

    for i, paper_info in enumerate(downloaded_files, 1):
        print(f"\n  è®ºæ–‡ {i}/{len(downloaded_files)}")

        try:
            result = client.upload_and_analyze(str(paper_info["pdf_path"]), prompt)

            # ä¿å­˜åˆ†æç»“æœ
            arxiv_id = paper_info.get("arxiv_id", "unknown")
            safe_title = re.sub(r'[\\/*?:"<>|]', "", paper_info.get('title', 'unknown')).strip()[:50]
            filename = f"{arxiv_id}_{safe_title}_analysis.md"
            output_path = output_dir / filename

            # æ„å»ºå®Œæ•´è¾“å‡º
            content = f"# {paper_info.get('title', 'Unknown')}\n\n"
            content += f"**arXiv ID**: {arxiv_id}\n"
            content += f"**ç»„ç»‡**: {paper_info.get('org', 'Unknown')}\n"
            content += f"**GitHub Stars**: {paper_info.get('stars', 0)}\n"
            content += f"**Upvotes**: {paper_info.get('upvotes', 0)}\n"
            content += f"**å¾—åˆ†**: {paper_info.get('score', 0)}\n"
            content += f"**æ ‡ç­¾**: {paper_info.get('reasons', 'N/A')}\n\n"
            content += "---\n\n"
            content += result

            output_path.write_text(content, encoding='utf-8')
            print(f"  [ä¿å­˜] {filename}")
            analysis_files.append(output_path)

        except Exception as e:
            print(f"  [é”™è¯¯] åˆ†æå¤±è´¥: {e}")

    # 7. æ±‡æ€»æŠ¥å‘Š
    print(f"\n[5/5] ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š...")
    summary_path = output_dir / "_summary.md"

    summary_content = f"# æ¯æ—¥è®ºæ–‡åˆ†ææŠ¥å‘Š - {target_date}\n\n"
    summary_content += f"## åˆ†ææ¦‚è§ˆ\n\n"
    summary_content += f"- **åˆ†ææ—¥æœŸ**: {target_date}\n"
    summary_content += f"- **è®ºæ–‡æ•°é‡**: {len(downloaded_files)}\n"
    summary_content += f"- **Frontier Labs**: {frontier_count}\n\n"
    summary_content += f"## è®ºæ–‡åˆ—è¡¨\n\n"

    for i, paper_info in enumerate(downloaded_files, 1):
        summary_content += f"{i}. **{paper_info['title'][:70]}...**\n"
        summary_content += f"   - ç»„ç»‡: {paper_info['org']}\n"
        summary_content += f"   - å¾—åˆ†: {paper_info['score']}\n"
        summary_content += f"   - æ ‡ç­¾: {paper_info['reasons']}\n"
        arxiv_id = paper_info['arxiv_id']
        summary_content += f"   - åˆ†æ: [{arxiv_id}_analysis.md]({arxiv_id}_analysis.md)\n\n"

    summary_path.write_text(summary_content, encoding='utf-8')

    print("\n" + "=" * 80)
    print("Pipeline å®Œæˆ!")
    print(f"  ä¸‹è½½ç›®å½•: {download_dir}")
    print(f"  è¾“å‡ºç›®å½•: {output_dir}")
    print(f"  åˆ†ææ–‡ä»¶: {len(analysis_files)} ç¯‡")
    print("=" * 80)

    return {
        'download_dir': str(download_dir),
        'output_dir': str(output_dir),
        'count': len(analysis_files)
    }


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(
        description="DailyNews å…¨è‡ªåŠ¨æµç¨‹",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python main.py                      # è¿è¡Œé»˜è®¤ç­–ç•¥ï¼ˆå…¬ä¼—å·/GitHubä»Šå¤©ï¼Œè®ºæ–‡æ˜¨å¤©ï¼‰
  python main.py 2026-02-01           # æ‰€æœ‰æ¨¡å—ç»Ÿä¸€ä½¿ç”¨æŒ‡å®šæ—¥æœŸ
  python main.py --dry-run            # åªè¿è¡Œåˆ°æ ¼å¼åŒ–ï¼Œä¸å‘å¸ƒ
  python main.py --fetch-only         # åªçˆ¬å–æ•°æ®ï¼ˆä½¿ç”¨é»˜è®¤æ—¥æœŸç­–ç•¥ï¼‰
  python main.py --summarize-only     # åªæ€»ç»“æ•°æ®ï¼ˆä» JSON åŠ è½½å·²çˆ¬å–çš„æ•°æ®ï¼‰
  python main.py --analyze-papers     # è¿è¡Œè®ºæ–‡æ·±åº¦åˆ†ææµç¨‹
  python main.py --analyze-papers --date 2026-01-30 --min-papers 5 --max-papers 15
        """
    )

    parser.add_argument(
        'date',
        nargs='?',
        help='ç›®æ ‡æ—¥æœŸ (YYYY-MM-DD)ï¼Œé»˜è®¤ä½¿ç”¨æ··åˆç­–ç•¥ï¼ˆå…¬ä¼—å·/GitHubä»Šå¤©ï¼Œè®ºæ–‡æ˜¨å¤©ï¼‰'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='åªè¿è¡Œåˆ°æ ¼å¼åŒ–é˜¶æ®µï¼Œä¸å®é™…å‘å¸ƒ'
    )
    parser.add_argument(
        '--fetch-only',
        action='store_true',
        help='åªçˆ¬å–æ•°æ®ï¼Œä¸è¿›è¡Œæ€»ç»“å’Œå‘å¸ƒ'
    )
    parser.add_argument(
        '--summarize-only',
        action='store_true',
        help='åªæ€»ç»“æ•°æ®ï¼ˆä» JSON åŠ è½½å·²çˆ¬å–çš„æ•°æ®ï¼‰ï¼Œä¸çˆ¬å–å’Œå‘å¸ƒ'
    )

    # è®ºæ–‡æ·±åº¦åˆ†æé€‰é¡¹
    parser.add_argument(
        '--analyze-papers',
        action='store_true',
        help='è¿è¡Œè®ºæ–‡æ·±åº¦åˆ†ææµç¨‹ï¼ˆä¸‹è½½PDF + Geminiåˆ†æï¼‰'
    )
    parser.add_argument(
        '--min-papers',
        type=int,
        default=3,
        help='æœ€å°‘ä¸‹è½½è®ºæ–‡æ•° (é»˜è®¤3ï¼Œä»…åœ¨--analyze-papersæ—¶æœ‰æ•ˆ)'
    )
    parser.add_argument(
        '--max-papers',
        type=int,
        default=20,
        help='æœ€å¤šä¸‹è½½è®ºæ–‡æ•° (é»˜è®¤20ï¼Œä»…åœ¨--analyze-papersæ—¶æœ‰æ•ˆ)'
    )
    parser.add_argument(
        '--topic-bonus',
        action='store_true',
        help='å¯ç”¨è®ºæ–‡å…´è¶£åŠ æˆ (ä»…åœ¨--analyze-papersæ—¶æœ‰æ•ˆ)'
    )
    parser.add_argument(
        '--publish-papers',
        action='store_true',
        help='åˆ†æå®Œæˆåå°†æ¯ç¯‡è®ºæ–‡å‘å¸ƒä¸ºç‹¬ç«‹è‰ç¨¿ (éœ€ä¸--analyze-papersä¸€èµ·ä½¿ç”¨)'
    )

    args = parser.parse_args()

    try:
        # è®ºæ–‡æ·±åº¦åˆ†ææ¨¡å¼
        if args.analyze_papers:
            result = run_paper_analysis_pipeline(
                target_date=args.date,
                min_papers=args.min_papers,
                max_papers=args.max_papers,
                enable_topic_bonus=args.topic_bonus,
                dry_run=args.dry_run
            )

            # å¦‚æœéœ€è¦å‘å¸ƒè®ºæ–‡
            if not args.dry_run and args.publish_papers and result:
                print("\n" + "=" * 60)
                print("ğŸ“¤ å‘å¸ƒè®ºæ–‡åˆ†æ...")
                print("=" * 60)
                publisher = WechatPublisher()
                date_str = args.date or (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
                results = publisher.publish_all_papers(date_str)
                success_count = sum(1 for r in results if r.get('status') == 'success')
                print(f"\nâœ… å®Œæˆ: {success_count}/{len(results)} ç¯‡è®ºæ–‡å‘å¸ƒæˆåŠŸ")
            return

        if args.summarize_only:
            # åªè¿è¡Œæ€»ç»“é˜¶æ®µï¼ˆä» JSON åŠ è½½æ•°æ®ï¼‰
            today = datetime.now().strftime('%Y-%m-%d')
            yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')

            wechat_date = github_date = today
            papers_date = yesterday

            # å¦‚æœç”¨æˆ·æŒ‡å®šäº†æ—¥æœŸ
            if args.date:
                specified_date = datetime.strptime(args.date, '%Y-%m-%d')
                papers_date = (specified_date - timedelta(days=1)).strftime('%Y-%m-%d')
                wechat_date = github_date = args.date

            print(f"ğŸ¤– åªæ€»ç»“æ•°æ®")
            print(f"   å…¬ä¼—å·: {wechat_date} | GitHub: {github_date} | è®ºæ–‡: {papers_date}")

            # åˆå§‹åŒ–
            client = GeminiClient()
            formatter = MarkdownFormatter()
            llm_deduplicator = LLMDeduplicator(client)

            priority_map = {"æ–°æ™ºå…ƒ": 3, "æœºå™¨ä¹‹å¿ƒ": 2, "é‡å­ä½": 1}
            output_dir = PROJECT_ROOT / "output" / wechat_date
            summaries_dir = PROJECT_ROOT / "data" / "summaries" / wechat_date
            papers_summaries_dir = PROJECT_ROOT / "data" / "summaries" / papers_date
            summaries_dir.mkdir(parents=True, exist_ok=True)
            papers_summaries_dir.mkdir(parents=True, exist_ok=True)

            # 1. å…¬ä¼—å·æ–‡ç« 
            wechat_fetcher = WechatFetcher()
            articles = wechat_fetcher.load_from_json(wechat_date)

            article_summaries = []
            if articles:
                article_summarizer = ArticleSummarizer(client)
                articles_json_path = summaries_dir / "articles.json"
                article_summaries = article_summarizer.summarize_batch(articles, delay=1.0, output_path=str(articles_json_path))

                # æ‰“åˆ†å’Œ is_ad ç°å·²é›†æˆåˆ° ArticleSummarizer ä¸­

            # 2. GitHub Trending
            github_fetcher = GithubTrendingFetcher()
            repos = github_fetcher._load_from_json(github_date)

            github_summaries = []
            if repos:
                github_summarizer = GithubSummarizer(client, date=github_date)
                github_summaries = github_summarizer.summarize_batch(
                    repos,
                    delay=0.5,
                    output_path=str(summaries_dir / "trending.json")
                )

            # 3. è®ºæ–‡ - åŠ è½½æ‰€æœ‰è®ºæ–‡ç”¨äºæ¯æ—¥æŠ¥å‘Š
            papers_fetcher = PapersFetcher()
            papers = papers_fetcher.load_from_json(papers_date)

            paper_summaries = []
            if papers:
                # ä½¿ç”¨ LLM ç”Ÿæˆä¸­æ–‡æ‘˜è¦
                paper_summarizer = PaperSummarizer(client)
                paper_summaries = paper_summarizer.summarize_batch_from_summary(
                    papers,
                    delay=1.0,
                    output_path=str(papers_summaries_dir / "papers.json")
                )

            # 4. LLM å»é‡
            cleaned_articles = llm_deduplicator.deduplicate(article_summaries, output_path=str(articles_json_path))

            # 5. æ ¼å¼åŒ–è¾“å‡º
            output_dir.mkdir(parents=True, exist_ok=True)
            papers_output_dir = PROJECT_ROOT / "output" / papers_date
            papers_output_dir.mkdir(parents=True, exist_ok=True)

            if cleaned_articles:
                daily_report = formatter.format_articles(cleaned_articles, wechat_date)
                formatter.save(daily_report, output_dir / "daily_report.md")

            if github_summaries:
                trending_report = formatter.format_github(github_summaries, wechat_date)
                formatter.save(trending_report, output_dir / "github_trending.md")

            if paper_summaries:
                papers_report = formatter.format_papers_summary(paper_summaries, papers_date)
                formatter.save(papers_report, papers_output_dir / "papers_summary.md")

            print("\nâœ… æ€»ç»“å®Œæˆ")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
            return

        if args.fetch_only:
            # åªè¿è¡Œçˆ¬å–é˜¶æ®µï¼ˆä½¿ç”¨æ··åˆæ—¥æœŸç­–ç•¥ï¼‰
            today = datetime.now().strftime('%Y-%m-%d')
            yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')

            wechat_date = github_date = today
            papers_date = yesterday

            # å¦‚æœç”¨æˆ·æŒ‡å®šäº†æ—¥æœŸï¼Œå…¬ä¼—å·å’Œ GitHub ç”¨è¯¥æ—¥æœŸï¼Œè®ºæ–‡ç”¨å‰ä¸€å¤©
            if args.date:
                specified_date = datetime.strptime(args.date, '%Y-%m-%d')
                papers_date = (specified_date - timedelta(days=1)).strftime('%Y-%m-%d')
                wechat_date = github_date = args.date

            print(f"ğŸ“¡ åªçˆ¬å–æ•°æ®")
            print(f"   å…¬ä¼—å·: {wechat_date} | GitHub: {github_date} | è®ºæ–‡: {papers_date}")

            wechat_fetcher = WechatFetcher()
            articles = wechat_fetcher.fetch(wechat_date)
            if articles:
                wechat_fetcher.save_raw_data(articles, wechat_date)

            github_fetcher = GithubTrendingFetcher()
            repos = github_fetcher.fetch(github_date)
            if repos:
                github_fetcher.save_raw_data(repos, github_date)
                # ä¸‹è½½ README
                github_fetcher.download_readmes(repos, date=github_date)

            papers_fetcher = PapersFetcher()
            papers = papers_fetcher.fetch(papers_date)
            if papers:
                # å…ˆä¸‹è½½ PDFï¼ˆè®© download_pdfs ä½¿ç”¨è‡ªå·±çš„è£å‰ªé€»è¾‘ï¼‰
                papers_fetcher.download_pdfs(papers, date=papers_date)
                # è·å–å®é™…ä¸‹è½½çš„è®ºæ–‡æ•°é‡ï¼Œç”¨äºä¿å­˜ JSON
                last_frontier_idx = 0
                for i, p in enumerate(papers):
                    reasons = p.get("rank_reasons", "")
                    if "Super Lab" in reasons or "Frontier Lab" in reasons:
                        last_frontier_idx = i
                # ä½¿ç”¨ä¸ download_pdfs ç›¸åŒçš„è£å‰ªé€»è¾‘
                download_count = max(3, last_frontier_idx + 1)
                download_count = min(download_count, 12)
                papers_to_save = papers[:download_count]
                papers_fetcher.save_raw_data(papers_to_save, papers_date)

            print("\nâœ… çˆ¬å–å®Œæˆ")

        else:
            run_pipeline(date=args.date, dry_run=args.dry_run)

    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
