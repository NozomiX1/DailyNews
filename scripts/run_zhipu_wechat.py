#!/usr/bin/env python3
"""
ä½¿ç”¨ Zhipu GLM 4.7 Flash è¿è¡Œå®Œæ•´çš„å…¬ä¼—å·æ–‡ç« å¤„ç†æµç¨‹

æµç¨‹:
1. çˆ¬å–/åŠ è½½æ–‡ç« 
2. LLM æ€»ç»“
3. LLM åŽ»é‡
4. ç”Ÿæˆ Markdown æŠ¥å‘Š

Usage:
    python scripts/run_zhipu_wechat.py [--date 2026-02-18]
"""
import sys
import os
import json
import argparse
from pathlib import Path
from datetime import datetime

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.summarizers.zhipu_client import ZhipuClient
from src.summarizers.article_summarizer import ArticleSummarizer
from src.processors.llm_deduplicator import LLMDeduplicator
from src.processors.formatter import MarkdownFormatter

# é…ç½®
ZHIPU_MODEL = "glm-4.7"  # ä½¿ç”¨æ–°çš„ GLM-5 æ¨¡åž‹
MAX_TOKENS = 65536
DELAY_BETWEEN_REQUESTS = 3.0  # GLM API è¯·æ±‚é—´éš”


def load_articles_from_backup(date: str) -> list:
    """ä»Žå¤‡ä»½çš„ daily_report.md è§£æžæ–‡ç« ï¼ˆä½œä¸ºåŽå¤‡æ–¹æ¡ˆï¼‰"""
    # å°è¯•å¤šä¸ªå¯èƒ½çš„å¤‡ä»½è·¯å¾„
    possible_paths = [
        PROJECT_ROOT / "output" / f"{date}-backup" / "daily_report.md",
        PROJECT_ROOT / "output" / date / "daily_report.md",
    ]

    backup_path = None
    for path in possible_paths:
        if path.exists():
            backup_path = path
            break

    if not backup_path:
        print(f"âŒ æ‰¾ä¸åˆ°å¤‡ä»½æ–‡ä»¶ï¼Œå°è¯•è¿‡: {possible_paths}")
        return []

    print(f"  ðŸ“‚ ä½¿ç”¨å¤‡ä»½: {backup_path}")

    with open(backup_path, "r", encoding="utf-8") as f:
        content = f.read()

    articles = []
    sections = content.split("---")

    for section in sections:
        if not section.strip():
            continue

        lines = section.strip().split("\n")
        if not lines:
            continue

        # Find title line (starts with ###)
        title_line = None
        for line in lines:
            if line.startswith("### "):
                # Remove number prefix like "1. "
                title_line = line[4:].strip()
                if title_line[0].isdigit() and ". " in title_line[:4]:
                    title_line = title_line.split(". ", 1)[1]
                break

        if not title_line:
            continue

        # Extract metadata
        source = ""
        url = ""
        time_str = ""
        tags = []
        score = 3

        for line in lines:
            if line.startswith("**æ¥æº**:"):
                source_raw = line.replace("**æ¥æº**:", "").strip()
                # Split by | to get source and time
                if " | " in source_raw:
                    parts = source_raw.split(" | ")
                    source = parts[0].strip()
                    for part in parts[1:]:
                        if "**æ—¶é—´**:" in part or "æ—¶é—´:" in part:
                            time_str = part.replace("**æ—¶é—´**:", "").replace("æ—¶é—´:", "").strip()
                else:
                    source = source_raw
            elif line.startswith("**é“¾æŽ¥**:"):
                url = line.replace("**é“¾æŽ¥**:", "").strip()
            elif line.startswith("**æ ‡ç­¾**:"):
                tags_str = line.replace("**æ ‡ç­¾**:", "").strip()
                # Parse tags like [tag1] [tag2]
                import re
                tags = re.findall(r'\[([^\]]+)\]', tags_str)
            elif line.startswith("**ä»·å€¼**:"):
                # Count stars
                score = line.count("ðŸŒŸ")

        # Get summary content (from blockquote onwards)
        summary_start = False
        summary_lines = []
        for line in lines:
            if line.startswith("> "):
                summary_start = True
            if summary_start:
                summary_lines.append(line)

        summary_content = "\n".join(summary_lines)

        articles.append({
            "title": title_line,
            "source": source,
            "url": url,
            "time_str": time_str,
            "timestamp": 0,
            "tags": tags,
            "score": score,
            "content": summary_content,  # Use existing summary as content
        })

    return articles


def fetch_fresh_articles(date: str) -> list:
    """å°è¯•çˆ¬å–æ–°é²œæ–‡ç« """
    try:
        from src.fetchers.wechat import WechatFetcher
        import config

        fetcher = WechatFetcher(data_dir=PROJECT_ROOT / "data")
        articles = fetcher.fetch(date)
        return articles
    except Exception as e:
        print(f"âš ï¸ çˆ¬å–å¤±è´¥: {e}")
        print("   å°†ä½¿ç”¨å¤‡ä»½æ•°æ®...")
        return []


def summarize_articles(client: ZhipuClient, articles: list) -> list:
    """ä½¿ç”¨ Zhipu æ€»ç»“æ–‡ç« """
    print(f"\n[2/4] ä½¿ç”¨ GLM 4.7 Flash æ€»ç»“æ–‡ç« ...")

    summarizer = ArticleSummarizer(client)
    summaries = []

    total = len(articles)
    for i, article in enumerate(articles, 1):
        title = article.get('title', article.get('original_title', ''))
        print(f"  [{i}/{total}] {title[:40]}...")

        content = article.get('content', '')
        if not content or len(content) < 100:
            print(f"      âš ï¸ å†…å®¹è¿‡çŸ­ï¼Œè·³è¿‡")
            continue

        try:
            result = summarizer.summarize(content, article)
            summaries.append(result)
            score = result.get('score', 0)
            stars = 'ðŸŒŸ' * score if score > 0 else 'N/A'
            print(f"      âœ… æ ‡ç­¾: {result.get('tags')} | è¯„åˆ†: {stars}")
        except Exception as e:
            print(f"      âŒ æ€»ç»“å¤±è´¥: {e}")
            # Add fallback
            summaries.append({
                'title': title[:50] + '...' if len(title) > 50 else title,
                'tags': ['æœªåˆ†ç±»'],
                'summary': f"æ€»ç»“å¤±è´¥: {str(e)}",
                'score': 1,
                'is_ad': False,
                'source': article.get('source', ''),
                'url': article.get('url', ''),
                'original_title': title,
            })

        # Rate limiting
        if i < total:
            import time
            time.sleep(DELAY_BETWEEN_REQUESTS)

    print(f"  âœ… æ€»ç»“å®Œæˆï¼Œå…± {len(summaries)} ç¯‡")
    return summaries


def deduplicate_articles(client: ZhipuClient, summaries: list) -> list:
    """ä½¿ç”¨ LLM åŽ»é‡"""
    print(f"\n[3/4] LLM åŽ»é‡...")

    deduplicator = LLMDeduplicator(client)
    before_count = len(summaries)

    try:
        cleaned = deduplicator.deduplicate(summaries)
        after_count = len(cleaned)
        print(f"  âœ… åŽ»é‡å®Œæˆ: {before_count} â†’ {after_count}")
        return cleaned
    except Exception as e:
        print(f"  âš ï¸ åŽ»é‡å¤±è´¥: {e}ï¼Œè¿”å›žåŽŸå§‹æ•°æ®")
        return summaries


def generate_report(summaries: list, date: str) -> str:
    """ç”Ÿæˆ Markdown æŠ¥å‘Š"""
    print(f"\n[4/4] ç”Ÿæˆ Markdown æŠ¥å‘Š...")

    formatter = MarkdownFormatter()
    content = formatter.format_articles(summaries, date)

    # Save to file
    output_dir = PROJECT_ROOT / "test_output" / f"{date}-zhipu"
    output_dir.mkdir(parents=True, exist_ok=True)
    output_path = output_dir / "daily_report.md"

    with open(output_path, "w", encoding="utf-8") as f:
        f.write(content)

    print(f"  ðŸ’¾ å·²ä¿å­˜: {output_path}")

    # Also save JSON
    json_path = output_dir / "articles.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(summaries, f, ensure_ascii=False, indent=2)
    print(f"  ðŸ’¾ å·²ä¿å­˜: {json_path}")

    return content


def main():
    parser = argparse.ArgumentParser(description="ä½¿ç”¨ Zhipu GLM è¿è¡Œå…¬ä¼—å·æ–‡ç« æµç¨‹")
    parser.add_argument("--date", default="2026-02-18", help="ç›®æ ‡æ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--use-backup", action="store_true", help="ç›´æŽ¥ä½¿ç”¨å¤‡ä»½æ•°æ®ï¼Œä¸çˆ¬å–")
    args = parser.parse_args()

    date = args.date

    print("=" * 60)
    print(f"ðŸ§ª Zhipu GLM-5 å…¬ä¼—å·æ–‡ç« å¤„ç†æµ‹è¯•")
    print(f"ðŸ“… æ—¥æœŸ: {date}")
    print("=" * 60)

    # Initialize Zhipu client
    print("\nðŸ“¦ åˆå§‹åŒ– ZhipuClient...")
    print(f"   æ¨¡åž‹: {ZHIPU_MODEL}")
    print(f"   Temperature: 1.0")
    print(f"   Max Tokens: {MAX_TOKENS}")

    try:
        client = ZhipuClient(
            model=ZHIPU_MODEL,
            max_tokens=MAX_TOKENS,
            enable_thinking=False,
        )
        print("âœ… ZhipuClient åˆå§‹åŒ–æˆåŠŸ")
    except ValueError as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        print("ðŸ’¡ è¯·è®¾ç½® ZHIPU_API_KEY çŽ¯å¢ƒå˜é‡")
        return

    # Step 1: èŽ·å–æ–‡ç« 
    print(f"\n[1/4] èŽ·å–æ–‡ç« ...")

    if args.use_backup:
        articles = load_articles_from_backup(date)
    else:
        # Try fresh fetch first, fallback to backup
        articles = fetch_fresh_articles(date)
        if not articles:
            articles = load_articles_from_backup(date)

    if not articles:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æ–‡ç« ")
        return

    print(f"âœ… æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")

    # Step 2: Summarize
    summaries = summarize_articles(client, articles)

    # Step 3: Deduplicate
    cleaned = deduplicate_articles(client, summaries)

    # Step 4: Generate report
    content = generate_report(cleaned, date)

    print("\n" + "=" * 60)
    print("âœ… å¤„ç†å®Œæˆï¼")
    print("=" * 60)

    # Print preview
    print(f"\nðŸ“„ æŠ¥å‘Šé¢„è§ˆ (å‰500å­—ç¬¦):")
    print("-" * 50)
    print(content[:500] + "..." if len(content) > 500 else content)
    print("-" * 50)


if __name__ == "__main__":
    main()
