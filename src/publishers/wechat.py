# DailyNews - WeChat Publisher
# Merged and refactored from wechat_publisher.py, github_publisher.py, paper_publisher.py
import requests
import json
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List

import sys
sys.path.insert(0, str(Path(__file__).parent.parent.parent))
import config

from .base import BasePublisher


class WechatPublisher(BasePublisher):
    """
    WeChat Official Account publisher.

    Publishes daily reports, GitHub trending, and paper analyses to WeChat drafts.
    """

    def __init__(self, app_id=None, app_secret=None):
        super().__init__()
        self.app_id = app_id or config.APP_ID
        self.app_secret = app_secret or config.APP_SECRET
        self.token = self._get_access_token()

    def _get_access_token(self):
        """è·å– access_token"""
        url = f"https://api.weixin.qq.com/cgi-bin/token?grant_type=client_credential&appid={self.app_id}&secret={self.app_secret}"
        try:
            resp = requests.get(url, proxies=getattr(config, 'PROXIES', None)).json()
            if 'access_token' in resp:
                return resp['access_token']
            else:
                raise Exception(f"è·å– access_token å¤±è´¥: {resp}")
        except Exception as e:
            raise Exception(f"è·å– access_token å¼‚å¸¸: {e}")

    # ================= Text Width Calculation =================

    def _calc_text_width(self, text):
        """è®¡ç®—æ–‡æœ¬å®½åº¦ï¼ˆä»¥ 1/3 å•ä½ä¸ºåŸºå‡†ï¼‰"""
        width = 0
        for char in text:
            if char in 'ğŸŒŸâ­ğŸ“ğŸ•’ğŸ·ï¸ğŸ”¥ğŸ’»ğŸ“„ğŸ”¬ğŸ“ŠğŸ”—':
                width += 4
            elif ord(char) > 127:
                width += 3
            else:
                width += 1
        return width

    # ================= HTML Generation =================

    def _generate_meta_row(self, item: Dict, item_type: str = 'article') -> str:
        """Generate metadata HTML row for an item."""
        if item_type == 'article':
            prefix1 = f'ğŸ“ æ¥æºï¼š{item["source"]}'
            prefix2 = f'â­ ä»·å€¼ï¼š{item["rating"]}' if item.get("rating") else ''

            width1 = self._calc_text_width(prefix1)
            width2 = self._calc_text_width(prefix2) if prefix2 else 0

            if width1 < width2:
                prefix1 += ' ' * (width2 - width1)
            elif width2 < width1:
                prefix2 += ' ' * (width1 - width2)

            meta_first_line = f'{prefix1}ã€€ğŸ•’ {item["time"]}'

            meta_second_parts = []
            if item.get("rating"):
                meta_second_parts.append(prefix2)
            if item.get("tag"):
                meta_second_parts.append(f'ã€€ğŸ·ï¸ æ ‡ç­¾ï¼š{item["tag"]}')

            meta_second_line = ''
            if meta_second_parts:
                meta_second_line = '<br>' + ''.join(meta_second_parts)

            return f'{meta_first_line}{meta_second_line}<br><span style="display: inline-block; margin-top: 4px; color: #576b95; word-break: break-all;">ğŸ”— é“¾æ¥ï¼š{item["url"]}</span>'

        elif item_type == 'github':
            parts = []
            if item.get('language'):
                parts.append(f'ğŸ’» è¯­è¨€ï¼š{item["language"]}')
            if item.get('stars'):
                parts.append(f'â­ Starsï¼š{item["stars"]}')
            if item.get('today_stars'):
                parts.append(f'ğŸ”¥ ä»Šæ—¥ï¼š+{item["today_stars"]}')

            meta_line = f'{"ã€€".join(parts)}<br><span style="display: inline-block; margin-top: 4px; color: #576b95; word-break: break-all;">ğŸ”— é“¾æ¥ï¼š{item["url"]}</span>'

            # Add tech stack below the link (without bold)
            if item.get('tech_stack'):
                meta_line += f'<br><span style="display: inline-block; margin-top: 4px; color: #666; font-size: 13px;">æŠ€æœ¯æ ˆ: {item["tech_stack"]}</span>'

            return meta_line

        elif item_type == 'paper':
            parts = []
            if item.get('score'):
                parts.append(f'ğŸ“Š å¾—åˆ†ï¼š{item["score"]}')
            if item.get('upvotes'):
                parts.append(f'ğŸ‘ {item["upvotes"]}')
            if item.get('stars'):
                parts.append(f'â­ {item["stars"]}')

            meta_line = 'ã€€'.join(parts) if parts else ''
            return f'{meta_line}<br><span style="display: inline-block; margin-top: 4px; color: #576b95; word-break: break-all;">ğŸ”— é“¾æ¥ï¼š{item["url"]}</span>'

        return ''

    def generate_html(self, items: List[Dict], item_type: str = 'article') -> str:
        """
        ä»æ–°é—»åˆ—è¡¨ç”Ÿæˆ HTML

        Args:
            items: News items list
            item_type: Type of items ('article', 'github', 'paper')

        Returns:
            HTML string
        """
        html_parts = ['<section style="font-family: -apple-system, BlinkMacSystemFont, Arial, sans-serif;">']
        html_parts.append('<section style="margin-top: 20px;"></section>')

        for idx, item in enumerate(items, 1):
            title = item.get('title', item.get('original_title', ''))

            title_html = f'<h3 style="margin-top: 30px; margin-bottom: 5px; font-size: 18px; font-weight: bold; color: #000;">{idx}. {title}</h3>'

            meta_html = f'<div style="font-size: 13px; color: #888; margin-bottom: 10px; background: #f9f9f9; padding: 8px; border-radius: 4px;">{self._generate_meta_row(item, item_type)}</div>'

            summary = item.get('summary', '')
            summary_text = summary.replace("\n", "<br>")
            summary_html = f'<p style="font-size: 16px; color: #333; line-height: 1.6; text-align: justify; margin-bottom: 25px;">{summary_text}</p>'

            # Add highlights section for papers
            highlights_html = ''
            if item_type == 'paper' and item.get('highlights'):
                highlights_html = f'<div style="font-size: 14px; color: #666; background: #f0f7ff; padding: 10px; border-radius: 4px; margin-bottom: 25px; border-left: 3px solid #3498db;"><strong>âœ¨ äº®ç‚¹:</strong><br>{item["highlights"]}</div>'

            divider = '<hr style="border: 0; border-top: 1px dashed #ddd; margin: 20px 0;" />' if idx < len(items) else ""
            html_parts.append(title_html + meta_html + summary_html + highlights_html + divider)

        html_parts.append("</section>")
        return "".join(html_parts)

    # ================= Parsing =================

    def _parse_daily_report(self, report_path) -> List[Dict]:
        """è§£æ daily_report.md æ–‡ä»¶"""
        with open(report_path, "r", encoding="utf-8") as f:
            content = f.read()

        news_items = []

        header_end = content.find('### 1.')
        if header_end != -1:
            content = content[header_end + 6:]

        articles = re.split(r'\n###\s+\d+\.\s+', content)

        for article in articles:
            if not article.strip():
                continue

            title_match = re.search(r'^(.+?)\n', article)
            title = title_match.group(1).strip() if title_match else "æ— æ ‡é¢˜"

            source_match = re.search(r'\*\*æ¥æº\*\*: (.+?) \|', article)
            source = source_match.group(1).strip() if source_match else "æœªçŸ¥"

            time_match = re.search(r'\|\s*\*\*æ—¶é—´\*\*: (.+?)\n', article)
            time_str = time_match.group(1).strip() if time_match else ""

            url_match = re.search(r'\*\*é“¾æ¥\*\*: (.+?)\n', article)
            url = url_match.group(1).strip() if url_match else ""

            rating_match = re.search(r'\*\*ä»·å€¼\*\*: (.+?)\n', article)
            rating = rating_match.group(1).strip() if rating_match else ""
            rating = re.sub(r'\s*\*\*æ ‡ç­¾\*\*:.+', '', rating).strip()

            tag_match = re.search(r'\*\*æ ‡ç­¾\*\*: (.+?)\n', article)
            tag = tag_match.group(1).strip() if tag_match else ""

            summary_match = re.search(r'\*\*æ‘˜è¦\*\*: (.+?)(?:\n---|\n\n###|\Z)', article, re.DOTALL)
            summary = summary_match.group(1).strip() if summary_match else ""
            summary = re.sub(r'<br>', '\n', summary)

            if title and source:
                news_items.append({
                    'title': title,
                    'source': source,
                    'time': time_str,
                    'url': url,
                    'rating': rating,
                    'tag': tag,
                    'summary': summary
                })

        return news_items

    def _parse_github_trending(self, report_path) -> List[Dict]:
        """è§£æ github_trending.md æ–‡ä»¶"""
        with open(report_path, "r", encoding="utf-8") as f:
            content = f.read()

        items = []
        articles = re.split(r'\n###\s+(\d+)\.\s+', content)

        for i in range(1, len(articles), 2):
            if i + 1 >= len(articles):
                break

            article = articles[i + 1]

            if not article.strip():
                continue

            title_match = re.search(r'^([^\n]+)', article)
            title = title_match.group(1).strip() if title_match else f"é¡¹ç›® {articles[i]}"

            lang_match = re.search(r'\*\*è¯­è¨€\*\*: ([^\n|]+)', article)
            language = lang_match.group(1).strip() if lang_match else "æœªçŸ¥"

            stars_match = re.search(r'\*\*Stars\*\*: ([\d,]+)', article)
            stars = stars_match.group(1).strip() if stars_match else ""

            today_match = re.search(r'\*\*ä»Šæ—¥\*\*: \+([\d,]+)', article)
            today_stars = today_match.group(1).strip() if today_match else ""

            url_match = re.search(r'\*\*é“¾æ¥\*\*: (.+?)\n', article)
            url = url_match.group(1).strip() if url_match else ""

            # Extract tech stack (æŠ€æœ¯æ ˆ)
            tech_stack_match = re.search(r'(?:\*\*æŠ€æœ¯æ ˆ\*\*|æŠ€æœ¯æ ˆ):\s*(.+?)(?:\n---|\n\n###|\Z)', article, re.DOTALL)
            tech_stack = tech_stack_match.group(1).strip() if tech_stack_match else ""
            # Clean up tech_stack text
            tech_stack = re.sub(r'\n+', ' ', tech_stack).strip()

            summary_match = re.search(r'\*\*æ‘˜è¦\*\*: (.+?)(?:\n---|\n\n###|\Z|(?:\*\*æŠ€æœ¯æ ˆ\*\*|æŠ€æœ¯æ ˆ):)', article, re.DOTALL)
            summary = summary_match.group(1).strip() if summary_match else ""

            if title:
                items.append({
                    'title': title,
                    'language': language,
                    'stars': stars,
                    'today_stars': today_stars,
                    'url': url,
                    'tech_stack': tech_stack,
                    'summary': summary
                })

        return items

    # ================= Publishing =================

    def publish(self, content: str, title: str, **kwargs) -> Dict[str, Any]:
        """Base publish method - creates a draft."""
        item_type = kwargs.get('item_type', 'article')
        items = kwargs.get('items', [])

        if items:
            html_content = self.generate_html(items, item_type)
        else:
            html_content = content

        draft_id = self._create_draft(title, html_content, config.COVER_MEDIA_ID)

        return {
            'status': 'success',
            'draft_id': draft_id,
            'title': title
        }

    def publish_daily_report(self, report_path: str, title: str = None, target_date: str = None) -> Dict[str, Any]:
        """
        å°† daily_report.md å‘å¸ƒåˆ°è‰ç¨¿ç®±

        Args:
            report_path: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
            title: è‰ç¨¿æ ‡é¢˜
            target_date: æŠ¥å‘Šæ—¥æœŸ

        Returns:
            Result dictionary
        """
        news_items = self._parse_daily_report(report_path)

        if not news_items:
            raise Exception("âŒ æŠ¥å‘Šä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡ç« ")

        print(f"  ğŸ“Š è§£æåˆ° {len(news_items)} ç¯‡æ–‡ç« ")

        if not title:
            if not target_date:
                target_date = datetime.now().strftime("%Y-%m-%d")
            title = f"AI æ¯æ—¥æƒ…æŠ¥ | {target_date}"

        content_html = self.generate_html(news_items, 'article')
        draft_id = self._create_draft(title, content_html, config.COVER_MEDIA_ID)

        return {
            'status': 'success',
            'draft_id': draft_id,
            'title': title,
            'count': len(news_items)
        }

    def publish_github_trending(self, report_path: str, title: str = None, target_date: str = None) -> Dict[str, Any]:
        """
        å°† GitHub Trending å‘å¸ƒåˆ°è‰ç¨¿ç®±

        Args:
            report_path: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
            title: è‰ç¨¿æ ‡é¢˜
            target_date: æŠ¥å‘Šæ—¥æœŸ (YYYY-MM-DD)

        Returns:
            Result dictionary
        """
        items = self._parse_github_trending(report_path)

        if not items:
            raise Exception("âŒ æŠ¥å‘Šä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•é¡¹ç›®")

        print(f"  ğŸ“Š è§£æåˆ° {len(items)} ä¸ªé¡¹ç›®")

        if not title:
            if not target_date:
                # å°è¯•ä» markdown æ–‡ä»¶ä¸­æå–æ—¥æœŸ
                target_date = self._extract_date_from_markdown(report_path)
            title = f"GitHub çƒ­é—¨é¡¹ç›® | {target_date}"

        content_html = self.generate_html(items, 'github')
        draft_id = self._create_draft(title, content_html, config.COVER_MEDIA_ID)

        return {
            'status': 'success',
            'draft_id': draft_id,
            'title': title,
            'count': len(items)
        }

    def publish_paper(self, paper_data: Dict) -> Dict[str, Any]:
        """
        å‘å¸ƒå•ç¯‡è®ºæ–‡åˆ°è‰ç¨¿ç®±

        Args:
            paper_data: Paper analysis data (æ”¯æŒä¸¤ç§æ ¼å¼)
                - ç®€åŒ–æ ¼å¼: {'title', 'arxiv_id', 'org', 'tags', 'score', 'upvotes', 'stars', 'analysis'}
                - å®Œæ•´æ ¼å¼: ä» _parse_analysis_file() è¿”å›çš„å­—å…¸

        Returns:
            Result dictionary
        """
        # åˆ¤æ–­æ˜¯ç®€åŒ–æ ¼å¼è¿˜æ˜¯å®Œæ•´æ ¼å¼
        if 'body' in paper_data:
            # å®Œæ•´æ ¼å¼ - ä½¿ç”¨ç²¾ç¾ HTML
            title = paper_data.get('title', 'è®ºæ–‡åˆ†æ')
            if len(title) > 50:
                title = title[:47] + '...'
            content_html = self._generate_paper_html(paper_data)
        else:
            # ç®€åŒ–æ ¼å¼ - ä½¿ç”¨ç®€å• HTML
            title = paper_data.get('title', 'è®ºæ–‡åˆ†æ')
            if len(title) > 50:
                title = title[:47] + '...'
            content_html = self._generate_simple_paper_html(paper_data)

        draft_id = self._create_draft(title, content_html, config.COVER_MEDIA_ID)

        return {
            'status': 'success',
            'draft_id': draft_id,
            'title': title
        }

    def publish_papers_summary(self, report_path: str, title: str = None, target_date: str = None) -> Dict[str, Any]:
        """
        å°†è®ºæ–‡æ±‡æ€»å‘å¸ƒåˆ°è‰ç¨¿ç®±

        Args:
            report_path: è®ºæ–‡æ±‡æ€»æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
            title: è‰ç¨¿æ ‡é¢˜
            target_date: æŠ¥å‘Šæ—¥æœŸ (YYYY-MM-DD)

        Returns:
            Result dictionary
        """
        items = self._parse_papers_summary(report_path)

        if not items:
            raise Exception("âŒ æŠ¥å‘Šä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è®ºæ–‡")

        print(f"  ğŸ“Š è§£æåˆ° {len(items)} ç¯‡è®ºæ–‡")

        if not title:
            if not target_date:
                target_date = self._extract_date_from_markdown(report_path)
            title = f"æ¯æ—¥è®ºæ–‡æ±‡æ€» | {target_date}"

        content_html = self.generate_html(items, 'paper')
        draft_id = self._create_draft(title, content_html, config.COVER_MEDIA_ID)

        return {
            'status': 'success',
            'draft_id': draft_id,
            'title': title,
            'count': len(items)
        }

    def _parse_papers_summary(self, report_path: str) -> List[Dict]:
        """è§£æ papers_summary.md æ–‡ä»¶"""
        with open(report_path, "r", encoding="utf-8") as f:
            content = f.read()

        items = []
        # Split by "###" followed by number and dot
        sections = re.split(r'\n###\s+(\d+)\.\s+', content)

        for i in range(1, len(sections), 2):
            if i + 1 >= len(sections):
                break

            section = sections[i + 1]
            if not section.strip():
                continue

            # Extract title (first non-empty line after the header)
            lines = section.split('\n')
            title = ""
            for line in lines:
                line = line.strip()
                if line and not line.startswith('**'):
                    title = line
                    break

            # Extract URL from **è®ºæ–‡é“¾æ¥**: [url](url) format
            url_match = re.search(r'\*\*è®ºæ–‡é“¾æ¥\*\*:\s*\[([^\]]+)\]\(([^)]+)\)', section)
            url = url_match.group(2).strip() if url_match else ""

            # Extract arXiv ID from URL
            arxiv_id = ""
            if url and 'arxiv.org/abs/' in url:
                arxiv_id = url.split('arxiv.org/abs/')[-1].split('/')[0]
            elif url:
                # Try to extract from URL as fallback
                arxiv_match = re.search(r'(\d+\.\d+)', url)
                arxiv_id = arxiv_match.group(1) if arxiv_match else ""

            # Extract organization
            org_match = re.search(r'\*\*ç»„ç»‡\*\*:\s*(.+?)\n', section)
            org = org_match.group(1).strip() if org_match else ""

            # Extract score
            score_match = re.search(r'\*\*å¾—åˆ†\*\*:\s*([\d.]+)', section)
            score = score_match.group(1).strip() if score_match else ""

            # Extract tags
            tags_match = re.search(r'\*\*æ ‡ç­¾\*\*:\s*(.+?)\n', section)
            tags = tags_match.group(1).strip() if tags_match else ""

            # Extract upvotes and stars (format: **Upvotes**: 15 | **Stars**: 42)
            upvotes_match = re.search(r'\*\*Upvotes\*\*:\s*(\d+)', section)
            upvotes = upvotes_match.group(1).strip() if upvotes_match else ""

            stars_match = re.search(r'\|\s*\*\*Stars\*\*:\s*(\d+)', section)
            stars = stars_match.group(1).strip() if stars_match else ""

            # Extract summary (after **æ‘˜è¦** until **äº®ç‚¹** or --- or end)
            summary_match = re.search(r'\*\*æ‘˜è¦\*\*:\s*(.+?)(?:\n\*\*äº®ç‚¹\*\*|\n---|\n\n###|\Z)', section, re.DOTALL)
            summary = summary_match.group(1).strip() if summary_match else ""
            summary = re.sub(r'\n+', ' ', summary)  # Convert newlines to spaces
            summary = summary[:500] + "..." if len(summary) > 500 else summary  # Limit length

            # Extract highlights (äº®ç‚¹)
            highlights_match = re.search(r'\*\*äº®ç‚¹\*\*:\s*(.+?)(?:\n---|\n\n###|\Z)', section, re.DOTALL)
            highlights = highlights_match.group(1).strip() if highlights_match else ""
            # Convert bullet points to clean text - handle both leading bullets and bullets after newlines
            highlights = re.sub(r'^\s*-\s*', 'â€¢ ', highlights, count=1)  # First bullet
            highlights = re.sub(r'\n\s*-\s*', '<br>â€¢ ', highlights)  # Subsequent bullets
            highlights = highlights[:300] + "..." if len(highlights) > 300 else highlights

            if title or arxiv_id:
                items.append({
                    'title': title or f"Paper {arxiv_id}",
                    'arxiv_id': arxiv_id,
                    'org': org,
                    'score': score,
                    'tags': tags,
                    'upvotes': upvotes,
                    'stars': stars,
                    'url': url,
                    'summary': summary,
                    'highlights': highlights
                })

        return items

    def _parse_analysis_file(self, analysis_path: str) -> Dict[str, Any]:
        """
        è§£æå•ç¯‡è®ºæ–‡çš„åˆ†ææ–‡ä»¶

        Args:
            analysis_path: åˆ†ææ–‡ä»¶è·¯å¾„

        Returns:
            åŒ…å«è®ºæ–‡ä¿¡æ¯çš„å­—å…¸
        """
        with open(analysis_path, "r", encoding="utf-8") as f:
            content = f.read()

        # æå–æ ‡é¢˜
        h1_matches = list(re.finditer(r'^#\s+(.+)$', content, re.MULTILINE))
        # ç¬¬ä¸€ä¸ª h1 æ˜¯è‹±æ–‡åŸæ ‡é¢˜ï¼ˆç”¨äº HTML æ­£æ–‡ï¼‰
        if h1_matches:
            english_title = h1_matches[0].group(1).strip()
        else:
            english_title = Path(analysis_path).stem

        # ç¬¬äºŒä¸ª h1 æ˜¯ä¸­æ–‡æ ‡é¢˜ï¼ˆç”¨äºè‰ç¨¿æ ‡é¢˜ï¼‰
        if len(h1_matches) >= 2:
            title = h1_matches[1].group(1).strip()  # ç¬¬äºŒä¸ª h1
        elif h1_matches:
            title = english_title  # åªæœ‰ä¸€ä¸ª h1ï¼Œç”¨ç¬¬ä¸€ä¸ª
        else:
            title = Path(analysis_path).stem

        # æå–è®ºæ–‡åŸæ ‡é¢˜ (ä»ç¬¬ä¸€è¡Œçš„ã€Šã€‹ä¸­æå–)
        paper_title_match = re.search(r'ã€Š(.+?)ã€‹', content.split('---')[0] if '---' in content else content)
        paper_title = paper_title_match.group(1) if paper_title_match else ''

        # æå–å…ƒæ•°æ® (arXiv ID, ç»„ç»‡, Stars, Upvotes, å¾—åˆ†, æ ‡ç­¾)
        arxiv_id_match = re.search(r'\*\*arXiv ID\*\*:\s*(.+)', content)
        # Extract arXiv ID from markdown link format
        if arxiv_id_match:
            arxiv_id_text = arxiv_id_match.group(1).strip()
            arxiv_id_link = re.search(r'\[([^\]]+)\]\(([^)]+)\)', arxiv_id_text)
            if arxiv_id_link:
                arxiv_id = arxiv_id_link.group(1)  # Use the display text
                arxiv_url = arxiv_id_link.group(2)
            else:
                arxiv_id = arxiv_id_text
                arxiv_url = f"https://arxiv.org/abs/{arxiv_id_text}"
        else:
            arxiv_id = ''
            arxiv_url = ''

        org = re.search(r'\*\*ç»„ç»‡\*\*:\s*(.+)', content)
        # Extract stars from format: **Upvotes**: 15 | **Stars**: 42
        upvotes = re.search(r'\*\*Upvotes\*\*:\s*(\d+)', content)
        stars = re.search(r'\|\s*\*\*Stars\*\*:\s*(\d+)', content)
        score = re.search(r'\*\*å¾—åˆ†\*\*:\s*([\d.]+)', content)
        tags = re.search(r'\*\*æ ‡ç­¾\*\*:\s*(.+)', content)

        # æå–æ­£æ–‡ (å»é™¤ --- ä¹‹åçš„å†…å®¹)
        parts = content.split('---', 1)
        body = parts[1].strip() if len(parts) > 1 else content

        # æå–ç¬¬ä¸€æ®µä½œä¸ºæ‘˜è¦ï¼ˆå»é™¤ç©ºè¡Œåç¬¬ä¸€ä¸ªéæ ‡é¢˜æ®µè½ï¼‰
        intro_match = re.search(r'^(?!#)(?!<)(.+)$', body, re.MULTILINE)
        intro = intro_match.group(1).strip() if intro_match else ''
        # å»é™¤ intro ä¸­çš„ markdown æ ¼å¼
        intro = re.sub(r'\*\*(.+?)\*\*', r'\1', intro)
        intro = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', intro)

        return {
            'title': title,
            'english_title': english_title,
            'paper_title': paper_title,
            'intro': intro,
            'arxiv_id': arxiv_id,
            'arxiv_url': arxiv_url,
            'org': org.group(1).strip() if org else '',
            'stars': stars.group(1).strip() if stars else '',
            'upvotes': upvotes.group(1).strip() if upvotes else '',
            'score': score.group(1).strip() if score else '',
            'tags': tags.group(1).strip() if tags else '',
            'body': body
        }

    def _markdown_to_html(self, markdown_text: str) -> str:
        """
        å°† Markdown è½¬æ¢ä¸ºå¾®ä¿¡å…¬ä¼—å· HTML - å®Œæ•´å®ç°
        æ”¯æŒæ ‡é¢˜ã€åˆ—è¡¨ã€é“¾æ¥ã€ç²—ä½“ç­‰æ ¼å¼
        """
        lines = markdown_text.split('\n')
        html_lines = []
        skip_first_h1 = True  # è·³è¿‡ç¬¬ä¸€ä¸ª h1ï¼ˆå› ä¸ºå·²åœ¨æ ‡é¢˜å¤„æ˜¾ç¤ºï¼‰

        # åˆ é™¤ç¬¬ä¸€æ®µï¼ˆå·²ä½œä¸º intro æ˜¾ç¤ºï¼‰
        first_para_removed = False

        i = 0
        while i < len(lines):
            line = lines[i]
            stripped = line.strip()

            # è·³è¿‡ --- åˆ†éš”çº¿
            if stripped == '---':
                i += 1
                continue

            # è·³è¿‡ç¬¬ä¸€ä¸ª h1 æ ‡é¢˜
            if skip_first_h1 and re.match(r'^#\s+', line):
                skip_first_h1 = False
                i += 1
                continue

            # è·³è¿‡ç¬¬ä¸€æ®µï¼ˆå·²ä½œä¸º intro æ˜¾ç¤ºåœ¨å¡ç‰‡ä¸­ï¼‰
            if not first_para_removed and stripped and not re.match(r'^[#\*\-\d\s]', line):
                first_para_removed = True
                i += 1
                continue

            # å¤„ç†å››çº§æ ‡é¢˜
            match = re.match(r'^####\s+(.+)$', line)
            if match:
                content = match.group(1)
                content = re.sub(r'\*\*(.+?)\*\*([ï¼š:ã€,ï¼Œ.ã€‚ï¼›;])', r'<strong style="color: #2c3e50; font-weight: 600;">\1\2</strong>', content)
                content = re.sub(r'\*\*(.+?)\*\*', r'<strong style="color: #2c3e50; font-weight: 600;">\1</strong>', content)
                html_lines.append(f'<h4 style="font-size: 16px; font-weight: bold; color: #555; text-align: left; margin: 15px 0 10px;">{content}</h4>')
                i += 1
                continue

            # å¤„ç†ä¸‰çº§æ ‡é¢˜
            match = re.match(r'^###\s+(.+)$', line)
            if match:
                content = match.group(1)
                content = re.sub(r'\*\*(.+?)\*\*([ï¼š:ã€,ï¼Œ.ã€‚ï¼›;])', r'<strong style="color: #2c3e50; font-weight: 600;">\1\2</strong>', content)
                content = re.sub(r'\*\*(.+?)\*\*', r'<strong style="color: #2c3e50; font-weight: 600;">\1</strong>', content)
                html_lines.append(f'<h3 style="font-size: 18px; font-weight: bold; color: #34495e; text-align: left; margin: 20px 0 12px; padding-left: 10px; border-left: 4px solid #3498db;">{content}</h3>')
                i += 1
                continue

            # å¤„ç†äºŒçº§æ ‡é¢˜
            match = re.match(r'^##\s+(.+)$', line)
            if match:
                content = match.group(1)
                content = re.sub(r'\*\*(.+?)\*\*([ï¼š:ã€,ï¼Œ.ã€‚ï¼›;])', r'<strong style="color: #2c3e50; font-weight: 600;">\1\2</strong>', content)
                content = re.sub(r'\*\*(.+?)\*\*', r'<strong style="color: #2c3e50; font-weight: 600;">\1</strong>', content)
                html_lines.append(f'<h2 style="font-size: 20px; font-weight: bold; color: #2c3e50; text-align: center; margin: 30px 0 15px; padding: 10px 0; border-top: 1px solid #e0e0e0; border-bottom: 1px solid #e0e0e0;">{content}</h2>')
                i += 1
                continue

            # å¤„ç†ä¸€çº§æ ‡é¢˜ï¼ˆè·³è¿‡ç¬¬ä¸€ä¸ªä¹‹åçš„å…¶ä»– h1ï¼‰
            match = re.match(r'^#\s+(.+)$', line)
            if match:
                content = match.group(1)
                content = re.sub(r'\*\*(.+?)\*\*([ï¼š:ã€,ï¼Œ.ã€‚ï¼›;])', r'<strong style="color: #2c3e50; font-weight: 600;">\1\2</strong>', content)
                content = re.sub(r'\*\*(.+?)\*\*', r'<strong style="color: #2c3e50; font-weight: 600;">\1</strong>', content)
                html_lines.append(f'<h1 style="font-size: 22px; font-weight: bold; color: #1a1a1a; text-align: center; margin: 25px 0 20px; padding-bottom: 10px;">{content}</h1>')
                i += 1
                continue

            # å¤„ç†ç©ºè¡Œ
            if not stripped:
                if html_lines and not html_lines[-1].startswith('</'):
                    html_lines.append('<br>')
                i += 1
                continue

            # æ”¶é›†åˆ—è¡¨ï¼ˆå¤šè¡Œï¼‰- æ”¯æŒçœŸæ­£çš„åµŒå¥—åˆ—è¡¨
            list_structure = []  # List of (content, children) tuples
            list_type = None  # 'ul' or 'ol'
            base_indent = None
            current_parents = []  # Track parent items with their indent levels

            while i < len(lines):
                line = lines[i]
                stripped_i = line.strip()

                # è·³è¿‡ --- åˆ†éš”çº¿
                if stripped_i == '---':
                    i += 1
                    break

                # ç©ºè¡Œç»“æŸåˆ—è¡¨
                if not stripped_i:
                    break

                # æ£€æµ‹åˆ—è¡¨é¡¹
                ul_match = re.match(r'^([\s]*)[\*\-]\s+', line)
                ol_match = re.match(r'^([\s]*)\d+\.\s+', line)

                match_obj = ul_match if ul_match else ol_match

                if match_obj:
                    indent = len(match_obj.group(1))

                    # ç¡®å®šåˆ—è¡¨ç±»å‹
                    if list_type is None:
                        list_type = 'ul' if ul_match else 'ol'
                        base_indent = indent

                    # æ£€æµ‹æ˜¯å¦æ˜¯ä¸åŒç±»å‹çš„åˆ—è¡¨
                    current_is_ul = ul_match is not None
                    if (current_is_ul and list_type != 'ul') or (not current_is_ul and list_type == 'ul'):
                        if list_structure:
                            break

                    start, end = match_obj.span()
                    content = line[end:].rstrip()

                    # å¤„ç†å†…è”æ ¼å¼ - å…ˆå¤„ç†å¸¦ä¸­æ–‡æ ‡ç‚¹çš„ boldï¼ŒæŠŠæ ‡ç‚¹åŒ…å«åœ¨ strong æ ‡ç­¾å†…
                    content = re.sub(r'\*\*(.+?)\*\*([ï¼š:ã€,ï¼Œ.ã€‚ï¼›;])', r'<strong style="color: #2c3e50; font-weight: 600;">\1\2</strong>', content)
                    content = re.sub(r'\*\*(.+?)\*\*', r'<strong style="color: #2c3e50; font-weight: 600;">\1</strong>', content)
                    content = re.sub(r'\[([^\]]+)\]\(([^)]+)\)', r'<a href="\2" style="color: #3498db;">\1</a>', content)

                    # åˆ¤æ–­å±‚çº§
                    level = 0
                    if indent > base_indent:
                        # è®¡ç®—åµŒå¥—å±‚çº§ (æ¯4ä¸ªç©ºæ ¼æˆ–1ä¸ªtabä¸ºä¸€çº§)
                        level = (indent - base_indent) // 4 + 1

                    # æ·»åŠ åˆ°ç»“æ„ä¸­
                    item = {'content': content, 'level': level, 'children': []}

                    # æ‰¾åˆ°æ­£ç¡®çš„çˆ¶çº§
                    while current_parents and current_parents[-1]['level'] >= level:
                        current_parents.pop()

                    if current_parents:
                        current_parents[-1]['children'].append(item)
                    else:
                        list_structure.append(item)

                    # å¦‚æœè¿™ä¸ªé¡¹å¯èƒ½æœ‰è‡ªå·±çš„å­é¡¹ï¼ŒåŠ å…¥çˆ¶çº§åˆ—è¡¨
                    # ä½†åªæœ‰å½“å†…å®¹ä¸ä¸ºç©ºæˆ–è€…æ˜¯æ ‡é¢˜å½¢å¼æ—¶æ‰ä½œä¸ºæ½œåœ¨çˆ¶çº§
                    if content or True:  # ä»»ä½•é¡¹ç›®éƒ½å¯èƒ½æœ‰å­é¡¹
                        current_parents.append(item)

                    i += 1
                else:
                    # éåˆ—è¡¨è¡Œï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯å‰ä¸€ä¸ªåˆ—è¡¨é¡¹çš„ç»­è¡Œ
                    if list_structure and (line.startswith('    ') or line.startswith('\t')):
                        continuation = line.rstrip()
                        continuation = re.sub(r'\*\*(.+?)\*\*([ï¼š:ã€,ï¼Œ.ã€‚ï¼›;])', r'<strong style="color: #2c3e50; font-weight: 600;">\1\2</strong>', continuation)
                        continuation = re.sub(r'\*\*(.+?)\*\*', r'<strong style="color: #2c3e50; font-weight: 600;">\1</strong>', continuation)
                        continuation = re.sub(r'\[([^\]]+)\]\(([^)]+)\)', r'<a href="\2" style="color: #3498db;">\1</a>', continuation)
                        # æ‰¾åˆ°æœ€åçš„åˆ—è¡¨é¡¹å¹¶æ·»åŠ ç»­è¡Œ
                        if current_parents:
                            current_parents[-1]['content'] += f'<br>{continuation}'
                        elif list_structure:
                            list_structure[-1]['content'] += f'<br>{continuation}'
                        i += 1
                    else:
                        break

            # ç”Ÿæˆ HTML
            if list_structure:
                def render_item(item, is_root=True):
                    content = item['content']
                    children = item['children']
                    children_html = ''

                    if children:
                        # é€’å½’æ¸²æŸ“å­åˆ—è¡¨
                        nested_items = ''.join(render_item(child, False) for child in children)
                        children_html = f'<ul style="margin: 5px 0; padding-left: 20px;">{nested_items}</ul>'

                    style = 'margin: 8px 0; line-height: 1.8; color: #333;' if is_root else 'margin: 4px 0; line-height: 1.8; color: #333;'

                    # å¦‚æœå†…å®¹åªæœ‰å†’å·æˆ–ä¸ºç©ºï¼Œä¸å­åˆ—è¡¨åˆå¹¶
                    if not content or content == 'ï¼š' or content == ':':
                        return f'<li style="{style}">{children_html}</li>'
                    elif children_html:
                        return f'<li style="{style}">{content}{children_html}</li>'
                    else:
                        return f'<li style="{style}">{content}</li>'

                all_items_html = ''.join(render_item(item) for item in list_structure)
                style = 'margin: 15px 0; padding-left: 20px;' if list_type == 'ul' else 'margin: 15px 0; padding-left: 25px;'
                html_lines.append(f'<{list_type} style="{style}">{all_items_html}</{list_type}>')
                continue
                continue

            # å¤„ç†æ™®é€šæ®µè½
            if stripped:
                line = re.sub(r'\*\*(.+?)\*\*([ï¼š:ã€,ï¼Œ.ã€‚ï¼›;])', r'<strong style="color: #2c3e50; font-weight: 600;">\1\2</strong>', line)
                line = re.sub(r'\*\*(.+?)\*\*', r'<strong style="color: #2c3e50; font-weight: 600;">\1</strong>', line)
                line = re.sub(r'\[([^\]]+)\]\(([^)]+)\)', r'<a href="\2" style="color: #3498db;">\1</a>', line)
                html_lines.append(f'<p style="font-size: 15px; color: #333; line-height: 1.9; margin-bottom: 10px; text-align: justify;">{line}</p>')

            i += 1

        return '\n'.join(html_lines)

    def _generate_paper_html(self, paper_data: Dict) -> str:
        """
        ç”Ÿæˆå•ç¯‡è®ºæ–‡çš„ç²¾ç¾ HTML

        Args:
            paper_data: è®ºæ–‡æ•°æ®å­—å…¸

        Returns:
            HTML å­—ç¬¦ä¸²
        """
        container = '<section style="font-family: -apple-system, BlinkMacSystemFont, \'Segoe UI\', Roboto, Helvetica, Arial, sans-serif; max-width: 677px; margin: 0 auto; padding: 20px 0;">'

        # æ ‡é¢˜å¤´éƒ¨ - ä½¿ç”¨è‹±æ–‡åŸæ ‡é¢˜
        english_title = paper_data.get('english_title', paper_data.get('title', ''))
        title_html = f'''
<div style="text-align: center; margin-bottom: 25px;">
    <h1 style="font-size: 24px; font-weight: bold; color: #1a1a1a; margin: 0 0 15px; line-height: 1.4;">{english_title}</h1>
</div>
'''

        # å…ƒä¿¡æ¯å¡ç‰‡
        meta_html = '<div style="background: linear-gradient(135deg, #f5f7fa 0%, #e8ecf1 100%); padding: 15px; border-radius: 8px; margin-bottom: 25px; font-size: 14px; color: #555;">'

        # è®ºæ–‡é“¾æ¥ - çº¯æ–‡æœ¬æ ¼å¼
        if paper_data.get('arxiv_url'):
            meta_html += f'<div style="margin-bottom: 8px;">ğŸ“„ è®ºæ–‡ï¼š<a href="{paper_data["arxiv_url"]}" style="color: #3498db; text-decoration: none;">{paper_data["arxiv_url"]}</a></div>'
        elif paper_data.get('arxiv_id'):
            arxiv_url = f"https://arxiv.org/abs/{paper_data['arxiv_id']}"
            meta_html += f'<div style="margin-bottom: 8px;">ğŸ“„ è®ºæ–‡ï¼š<a href="{arxiv_url}" style="color: #3498db; text-decoration: none;">{arxiv_url}</a></div>'

        if paper_data.get('org'):
            meta_html += f'<div style="margin-bottom: 8px;">ğŸ”¬ <strong>æœºæ„ï¼š</strong>{paper_data["org"]}</div>'
        if paper_data.get('tags'):
            meta_html += f'<div style="margin-bottom: 8px;">ğŸ·ï¸ <strong>æ ‡ç­¾ï¼š</strong>{paper_data["tags"]}</div>'

        # å¾—åˆ†å’Œäº’åŠ¨æ•°æ®
        stats_row = ''
        if paper_data.get('score'):
            stats_row += f'<span style="display: inline-block; margin-right: 15px;">ğŸ“Š {paper_data["score"]}</span>'
        if paper_data.get('upvotes'):
            stats_row += f'<span style="display: inline-block; margin-right: 15px;">ğŸ‘ {paper_data["upvotes"]}</span>'
        if paper_data.get('stars'):
            stats_row += f'<span style="display: inline-block;">ğŸŒŸ {paper_data["stars"]}</span>'
        if stats_row:
            meta_html += f'<div style="margin-top: 10px; padding-top: 10px; border-top: 1px solid #d0d7de;">{stats_row}</div>'

        meta_html += '</div>'

        # æ‘˜è¦æ®µè½
        intro_html = ''
        if paper_data.get('intro'):
            intro_html = f'<p style="font-size: 15px; color: #444; line-height: 1.8; margin-bottom: 20px; text-align: justify; padding: 12px; background: #f9f9f9; border-radius: 6px;">{paper_data["intro"]}</p>'

        # æ­£æ–‡ï¼ˆä½¿ç”¨å®Œæ•´ Markdownâ†’HTML è½¬æ¢ï¼‰
        body_html = self._markdown_to_html(paper_data['body'])

        return container + title_html + meta_html + intro_html + body_html + '</section>'

    def _generate_simple_paper_html(self, paper: Dict) -> str:
        """
        ç”Ÿæˆç®€åŒ–ç‰ˆè®ºæ–‡ HTMLï¼ˆç”¨äº publish_paper æ–¹æ³•ï¼‰

        Args:
            paper: ç®€åŒ–è®ºæ–‡æ•°æ®

        Returns:
            HTML å­—ç¬¦ä¸²
        """
        container = '<section style="font-family: -apple-system, BlinkMacSystemFont, Arial, sans-serif; max-width: 677px; margin: 0 auto; padding: 20px 0;">'

        title_html = f'<h1 style="font-size: 24px; font-weight: bold; color: #1a1a1a; margin: 0 0 15px; text-align: center;">{paper.get("title", "æœªçŸ¥")}</h1>'

        meta_html = '<div style="background: #f5f7fa; padding: 15px; border-radius: 8px; margin-bottom: 20px; font-size: 14px; color: #555;">'

        if paper.get('arxiv_id'):
            arxiv_url = f"https://arxiv.org/abs/{paper['arxiv_id']}"
            meta_html += f'<div style="margin-bottom: 8px;">ğŸ“„ <strong>è®ºæ–‡ï¼š</strong><a href="{arxiv_url}" style="color: #3498db;">{paper["arxiv_id"]}</a></div>'

        if paper.get('org'):
            meta_html += f'<div style="margin-bottom: 8px;">ğŸ”¬ <strong>æœºæ„ï¼š</strong>{paper["org"]}</div>'

        if paper.get('tags'):
            meta_html += f'<div style="margin-bottom: 8px;">ğŸ·ï¸ <strong>æ ‡ç­¾ï¼š</strong>{paper["tags"]}</div>'

        stats_row = ''
        if paper.get('score'):
            stats_row += f'<span style="display: inline-block; margin-right: 15px;">ğŸ“Š {paper["score"]}</span>'
        if paper.get('upvotes'):
            stats_row += f'<span style="display: inline-block; margin-right: 15px;">ğŸ‘ {paper["upvotes"]}</span>'
        if paper.get('stars'):
            stats_row += f'<span style="display: inline-block;">â­ {paper["stars"]}</span>'

        if stats_row:
            meta_html += f'<div style="margin-top: 10px; padding-top: 10px; border-top: 1px solid #d0d7de;">{stats_row}</div>'

        meta_html += '</div>'

        # Convert markdown analysis to HTML (simplified)
        analysis = paper.get('analysis', '')
        body_html = f'<div style="font-size: 15px; color: #333; line-height: 1.8;">{analysis.replace("\n", "<br>")}</div>'

        return container + title_html + meta_html + body_html + '</section>'

    def publish_single_paper(self, analysis_path: str) -> Dict[str, Any]:
        """
        å‘å¸ƒå•ç¯‡è®ºæ–‡åˆ†æåˆ°è‰ç¨¿ç®±

        Args:
            analysis_path: åˆ†ææ–‡ä»¶è·¯å¾„

        Returns:
            ç»“æœå­—å…¸ï¼ŒåŒ…å« draft_id å’Œ title
        """
        paper_data = self._parse_analysis_file(analysis_path)

        print(f"ğŸ“„ æ­£åœ¨å‘å¸ƒ: {paper_data['title']}")

        # ç”Ÿæˆæ ‡é¢˜ (å»é™¤è¿‡é•¿æ ‡é¢˜)
        title = paper_data['title']
        if len(title) > 50:
            title = title[:47] + '...'

        # ç”Ÿæˆ HTML
        content_html = self._generate_paper_html(paper_data)

        # åˆ›å»ºè‰ç¨¿
        draft_id = self._create_draft(title, content_html, config.COVER_MEDIA_ID)

        return {
            'status': 'success',
            'draft_id': draft_id,
            'title': title
        }

    def publish_all_papers(self, date_str: str) -> List[Dict[str, Any]]:
        """
        å‘å¸ƒæŸä¸€å¤©çš„æ‰€æœ‰è®ºæ–‡åˆ†æ

        Args:
            date_str: æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)

        Returns:
            ç»“æœåˆ—è¡¨
        """
        project_root = Path(__file__).parent.parent.parent
        output_dir = project_root / "output" / date_str

        if not output_dir.exists():
            raise Exception(f"è¾“å‡ºç›®å½•ä¸å­˜åœ¨: {output_dir}")

        # æ‰¾åˆ°æ‰€æœ‰åˆ†ææ–‡ä»¶ (æ’é™¤ _summary.md)
        analysis_files = [
            f for f in output_dir.glob("papers/papers_note_*.md")
            if not f.name.startswith('_')
        ]

        if not analysis_files:
            print(f"âŒ æ²¡æœ‰æ‰¾åˆ°åˆ†ææ–‡ä»¶: {output_dir}")
            return []

        print(f"ğŸ“Š æ‰¾åˆ° {len(analysis_files)} ç¯‡è®ºæ–‡åˆ†æ")

        results = []
        for i, analysis_file in enumerate(analysis_files, 1):
            print(f"\n[{i}/{len(analysis_files)}] {analysis_file.name}")
            try:
                result = self.publish_single_paper(str(analysis_file))
                result['file'] = analysis_file.name
                results.append(result)
                print(f"  âœ… æˆåŠŸ - Media ID: {result['draft_id']}")
            except Exception as e:
                results.append({
                    'file': analysis_file.name,
                    'error': str(e),
                    'status': 'failed'
                })
                print(f"  âŒ å¤±è´¥ - {e}")

        return results

    def _extract_date_from_markdown(self, report_path: str) -> str:
        """ä» markdown æ–‡ä»¶æ ‡é¢˜è¡Œä¸­æå–æ—¥æœŸ"""
        with open(report_path, "r", encoding="utf-8") as f:
            first_line = f.readline()
        # åŒ¹é…æ ¼å¼: # GitHub çƒ­é—¨é¡¹ç›® | 2026-02-01 æˆ– # æ¯æ—¥è®ºæ–‡æ±‡æ€» | 2026-02-01
        match = re.search(r'\| (\d{4}-\d{2}-\d{2})', first_line)
        if match:
            return match.group(1)
        return datetime.now().strftime('%Y-%m-%d')

    def _create_draft(self, title: str, content: str, thumb_id: str) -> str:
        """åˆ›å»ºè‰ç¨¿"""
        url = f"https://api.weixin.qq.com/cgi-bin/draft/add?access_token={self.token}"
        data = {
            "articles": [{
                "title": title,
                "author": "AI Report",
                "digest": "AI æƒ…æŠ¥æ‘˜è¦...",
                "content": content,
                "thumb_media_id": thumb_id
            }]
        }

        resp = requests.post(
            url,
            data=json.dumps(data, ensure_ascii=False).encode('utf-8'),
            headers={'Content-Type': 'application/json; charset=utf-8'},
            proxies=getattr(config, 'PROXIES', None)
        )

        result = resp.json()

        if 'media_id' not in result:
            raise Exception(f"âŒ è‰ç¨¿åˆ›å»ºå¤±è´¥: {result}")

        return result['media_id']
