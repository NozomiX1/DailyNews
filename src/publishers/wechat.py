# DailyNews - WeChat Publisher
# Merged and refactored from wechat_publisher.py, github_publisher.py, paper_publisher.py
import requests
import json
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List

import sys
sys.path.insert(0, str(Path(__file__).parent.parent.parent))
import config

import mistune
from premailer import transform as premailer_transform

from .base import BasePublisher
from .css_loader import get_inline_styles_css


# ==================== Markdown Renderer ====================

class WeChatRenderer(mistune.HTMLRenderer):
    """
    å¾®ä¿¡å…¬ä¼—å·æ¸²æŸ“å™¨ - è¾“å‡ºå¸¦ class çš„ HTML
    æ ·å¼ç”± CSS æ–‡ä»¶å®šä¹‰ï¼Œæœ€åé€šè¿‡ premailer è½¬æ¢ä¸ºå†…è”æ ·å¼
    """

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.footnotes = []
        self.footnote_index = 0

    def reset_footnotes(self):
        self.footnotes = []
        self.footnote_index = 0

    def build_footnotes(self):
        if not self.footnotes:
            return ''

        html = '<h4>å¼•ç”¨é“¾æ¥</h4>\n<p class="footnotes">'
        for idx, title, url in self.footnotes:
            html += f'<code>[{idx}]</code>: <i>{url}</i><br/>'
        html += '</p>'
        return html

    def heading(self, text, level, **attrs):
        return f'<h{level}>{text}</h{level}>\n'

    def paragraph(self, text):
        return f'<p>{text}</p>\n'

    def strong(self, text):
        return f'<strong>{text}</strong>'

    def emphasis(self, text):
        return f'<em>{text}</em>'

    def link(self, text, url, title=None):
        # å¾®ä¿¡å†…éƒ¨é“¾æ¥ç›´æ¥æ¸²æŸ“
        if url.startswith('https://mp.weixin.qq.com'):
            return f'<a href="{url}">{text}</a>'

        # å¤–éƒ¨é“¾æ¥è½¬æ¢ä¸ºè„šæ³¨
        self.footnote_index += 1
        self.footnotes.append((self.footnote_index, text, url))
        return f'{text}<sup>[{self.footnote_index}]</sup>'

    def codespan(self, text):
        return f'<code>{text}</code>'

    def block_code(self, code, info=None):
        escaped = mistune.escape(code)
        return f'<pre class="code__pre"><code>{escaped}</code></pre>\n'

    def list(self, text, ordered, **attrs):
        tag = 'ol' if ordered else 'ul'
        return f'<{tag}>{text}</{tag}>\n'

    def list_item(self, text, **attrs):
        return f'<li>{text}</li>\n'

    def block_quote(self, text):
        return f'<blockquote>{text}</blockquote>\n'

    def thematic_break(self):
        return '<hr>\n'

    def image(self, alt, url, title=None):
        title_attr = f' title="{title}"' if title else ''
        return f'<img src="{url}" alt="{alt}"{title_attr}>'

    def table(self, header, body):
        return f'<table><thead>{header}</thead><tbody>{body}</tbody></table>\n'

    def table_head(self, text):
        return f'<tr>{text}</tr>\n'

    def table_body(self, text):
        return text

    def table_row(self, text):
        return f'<tr>{text}</tr>\n'

    def table_cell(self, text, **attrs):
        tag = 'th' if attrs.get('is_head') else 'td'
        return f'<{tag}>{text}</{tag}>'


def _preprocess_latex(text: str) -> str:
    """
    é¢„å¤„ç† LaTeX å…¬å¼
    è½¬æ¢ $$...$$ ä¸º [formula]...[/formula]
    """
    # å—çº§å…¬å¼
    text = re.sub(r'\$\$([^$]+)\$\$', r'[formula]\1[/formula]', text)
    # è¡Œå†…å…¬å¼
    text = re.sub(r'\$([^$]+)\$', r'[inline_formula]\1[/inline_formula]', text)
    return text


def _apply_inline_styles(html: str) -> str:
    """
    å°† class-based HTML è½¬æ¢ä¸ºå†…è”æ ·å¼ HTML

    Args:
        html: å¸¦ class çš„ HTML

    Returns:
        å¸¦å†…è”æ ·å¼çš„ HTMLï¼ˆå¾®ä¿¡å…¼å®¹ï¼‰
    """
    css = get_inline_styles_css()

    # åŒ…è£…ä¸ºå®Œæ•´ HTML æ–‡æ¡£ä¾› premailer å¤„ç†
    full_html = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <style>{css}</style>
    </head>
    <body>
        <section>{html}</section>
    </body>
    </html>
    """

    # premailer è½¬æ¢
    result = premailer_transform(
        full_html,
        remove_classes=True,
        strip_important=True,
        keep_style_tags=False,
        cssutils_logging_level='CRITICAL'
    )

    # æå– <section> å†…å®¹
    match = re.search(r'<section[^>]*>(.*?)</section>', result, re.DOTALL)
    if match:
        return match.group(1).strip()
    return html


def _create_wechat_markdown_parser():
    """åˆ›å»ºé…ç½®å¥½çš„ mistune Markdown è§£æå™¨"""
    renderer = WeChatRenderer(escape=False)
    md = mistune.create_markdown(renderer=renderer)

    def parse_with_styles(text):
        # é‡ç½®è„šæ³¨
        renderer.reset_footnotes()
        # é¢„å¤„ç† LaTeX å…¬å¼
        preprocessed = _preprocess_latex(text)
        # æ¸²æŸ“ä¸º class-based HTML
        html = md(preprocessed)
        # æ·»åŠ è„šæ³¨
        footnotes = renderer.build_footnotes()
        full_html = html + footnotes
        # è½¬æ¢ä¸ºå†…è”æ ·å¼
        return _apply_inline_styles(full_html)

    return parse_with_styles


# ==================== Publisher ====================


class WechatPublisher(BasePublisher):
    """
    WeChat Official Account publisher.

    Publishes daily reports, GitHub trending, and paper analyses to WeChat drafts.
    """

    def __init__(self, app_id=None, app_secret=None):
        super().__init__()
        self.app_id = app_id or config.APP_ID
        self.app_secret = app_secret or config.APP_SECRET
        self.token = self._get_access_token()

    def _get_access_token(self):
        """è·å– access_token"""
        url = f"https://api.weixin.qq.com/cgi-bin/token?grant_type=client_credential&appid={self.app_id}&secret={self.app_secret}"
        try:
            resp = requests.get(url, proxies=getattr(config, 'PROXIES', None)).json()
            if 'access_token' in resp:
                return resp['access_token']
            else:
                raise Exception(f"è·å– access_token å¤±è´¥: {resp}")
        except Exception as e:
            raise Exception(f"è·å– access_token å¼‚å¸¸: {e}")

    # ================= Text Width Calculation =================

    def _calc_text_width(self, text):
        """è®¡ç®—æ–‡æœ¬å®½åº¦ï¼ˆä»¥ 1/3 å•ä½ä¸ºåŸºå‡†ï¼‰"""
        width = 0
        for char in text:
            if char in 'ğŸŒŸâ­ğŸ“ğŸ•’ğŸ·ï¸ğŸ”¥ğŸ’»ğŸ“„ğŸ”¬ğŸ“ŠğŸ”—':
                width += 4
            elif ord(char) > 127:
                width += 3
            else:
                width += 1
        return width

    # ================= HTML Generation =================

    def _generate_meta_row(self, item: Dict, item_type: str = 'article') -> str:
        """Generate metadata HTML row for an item."""
        if item_type == 'article':
            prefix1 = f'ğŸ“ æ¥æºï¼š{item["source"]}'
            prefix2 = f'â­ ä»·å€¼ï¼š{item["rating"]}' if item.get("rating") else ''

            width1 = self._calc_text_width(prefix1)
            width2 = self._calc_text_width(prefix2) if prefix2 else 0

            if width1 < width2:
                prefix1 += ' ' * (width2 - width1)
            elif width2 < width1:
                prefix2 += ' ' * (width1 - width2)

            meta_first_line = f'{prefix1}ã€€ğŸ•’ {item["time"]}'

            meta_second_parts = []
            if item.get("rating"):
                meta_second_parts.append(prefix2)
            if item.get("tag"):
                meta_second_parts.append(f'ã€€ğŸ·ï¸ æ ‡ç­¾ï¼š{item["tag"]}')

            meta_second_line = ''
            if meta_second_parts:
                meta_second_line = '<br>' + ''.join(meta_second_parts)

            return f'{meta_first_line}{meta_second_line}<br><span style="display: inline-block; margin-top: 4px; color: #576b95; word-break: break-all;">ğŸ”— é“¾æ¥ï¼š{item["url"]}</span>'

        elif item_type == 'github':
            parts = []
            if item.get('language'):
                parts.append(f'ğŸ’» è¯­è¨€ï¼š{item["language"]}')
            if item.get('stars'):
                parts.append(f'â­ Starsï¼š{item["stars"]}')
            if item.get('today_stars'):
                parts.append(f'ğŸ”¥ ä»Šæ—¥ï¼š+{item["today_stars"]}')

            meta_line = f'{"ã€€".join(parts)}<br><span style="display: inline-block; margin-top: 4px; color: #576b95; word-break: break-all;">ğŸ”— é“¾æ¥ï¼š{item["url"]}</span>'

            # Add tech stack below the link (without bold)
            if item.get('tech_stack'):
                meta_line += f'<br><span style="display: inline-block; margin-top: 4px; color: #666; font-size: 13px;">æŠ€æœ¯æ ˆ: {item["tech_stack"]}</span>'

            return meta_line

        elif item_type == 'paper':
            parts = []
            if item.get('score'):
                parts.append(f'ğŸ“Š å¾—åˆ†ï¼š{item["score"]}')
            if item.get('upvotes'):
                parts.append(f'ğŸ‘ {item["upvotes"]}')
            if item.get('stars'):
                parts.append(f'â­ {item["stars"]}')

            meta_line = 'ã€€'.join(parts) if parts else ''
            return f'{meta_line}<br><span style="display: inline-block; margin-top: 4px; color: #576b95; word-break: break-all;">ğŸ”— é“¾æ¥ï¼š{item["url"]}</span>'

        return ''

    def generate_html(self, items: List[Dict], item_type: str = 'article') -> str:
        """
        ä»æ–°é—»åˆ—è¡¨ç”Ÿæˆ HTML

        Args:
            items: News items list
            item_type: Type of items ('article', 'github', 'paper')

        Returns:
            HTML string
        """
        html_parts = ['<section style="font-family: -apple-system, BlinkMacSystemFont, Arial, sans-serif;">']
        html_parts.append('<section style="margin-top: 20px;"></section>')

        for idx, item in enumerate(items, 1):
            title = item.get('title', item.get('original_title', ''))

            title_html = f'<h3 style="margin-top: 30px; margin-bottom: 5px; font-size: 18px; font-weight: bold; color: #000;">{idx}. {title}</h3>'

            meta_html = f'<div style="font-size: 13px; color: #888; margin-bottom: 10px; background: #f9f9f9; padding: 8px; border-radius: 4px;">{self._generate_meta_row(item, item_type)}</div>'

            summary = item.get('summary', '')
            # Use markdown conversion instead of simple newline replacement
            summary_html = self._simple_markdown_to_html(summary)
            summary_html = f'<div style="font-size: 16px; color: #333; line-height: 1.6; margin-bottom: 25px;">{summary_html}</div>'

            # Add highlights section for papers and use_cases/highlights for github
            highlights_html = ''
            if item_type == 'paper' and item.get('highlights'):
                highlights_html = f'<div style="font-size: 14px; color: #666; background: #f0f7ff; padding: 10px; border-radius: 4px; margin-bottom: 25px; border-left: 3px solid #3498db;"><strong>âœ¨ äº®ç‚¹:</strong><br>{item["highlights"]}</div>'
            elif item_type == 'github':
                # Use cases section
                if item.get('use_cases'):
                    highlights_html += f'<div style="font-size: 14px; color: #666; background: #f0f7ff; padding: 10px; border-radius: 4px; margin-bottom: 10px; border-left: 3px solid #3498db;"><strong>ğŸ¯ ä½¿ç”¨åœºæ™¯:</strong><br>{item["use_cases"]}</div>'
                # Highlights section
                if item.get('highlights'):
                    highlights_html += f'<div style="font-size: 14px; color: #666; background: #fff8e1; padding: 10px; border-radius: 4px; margin-bottom: 25px; border-left: 3px solid #f39c12;"><strong>âœ¨ äº®ç‚¹:</strong><br>{item["highlights"]}</div>'

            divider = '<hr style="border: 0; border-top: 1px dashed #ddd; margin: 20px 0;" />' if idx < len(items) else ""
            html_parts.append(title_html + meta_html + summary_html + highlights_html + divider)

        html_parts.append("</section>")
        return "".join(html_parts)

    # ================= Parsing =================

    def _parse_daily_report(self, report_path) -> List[Dict]:
        """è§£æ daily_report.md æ–‡ä»¶"""
        with open(report_path, "r", encoding="utf-8") as f:
            content = f.read()

        news_items = []

        header_end = content.find('### 1.')
        if header_end != -1:
            content = content[header_end + 6:]

        articles = re.split(r'\n###\s+\d+\.\s+', content)

        for article in articles:
            if not article.strip():
                continue

            title_match = re.search(r'^(.+?)\n', article)
            title = title_match.group(1).strip() if title_match else "æ— æ ‡é¢˜"

            source_match = re.search(r'\*\*æ¥æº\*\*: (.+?) \|', article)
            source = source_match.group(1).strip() if source_match else "æœªçŸ¥"

            time_match = re.search(r'\|\s*\*\*æ—¶é—´\*\*: (.+?)\n', article)
            time_str = time_match.group(1).strip() if time_match else ""

            url_match = re.search(r'\*\*é“¾æ¥\*\*: (.+?)\n', article)
            url = url_match.group(1).strip() if url_match else ""

            rating_match = re.search(r'\*\*ä»·å€¼\*\*: (.+?)\n', article)
            rating = rating_match.group(1).strip() if rating_match else ""
            rating = re.sub(r'\s*\*\*æ ‡ç­¾\*\*:.+', '', rating).strip()

            tag_match = re.search(r'\*\*æ ‡ç­¾\*\*: (.+?)\n', article)
            tag = tag_match.group(1).strip() if tag_match else ""

            # Extract one-line summary from quote block format - preserve the label
            summary_match = re.search(r'> ğŸ¯ \*\*ä¸€å¥è¯æ‘˜è¦\*\*ï¼š(.+?)(?=\n|$)', article)
            one_line_summary = summary_match.group(1).strip() if summary_match else ""

            # Extract all content after the link (from quote block to separator)
            # This captures: ä¸€å¥è¯æ‘˜è¦ + æ ¸å¿ƒæŠ€æœ¯ + å®éªŒæ•°æ® + ç‹¬å®¶æ´å¯Ÿ + ç›¸å…³èµ„æº
            # Use greedy match to capture all sections until --- or next ###
            full_content_match = re.search(
                r'> ğŸ¯ \*\*ä¸€å¥è¯æ‘˜è¦\*\*ï¼š\s*(.+)(?=\n---|\n\n###)',
                article,
                re.DOTALL
            )

            # Build the full summary with all sections
            full_summary = ""
            if full_content_match and full_content_match.group(1):
                full_content = full_content_match.group(1).strip()
                # Convert markdown headers to readable text
                # Note: The actual headers include the text after emoji, so we need to replace the entire header line
                # Handle both with and without space after emoji
                full_summary = re.sub(r'####\s+ğŸ”¹\s*æ ¸å¿ƒæŠ€æœ¯/å®ç°é€»è¾‘', '\n\n#### **æ ¸å¿ƒæŠ€æœ¯**', full_content, count=1)
                full_summary = re.sub(r'####\s+ğŸ“Š\s*å®éªŒæ•°æ®/å…³é”®ç»“è®º', '\n\n#### **å®éªŒæ•°æ®**', full_summary, count=1)
                full_summary = re.sub(r'####\s+ğŸ’¡\s*ç‹¬å®¶æ´å¯Ÿ/å±€é™æ€§', '\n\n#### **ç‹¬å®¶æ´å¯Ÿ**', full_summary, count=1)
                full_summary = re.sub(r'####\s+ğŸ”—\s*ç›¸å…³èµ„æº', '\n\n#### **ç›¸å…³èµ„æº**', full_summary, count=1)
                # Clean up list items - convert markdown lists to proper markdown format with hyphens
                full_summary = re.sub(r'^\u2022\s+', '- ', full_summary, flags=re.MULTILINE)  # bullet character
                full_summary = re.sub(r'\n\u2022\s+', '\n- ', full_summary)  # bullet character after newline
                full_summary = re.sub(r'^-\s+\*\*', '- **', full_summary, flags=re.MULTILINE)
                full_summary = re.sub(r'\n-\s+\*\*', '\n- **', full_summary)
                # Also handle items with * instead of -
                full_summary = re.sub(r'\n\*\s+\*\*', '\n* **', full_summary)
                full_summary = full_summary.strip()
                # Preserve the "ä¸€å¥è¯æ‘˜è¦ï¼š" label (full_content already has the one-line summary, so just add label)
                full_summary = f'**ä¸€å¥è¯æ‘˜è¦ï¼š**{full_summary}'

            # If no detailed content, still preserve the label
            if not full_summary and one_line_summary:
                full_summary = f'**ä¸€å¥è¯æ‘˜è¦ï¼š**{one_line_summary}'

            summary = full_summary if full_summary else (f'**ä¸€å¥è¯æ‘˜è¦ï¼š**{one_line_summary}' if one_line_summary else "")

            if title and source:
                news_items.append({
                    'title': title,
                    'source': source,
                    'time': time_str,
                    'url': url,
                    'rating': rating,
                    'tag': tag,
                    'summary': summary
                })

        return news_items

    def _parse_github_trending(self, report_path) -> List[Dict]:
        """è§£æ github_trending.md æ–‡ä»¶"""
        with open(report_path, "r", encoding="utf-8") as f:
            content = f.read()

        items = []
        articles = re.split(r'\n###\s+(\d+)\.\s+', content)

        for i in range(1, len(articles), 2):
            if i + 1 >= len(articles):
                break

            article = articles[i + 1]

            if not article.strip():
                continue

            title_match = re.search(r'^([^\n]+)', article)
            title = title_match.group(1).strip() if title_match else f"é¡¹ç›® {articles[i]}"

            lang_match = re.search(r'\*\*è¯­è¨€\*\*: ([^\n|]+)', article)
            language = lang_match.group(1).strip() if lang_match else "æœªçŸ¥"

            stars_match = re.search(r'\*\*Stars\*\*: ([\d,]+)', article)
            stars = stars_match.group(1).strip() if stars_match else ""

            today_match = re.search(r'\*\*ä»Šæ—¥\*\*: \+([\d,]+)', article)
            today_stars = today_match.group(1).strip() if today_match else ""

            url_match = re.search(r'\*\*é“¾æ¥\*\*: (.+?)\n', article)
            url = url_match.group(1).strip() if url_match else ""

            # Extract tech stack (æŠ€æœ¯æ ˆ) - stop at next section (next **)
            tech_stack_match = re.search(r'(?:\*\*æŠ€æœ¯æ ˆ\*\*|æŠ€æœ¯æ ˆ):\s*(.+?)(?:\n\*\*|\n---|\n\n###|\Z)', article, re.DOTALL)
            tech_stack = tech_stack_match.group(1).strip() if tech_stack_match else ""
            # Clean up tech_stack text
            tech_stack = re.sub(r'\n+', ' ', tech_stack).strip()

            # Extract use cases (ä½¿ç”¨åœºæ™¯)
            use_cases_match = re.search(r'\*\*ä½¿ç”¨åœºæ™¯\*\*:\s*(.+?)(?:\n\*\*äº®ç‚¹\*\*|\n---|\n\n###|\Z)', article, re.DOTALL)
            use_cases = use_cases_match.group(1).strip() if use_cases_match else ""
            # Convert bullet points to clean text
            use_cases = re.sub(r'^\s*-\s*', 'â€¢ ', use_cases, count=1)
            use_cases = re.sub(r'\n\s*-\s*', '<br>â€¢ ', use_cases)

            # Extract highlights (äº®ç‚¹)
            highlights_match = re.search(r'\*\*äº®ç‚¹\*\*:\s*(.+?)(?:\n---|\n\n###|\Z)', article, re.DOTALL)
            highlights = highlights_match.group(1).strip() if highlights_match else ""
            # Convert bullet points to clean text
            highlights = re.sub(r'^\s*-\s*', 'â€¢ ', highlights, count=1)
            highlights = re.sub(r'\n\s*-\s*', '<br>â€¢ ', highlights)

            summary_match = re.search(r'\*\*æ‘˜è¦\*\*: (.+?)(?:\n---|\n\n###|\Z|(?:\*\*æŠ€æœ¯æ ˆ\*\*|æŠ€æœ¯æ ˆ):)', article, re.DOTALL)
            summary = summary_match.group(1).strip() if summary_match else ""

            if title:
                items.append({
                    'title': title,
                    'language': language,
                    'stars': stars,
                    'today_stars': today_stars,
                    'url': url,
                    'tech_stack': tech_stack,
                    'summary': summary,
                    'use_cases': use_cases,
                    'highlights': highlights
                })

        return items

    # ================= Publishing =================

    def publish(self, content: str, title: str, **kwargs) -> Dict[str, Any]:
        """Base publish method - creates a draft."""
        item_type = kwargs.get('item_type', 'article')
        items = kwargs.get('items', [])

        if items:
            html_content = self.generate_html(items, item_type)
        else:
            html_content = content

        draft_id = self._create_draft(title, html_content, config.COVER_MEDIA_ID)

        return {
            'status': 'success',
            'draft_id': draft_id,
            'title': title
        }

    def publish_daily_report(self, report_path: str, title: str = None, target_date: str = None) -> Dict[str, Any]:
        """
        å°† daily_report.md å‘å¸ƒåˆ°è‰ç¨¿ç®±

        Args:
            report_path: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
            title: è‰ç¨¿æ ‡é¢˜
            target_date: æŠ¥å‘Šæ—¥æœŸ

        Returns:
            Result dictionary
        """
        news_items = self._parse_daily_report(report_path)

        if not news_items:
            raise Exception("âŒ æŠ¥å‘Šä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡ç« ")

        print(f"  ğŸ“Š è§£æåˆ° {len(news_items)} ç¯‡æ–‡ç« ")

        if not title:
            if not target_date:
                target_date = datetime.now().strftime("%Y-%m-%d")
            title = f"AI æ¯æ—¥æƒ…æŠ¥ | {target_date}"

        content_html = self.generate_html(news_items, 'article')
        draft_id = self._create_draft(title, content_html, config.COVER_MEDIA_ID)

        return {
            'status': 'success',
            'draft_id': draft_id,
            'title': title,
            'count': len(news_items)
        }

    def publish_github_trending(self, report_path: str, title: str = None, target_date: str = None) -> Dict[str, Any]:
        """
        å°† GitHub Trending å‘å¸ƒåˆ°è‰ç¨¿ç®±

        Args:
            report_path: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
            title: è‰ç¨¿æ ‡é¢˜
            target_date: æŠ¥å‘Šæ—¥æœŸ (YYYY-MM-DD)

        Returns:
            Result dictionary
        """
        items = self._parse_github_trending(report_path)

        if not items:
            raise Exception("âŒ æŠ¥å‘Šä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•é¡¹ç›®")

        print(f"  ğŸ“Š è§£æåˆ° {len(items)} ä¸ªé¡¹ç›®")

        if not title:
            if not target_date:
                # å°è¯•ä» markdown æ–‡ä»¶ä¸­æå–æ—¥æœŸ
                target_date = self._extract_date_from_markdown(report_path)
            title = f"GitHub çƒ­é—¨é¡¹ç›® | {target_date}"

        content_html = self.generate_html(items, 'github')
        draft_id = self._create_draft(title, content_html, config.COVER_MEDIA_ID)

        return {
            'status': 'success',
            'draft_id': draft_id,
            'title': title,
            'count': len(items)
        }

    def publish_paper(self, paper_data: Dict) -> Dict[str, Any]:
        """
        å‘å¸ƒå•ç¯‡è®ºæ–‡åˆ°è‰ç¨¿ç®±

        Args:
            paper_data: Paper analysis data (æ”¯æŒä¸¤ç§æ ¼å¼)
                - ç®€åŒ–æ ¼å¼: {'title', 'arxiv_id', 'org', 'tags', 'score', 'upvotes', 'stars', 'analysis'}
                - å®Œæ•´æ ¼å¼: ä» _parse_analysis_file() è¿”å›çš„å­—å…¸

        Returns:
            Result dictionary
        """
        # åˆ¤æ–­æ˜¯ç®€åŒ–æ ¼å¼è¿˜æ˜¯å®Œæ•´æ ¼å¼
        if 'body' in paper_data:
            # å®Œæ•´æ ¼å¼ - ä½¿ç”¨ç²¾ç¾ HTML
            title = paper_data.get('title', 'è®ºæ–‡åˆ†æ')
            if len(title) > 50:
                title = title[:47] + '...'
            content_html = self._generate_paper_html(paper_data)
        else:
            # ç®€åŒ–æ ¼å¼ - ä½¿ç”¨ç®€å• HTML
            title = paper_data.get('title', 'è®ºæ–‡åˆ†æ')
            if len(title) > 50:
                title = title[:47] + '...'
            content_html = self._generate_simple_paper_html(paper_data)

        draft_id = self._create_draft(title, content_html, config.COVER_MEDIA_ID)

        return {
            'status': 'success',
            'draft_id': draft_id,
            'title': title
        }

    def publish_papers_summary(self, report_path: str, title: str = None, target_date: str = None) -> Dict[str, Any]:
        """
        å°†è®ºæ–‡æ±‡æ€»å‘å¸ƒåˆ°è‰ç¨¿ç®±

        Args:
            report_path: è®ºæ–‡æ±‡æ€»æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
            title: è‰ç¨¿æ ‡é¢˜
            target_date: æŠ¥å‘Šæ—¥æœŸ (YYYY-MM-DD)

        Returns:
            Result dictionary
        """
        items = self._parse_papers_summary(report_path)

        if not items:
            raise Exception("âŒ æŠ¥å‘Šä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è®ºæ–‡")

        print(f"  ğŸ“Š è§£æåˆ° {len(items)} ç¯‡è®ºæ–‡")

        if not title:
            if not target_date:
                target_date = self._extract_date_from_markdown(report_path)
            title = f"æ¯æ—¥è®ºæ–‡æ±‡æ€» | {target_date}"

        content_html = self.generate_html(items, 'paper')
        draft_id = self._create_draft(title, content_html, config.COVER_MEDIA_ID)

        return {
            'status': 'success',
            'draft_id': draft_id,
            'title': title,
            'count': len(items)
        }

    def _parse_papers_summary(self, report_path: str) -> List[Dict]:
        """è§£æ papers_summary.md æ–‡ä»¶"""
        with open(report_path, "r", encoding="utf-8") as f:
            content = f.read()

        items = []
        # Split by "###" followed by optional emoji/chars, then number and dot
        sections = re.split(r'\n###\s*[^0-9]*?(\d+)\.\s+', content)

        for i in range(1, len(sections), 2):
            if i + 1 >= len(sections):
                break

            section = sections[i + 1]
            if not section.strip():
                continue

            # Extract title (first non-empty line after the header)
            lines = section.split('\n')
            title = ""
            for line in lines:
                line = line.strip()
                if line and not line.startswith('**'):
                    title = line
                    break

            # Extract URL from **è®ºæ–‡é“¾æ¥**: [url](url) format
            url_match = re.search(r'\*\*è®ºæ–‡é“¾æ¥\*\*:\s*\[([^\]]+)\]\(([^)]+)\)', section)
            url = url_match.group(2).strip() if url_match else ""

            # Extract arXiv ID from URL
            arxiv_id = ""
            if url and 'arxiv.org/abs/' in url:
                arxiv_id = url.split('arxiv.org/abs/')[-1].split('/')[0]
            elif url:
                # Try to extract from URL as fallback
                arxiv_match = re.search(r'(\d+\.\d+)', url)
                arxiv_id = arxiv_match.group(1) if arxiv_match else ""

            # Extract organization
            org_match = re.search(r'\*\*ç»„ç»‡\*\*:\s*(.+?)\n', section)
            org = org_match.group(1).strip() if org_match else ""

            # Extract score
            score_match = re.search(r'\*\*å¾—åˆ†\*\*:\s*([\d.]+)', section)
            score = score_match.group(1).strip() if score_match else ""

            # Extract tags
            tags_match = re.search(r'\*\*æ ‡ç­¾\*\*:\s*(.+?)\n', section)
            tags = tags_match.group(1).strip() if tags_match else ""

            # Extract upvotes and stars (format: **Upvotes**: 15 | **Stars**: 42)
            upvotes_match = re.search(r'\*\*Upvotes\*\*:\s*(\d+)', section)
            upvotes = upvotes_match.group(1).strip() if upvotes_match else ""

            stars_match = re.search(r'\|\s*\*\*Stars\*\*:\s*(\d+)', section)
            stars = stars_match.group(1).strip() if stars_match else ""

            # Extract summary (after **æ‘˜è¦** until **äº®ç‚¹** or --- or end)
            summary_match = re.search(r'\*\*æ‘˜è¦\*\*:\s*(.+?)(?:\n\*\*äº®ç‚¹\*\*|\n---|\n\n###|\Z)', section, re.DOTALL)
            summary = summary_match.group(1).strip() if summary_match else ""
            summary = re.sub(r'\n+', ' ', summary)  # Convert newlines to spaces
            summary = summary[:500] + "..." if len(summary) > 500 else summary  # Limit length

            # Extract highlights (äº®ç‚¹)
            highlights_match = re.search(r'\*\*äº®ç‚¹\*\*:\s*(.+?)(?:\n---|\n\n###|\Z)', section, re.DOTALL)
            highlights = highlights_match.group(1).strip() if highlights_match else ""
            # Convert bullet points to clean text - handle both leading bullets and bullets after newlines
            highlights = re.sub(r'^\s*-\s*', 'â€¢ ', highlights, count=1)  # First bullet
            highlights = re.sub(r'\n\s*-\s*', '<br>â€¢ ', highlights)  # Subsequent bullets
            highlights = highlights[:300] + "..." if len(highlights) > 300 else highlights

            if title or arxiv_id:
                items.append({
                    'title': title or f"Paper {arxiv_id}",
                    'arxiv_id': arxiv_id,
                    'org': org,
                    'score': score,
                    'tags': tags,
                    'upvotes': upvotes,
                    'stars': stars,
                    'url': url,
                    'summary': summary,
                    'highlights': highlights
                })

        return items

    def _parse_analysis_file(self, analysis_path: str) -> Dict[str, Any]:
        """
        è§£æå•ç¯‡è®ºæ–‡çš„åˆ†ææ–‡ä»¶

        Args:
            analysis_path: åˆ†ææ–‡ä»¶è·¯å¾„

        Returns:
            åŒ…å«è®ºæ–‡ä¿¡æ¯çš„å­—å…¸
        """
        with open(analysis_path, "r", encoding="utf-8") as f:
            content = f.read()

        # æå–æ ‡é¢˜
        h1_matches = list(re.finditer(r'^#\s+(.+)$', content, re.MULTILINE))
        # ç¬¬ä¸€ä¸ª h1 æ˜¯è‹±æ–‡åŸæ ‡é¢˜ï¼ˆç”¨äº HTML æ­£æ–‡ï¼‰
        if h1_matches:
            english_title = h1_matches[0].group(1).strip()
        else:
            english_title = Path(analysis_path).stem

        # ç¬¬äºŒä¸ª h1 æ˜¯ä¸­æ–‡æ ‡é¢˜ï¼ˆç”¨äºè‰ç¨¿æ ‡é¢˜ï¼‰
        if len(h1_matches) >= 2:
            title = h1_matches[1].group(1).strip()  # ç¬¬äºŒä¸ª h1
        elif h1_matches:
            title = english_title  # åªæœ‰ä¸€ä¸ª h1ï¼Œç”¨ç¬¬ä¸€ä¸ª
        else:
            title = Path(analysis_path).stem

        # æå–è®ºæ–‡åŸæ ‡é¢˜ (ä»ç¬¬ä¸€è¡Œçš„ã€Šã€‹ä¸­æå–)
        paper_title_match = re.search(r'ã€Š(.+?)ã€‹', content.split('---')[0] if '---' in content else content)
        paper_title = paper_title_match.group(1) if paper_title_match else ''

        # æå–å…ƒæ•°æ® (arXiv ID, ç»„ç»‡, Stars, Upvotes, å¾—åˆ†, æ ‡ç­¾)
        arxiv_id_match = re.search(r'\*\*arXiv ID\*\*:\s*(.+)', content)
        # Extract arXiv ID from markdown link format
        if arxiv_id_match:
            arxiv_id_text = arxiv_id_match.group(1).strip()
            arxiv_id_link = re.search(r'\[([^\]]+)\]\(([^)]+)\)', arxiv_id_text)
            if arxiv_id_link:
                arxiv_id = arxiv_id_link.group(1)  # Use the display text
                arxiv_url = arxiv_id_link.group(2)
            else:
                arxiv_id = arxiv_id_text
                arxiv_url = f"https://arxiv.org/abs/{arxiv_id_text}"
        else:
            arxiv_id = ''
            arxiv_url = ''

        org = re.search(r'\*\*ç»„ç»‡\*\*:\s*(.+)', content)
        # Extract stars from format: **Upvotes**: 15 | **Stars**: 42
        upvotes = re.search(r'\*\*Upvotes\*\*:\s*(\d+)', content)
        stars = re.search(r'\|\s*\*\*Stars\*\*:\s*(\d+)', content)
        score = re.search(r'\*\*å¾—åˆ†\*\*:\s*([\d.]+)', content)
        tags = re.search(r'\*\*æ ‡ç­¾\*\*:\s*(.+)', content)

        # æå–æ­£æ–‡ (å»é™¤ --- ä¹‹åçš„å†…å®¹)
        parts = content.split('---', 1)
        body = parts[1].strip() if len(parts) > 1 else content

        # æå–ç¬¬ä¸€æ®µä½œä¸ºæ‘˜è¦ï¼ˆå»é™¤ç©ºè¡Œåç¬¬ä¸€ä¸ªéæ ‡é¢˜æ®µè½ï¼‰
        intro_match = re.search(r'^(?!#)(?!<)(.+)$', body, re.MULTILINE)
        intro = intro_match.group(1).strip() if intro_match else ''
        # å»é™¤ intro ä¸­çš„ markdown æ ¼å¼
        intro = re.sub(r'\*\*(.+?)\*\*', r'\1', intro)
        intro = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', intro)

        return {
            'title': title,
            'english_title': english_title,
            'paper_title': paper_title,
            'intro': intro,
            'arxiv_id': arxiv_id,
            'arxiv_url': arxiv_url,
            'org': org.group(1).strip() if org else '',
            'stars': stars.group(1).strip() if stars else '',
            'upvotes': upvotes.group(1).strip() if upvotes else '',
            'score': score.group(1).strip() if score else '',
            'tags': tags.group(1).strip() if tags else '',
            'body': body
        }

    def _simple_markdown_to_html(self, text: str) -> str:
        """
        è½»é‡çº§ Markdown è½¬ HTMLï¼Œç”¨äº summary æ ¼å¼åŒ–

        å¤„ç†:
        - å››çº§æ ‡é¢˜ (####)
        - åŠ ç²— (**text**)
        - é“¾æ¥ ([text](url))
        - åµŒå¥—åˆ—è¡¨é¡¹ (- æˆ– * å¼€å¤´, æ ¹æ®ç¼©è¿›åˆ¤æ–­å±‚çº§)
        - æ®µè½é—´è·
        """
        if not text:
            return ""

        lines = text.split('\n')
        result = []

        i = 0
        while i < len(lines):
            line = lines[i]
            stripped = line.strip()

            # è·³è¿‡ç©ºè¡Œ
            if not stripped:
                result.append('<br>')
                i += 1
                continue

            # å¤„ç†å››çº§æ ‡é¢˜
            if stripped.startswith('#### '):
                content = stripped[5:].strip()
                # å¤„ç†æ ‡é¢˜ä¸­çš„åŠ ç²—
                content = re.sub(r'\*\*(.+?)\*\*', r'<strong style="color: #2c3e50; font-weight: 600;">\1</strong>', content)
                result.append(f'<h4 style="font-size: 16px; font-weight: bold; color: #555; text-align: center; margin: 20px 0 12px; padding: 8px 0; border-top: 1px solid #e0e0e0; border-bottom: 1px solid #e0e0e0;">{content}</h4>')
                i += 1
                continue

            # æ£€æµ‹åˆ—è¡¨é¡¹ (æ”¯æŒ markdown æ ¼å¼å’Œ bullet å­—ç¬¦)
            list_match = re.match(r'^(\s*)([-*]|\u2022)\s+', line)
            if list_match:
                # æ”¶é›†è¿ç»­çš„åˆ—è¡¨é¡¹å¹¶æ„å»ºåµŒå¥—ç»“æ„
                list_items = []
                base_indent = None

                while i < len(lines):
                    line = lines[i]
                    list_match = re.match(r'^(\s*)([-*]|\u2022)\s+', line)

                    if not list_match:
                        break

                    indent_str = list_match.group(1)
                    content_start = list_match.end()
                    content = line[content_start:].rstrip()

                    # è®¡ç®—ç¼©è¿›å±‚çº§ (æ¯4ä¸ªç©ºæ ¼ä¸ºä¸€çº§)
                    indent = len(indent_str)
                    if base_indent is None:
                        base_indent = indent

                    # è®¡ç®—ç›¸å¯¹å±‚çº§ (0-based)
                    level = 0
                    if indent > base_indent:
                        level = (indent - base_indent) // 4 + 1

                    # å¤„ç†å†…è”æ ¼å¼
                    content = re.sub(r'\*\*(.+?)\*\*([ï¼š:ã€,ï¼Œ.ã€‚ï¼›;])', r'<strong style="color: #2c3e50; font-weight: 600;">\1\2</strong>', content)
                    content = re.sub(r'\*\*([^*]+?)\*\*', r'<strong style="color: #2c3e50; font-weight: 600;">\1</strong>', content)
                    content = re.sub(r'\[([^\]]+)\]\(([^)]+)\)', r'<a href="\2" style="color: #3498db;">\1</a>', content)

                    list_items.append({'level': level, 'content': content})
                    i += 1

                    # æ£€æŸ¥ä¸‹ä¸€è¡Œæ˜¯å¦æ˜¯åˆ—è¡¨çš„ç»­è¡Œï¼ˆç¼©è¿›æ›´å¤šä¸”ä¸æ˜¯æ–°çš„åˆ—è¡¨é¡¹ï¼‰
                    if i < len(lines):
                        next_line = lines[i]
                        if next_line.strip() and not re.match(r'^\s*[-*]|\u2020\s+', next_line):
                            next_indent = len(next_line) - len(next_line.lstrip())
                            if next_indent > indent:
                                # è¿™æ˜¯ç»­è¡Œï¼Œæ·»åŠ åˆ°å½“å‰é¡¹
                                continuation = next_line.rstrip()
                                continuation = re.sub(r'\*\*(.+?)\*\*([ï¼š:ã€,ï¼Œ.ã€‚ï¼›;])', r'<strong style="color: #2c3e50; font-weight: 600;">\1\2</strong>', continuation)
                                continuation = re.sub(r'\*\*([^*]+?)\*\*', r'<strong style="color: #2c3e50; font-weight: 600;">\1</strong>', continuation)
                                continuation = re.sub(r'\[([^\]]+)\]\(([^)]+)\)', r'<a href="\2" style="color: #3498db;">\1</a>', continuation)
                                list_items[-1]['content'] += f' {continuation}'
                                i += 1

                # ç”ŸæˆåµŒå¥—åˆ—è¡¨ HTML
                result.append(self._render_nested_list(list_items))
                continue

            # å¤„ç†æ™®é€šæ®µè½
            processed = re.sub(r'\*\*(.+?)\*\*([ï¼š:ã€,ï¼Œ.ã€‚ï¼›;])', r'<strong style="color: #2c3e50; font-weight: 600;">\1\2</strong>', stripped)
            processed = re.sub(r'\*\*([^*]+?)\*\*', r'<strong style="color: #2c3e50; font-weight: 600;">\1</strong>', processed)
            processed = re.sub(r'\[([^\]]+)\]\(([^)]+)\)', r'<a href="\2" style="color: #3498db;">\1</a>', processed)
            result.append(f'<p style="margin: 8px 0; line-height: 1.6;">{processed}</p>')
            i += 1

        return ''.join(result)

    def _render_nested_list(self, items: List[Dict]) -> str:
        """
        æ¸²æŸ“åµŒå¥—åˆ—è¡¨ç»“æ„ä¸º HTML

        Args:
            items: åˆ—è¡¨é¡¹å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« 'level' å’Œ 'content'

        Returns:
            HTML å­—ç¬¦ä¸²
        """
        if not items:
            return ''

        def build_tree(items):
            """å°†æ‰å¹³åˆ—è¡¨é¡¹æ ‘ç»“æ„"""
            if not items:
                return []

            root = []
            stack = [(root, -1)]  # (parent_list, level)

            for item in items:
                level = item['level']
                content = item['content']

                node = {'content': content, 'children': []}

                # æ‰¾åˆ°æ­£ç¡®çš„çˆ¶çº§
                while stack and stack[-1][1] >= level:
                    stack.pop()

                if stack:
                    stack[-1][0].append(node)
                else:
                    root.append(node)

                # å°†æ­¤èŠ‚ç‚¹ä½œä¸ºå¯èƒ½çš„çˆ¶çº§
                stack.append((node['children'], level))

            return root

        def render_items(nodes, is_root=True):
            """é€’å½’æ¸²æŸ“åˆ—è¡¨é¡¹"""
            html = []
            for node in nodes:
                content = node['content']
                children = node['children']

                if children:
                    children_html = render_items(children, is_root=False)
                    # å¦‚æœå†…å®¹åªæœ‰å†’å·æˆ–ä¸ºç©ºï¼Œåªæ¸²æŸ“å­åˆ—è¡¨
                    if not content or content in [':', 'ï¼š']:
                        html.append(f'<li style="margin: 5px 0; line-height: 1.6;">{children_html}</li>')
                    else:
                        html.append(f'<li style="margin: 5px 0; line-height: 1.6;">{content}<ul style="margin: 5px 0; padding-left: 20px;">{children_html}</ul></li>')
                else:
                    html.append(f'<li style="margin: 5px 0; line-height: 1.6;">{content}</li>')
            return ''.join(html)

        tree = build_tree(items)
        items_html = render_items(tree)
        return f'<ul style="margin: 5px 0; padding-left: 20px;">{items_html}</ul>'

    def _markdown_to_html(self, markdown_text: str) -> str:
        """
        å°† Markdown è½¬æ¢ä¸ºå¾®ä¿¡å…¬ä¼—å· HTML - CSS é©±åŠ¨ç‰ˆæœ¬
        ä½¿ç”¨ mistune æ¸²æŸ“ï¼Œé€šè¿‡ premailer å°† CSS è½¬ä¸ºå†…è”æ ·å¼
        """
        # è·å–è§£æå™¨ï¼ˆå¸¦ç¼“å­˜ï¼‰
        if not hasattr(self, '_markdown_parser'):
            self._markdown_parser = _create_wechat_markdown_parser()

        # é¢„å¤„ç†ï¼šè·³è¿‡ç¬¬ä¸€ä¸ª h1ï¼ˆå› ä¸ºå·²åœ¨æ ‡é¢˜å¤„æ˜¾ç¤ºï¼‰
        lines = markdown_text.split('\n')
        first_h1_skipped = False
        processed_lines = []

        for line in lines:
            # è·³è¿‡ --- åˆ†éš”çº¿
            if line.strip() == '---':
                continue
            # è·³è¿‡ç¬¬ä¸€ä¸ª h1
            if not first_h1_skipped and re.match(r'^#\s+', line):
                first_h1_skipped = True
                continue
            processed_lines.append(line)

        processed_text = '\n'.join(processed_lines)
        return self._markdown_parser(processed_text)

    def _generate_paper_html(self, paper_data: Dict) -> str:
        """
        ç”Ÿæˆå•ç¯‡è®ºæ–‡çš„ç²¾ç¾ HTMLï¼ˆCSS é©±åŠ¨ï¼‰

        Args:
            paper_data: è®ºæ–‡æ•°æ®å­—å…¸

        Returns:
            HTML å­—ç¬¦ä¸²
        """
        english_title = paper_data.get('english_title', paper_data.get('title', ''))

        html_parts = []
        html_parts.append(f'<h1>{english_title}</h1>')

        # å…ƒä¿¡æ¯
        meta_parts = []
        if paper_data.get('arxiv_url'):
            meta_parts.append(f'ğŸ“„ è®ºæ–‡ï¼š<a href="{paper_data["arxiv_url"]}">{paper_data["arxiv_url"]}</a>')
        elif paper_data.get('arxiv_id'):
            url = f"https://arxiv.org/abs/{paper_data['arxiv_id']}"
            meta_parts.append(f'ğŸ“„ è®ºæ–‡ï¼š<a href="{url}">{url}</a>')
        if paper_data.get('org'):
            meta_parts.append(f'ğŸ”¬ <strong>æœºæ„ï¼š</strong>{paper_data["org"]}')
        if paper_data.get('tags'):
            meta_parts.append(f'ğŸ·ï¸ <strong>æ ‡ç­¾ï¼š</strong>{paper_data["tags"]}')

        stats = []
        if paper_data.get('score'):
            stats.append(f'ğŸ“Š {paper_data["score"]}')
        if paper_data.get('upvotes'):
            stats.append(f'ğŸ‘ {paper_data["upvotes"]}')
        if paper_data.get('stars'):
            stats.append(f'ğŸŒŸ {paper_data["stars"]}')

        if meta_parts or stats:
            html_parts.append('<blockquote>')
            html_parts.append('<p>' + '<br>'.join(meta_parts) + '</p>')
            if stats:
                html_parts.append('<p>' + ' | '.join(stats) + '</p>')
            html_parts.append('</blockquote>')

        if paper_data.get('intro'):
            html_parts.append(f'<p><em>{paper_data["intro"]}</em></p>')

        # æ­£æ–‡
        body_html = self._markdown_to_html(paper_data['body'])
        html_parts.append(body_html)

        full_html = '<section>' + '\n'.join(html_parts) + '</section>'
        return _apply_inline_styles(full_html)

    def _generate_simple_paper_html(self, paper: Dict) -> str:
        """
        ç”Ÿæˆç®€åŒ–ç‰ˆè®ºæ–‡ HTMLï¼ˆCSS é©±åŠ¨ï¼‰

        Args:
            paper: ç®€åŒ–è®ºæ–‡æ•°æ®

        Returns:
            HTML å­—ç¬¦ä¸²
        """
        html_parts = []
        html_parts.append(f'<h1>{paper.get("title", "æœªçŸ¥")}</h1>')

        meta_parts = []
        if paper.get('arxiv_id'):
            url = f"https://arxiv.org/abs/{paper['arxiv_id']}"
            meta_parts.append(f'ğŸ“„ <strong>è®ºæ–‡ï¼š</strong><a href="{url}">{paper["arxiv_id"]}</a>')
        if paper.get('org'):
            meta_parts.append(f'ğŸ”¬ <strong>æœºæ„ï¼š</strong>{paper["org"]}')
        if paper.get('tags'):
            meta_parts.append(f'ğŸ·ï¸ <strong>æ ‡ç­¾ï¼š</strong>{paper["tags"]}')

        if meta_parts:
            html_parts.append('<blockquote><p>' + '<br>'.join(meta_parts) + '</p></blockquote>')

        analysis = paper.get('analysis', '')
        if analysis:
            html_parts.append(f'<p>{analysis}</p>')

        full_html = '<section>' + '\n'.join(html_parts) + '</section>'
        return _apply_inline_styles(full_html)

    def publish_single_paper(self, analysis_path: str) -> Dict[str, Any]:
        """
        å‘å¸ƒå•ç¯‡è®ºæ–‡åˆ†æåˆ°è‰ç¨¿ç®±

        Args:
            analysis_path: åˆ†ææ–‡ä»¶è·¯å¾„

        Returns:
            ç»“æœå­—å…¸ï¼ŒåŒ…å« draft_id å’Œ title
        """
        paper_data = self._parse_analysis_file(analysis_path)

        print(f"ğŸ“„ æ­£åœ¨å‘å¸ƒ: {paper_data['title']}")

        # ç”Ÿæˆæ ‡é¢˜ (å»é™¤è¿‡é•¿æ ‡é¢˜)
        title = paper_data['title']
        if len(title) > 50:
            title = title[:47] + '...'

        # ç”Ÿæˆ HTML
        content_html = self._generate_paper_html(paper_data)

        # åˆ›å»ºè‰ç¨¿
        draft_id = self._create_draft(title, content_html, config.COVER_MEDIA_ID)

        return {
            'status': 'success',
            'draft_id': draft_id,
            'title': title
        }

    def publish_all_papers(self, date_str: str) -> List[Dict[str, Any]]:
        """
        å‘å¸ƒæŸä¸€å¤©çš„æ‰€æœ‰è®ºæ–‡åˆ†æ

        Args:
            date_str: æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)

        Returns:
            ç»“æœåˆ—è¡¨
        """
        project_root = Path(__file__).parent.parent.parent
        output_dir = project_root / "output" / date_str

        if not output_dir.exists():
            raise Exception(f"è¾“å‡ºç›®å½•ä¸å­˜åœ¨: {output_dir}")

        # æ‰¾åˆ°æ‰€æœ‰åˆ†ææ–‡ä»¶ (æ’é™¤ _summary.md)
        analysis_files = [
            f for f in output_dir.glob("papers/papers_note_*.md")
            if not f.name.startswith('_')
        ]

        if not analysis_files:
            print(f"âŒ æ²¡æœ‰æ‰¾åˆ°åˆ†ææ–‡ä»¶: {output_dir}")
            return []

        print(f"ğŸ“Š æ‰¾åˆ° {len(analysis_files)} ç¯‡è®ºæ–‡åˆ†æ")

        results = []
        for i, analysis_file in enumerate(analysis_files, 1):
            print(f"\n[{i}/{len(analysis_files)}] {analysis_file.name}")
            try:
                result = self.publish_single_paper(str(analysis_file))
                result['file'] = analysis_file.name
                results.append(result)
                print(f"  âœ… æˆåŠŸ - Media ID: {result['draft_id']}")
            except Exception as e:
                results.append({
                    'file': analysis_file.name,
                    'error': str(e),
                    'status': 'failed'
                })
                print(f"  âŒ å¤±è´¥ - {e}")

        return results

    def _extract_date_from_markdown(self, report_path: str) -> str:
        """ä» markdown æ–‡ä»¶æ ‡é¢˜è¡Œä¸­æå–æ—¥æœŸ"""
        with open(report_path, "r", encoding="utf-8") as f:
            first_line = f.readline()
        # åŒ¹é…æ ¼å¼: # GitHub çƒ­é—¨é¡¹ç›® | 2026-02-01 æˆ– # æ¯æ—¥è®ºæ–‡æ±‡æ€» | 2026-02-01
        match = re.search(r'\| (\d{4}-\d{2}-\d{2})', first_line)
        if match:
            return match.group(1)
        return datetime.now().strftime('%Y-%m-%d')

    def _create_draft(self, title: str, content: str, thumb_id: str) -> str:
        """åˆ›å»ºè‰ç¨¿"""
        url = f"https://api.weixin.qq.com/cgi-bin/draft/add?access_token={self.token}"
        data = {
            "articles": [{
                "title": title,
                "author": "AI Report",
                "digest": "AI æƒ…æŠ¥æ‘˜è¦...",
                "content": content,
                "thumb_media_id": thumb_id
            }]
        }

        resp = requests.post(
            url,
            data=json.dumps(data, ensure_ascii=False).encode('utf-8'),
            headers={'Content-Type': 'application/json; charset=utf-8'},
            proxies=getattr(config, 'PROXIES', None)
        )

        result = resp.json()

        if 'media_id' not in result:
            raise Exception(f"âŒ è‰ç¨¿åˆ›å»ºå¤±è´¥: {result}")

        return result['media_id']
