#!/usr/bin/env python3
"""
Paper Analysis Pipeline

å®Œæ•´æµç¨‹:
1. è·å– HuggingFace Daily Papers
2. ä½¿ç”¨ PaperRanker æ’åº
3. ä¸‹è½½è®ºæ–‡ (ä»ç¬¬1ç¯‡åˆ°æœ€åä¸€ç¯‡ Frontier Labï¼Œæœ€å°‘1-3ç¯‡)
4. é€ä¸ªåˆ†æè®ºæ–‡ (ä½¿ç”¨ Gemini)
5. è¾“å‡ºåˆ° output/{date}/ æ–‡ä»¶å¤¹
"""
import os
import sys
import time
import requests
from datetime import date, timedelta
from pathlib import Path

# æ¸…é™¤ä»£ç†ç¯å¢ƒå˜é‡ (é¿å…å½±å“ requests)
for proxy_var in ['HTTP_PROXY', 'HTTPS_PROXY', 'http_proxy', 'https_proxy']:
    os.environ.pop(proxy_var, None)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.paper_ranker import PaperRanker
from src.gemini_client import GeminiClient


def load_prompt(prompt_path: str = None) -> str:
    """åŠ è½½åˆ†ææç¤ºè¯"""
    if prompt_path is None:
        prompt_path = Path(__file__).parent.parent / "prompt.md"
    return Path(prompt_path).read_text(encoding='utf-8')


def download_pdf(arxiv_id: str, title: str, save_dir: Path) -> Path | None:
    """ä¸‹è½½å•ç¯‡è®ºæ–‡ PDF"""
    pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"

    # æ¸…ç†æ–‡ä»¶å
    import re
    safe_title = re.sub(r'[\\/*?:"<>|]', "", title).strip()
    filename = f"{arxiv_id}_{safe_title[:80]}.pdf"
    file_path = save_dir / filename

    if file_path.exists():
        print(f"    [å·²å­˜åœ¨] {filename}")
        return file_path

    try:
        print(f"    [ä¸‹è½½ä¸­] {filename}...")
        r = requests.get(
            pdf_url,
            headers={"User-Agent": "Mozilla/5.0"},
            stream=True,
            timeout=60
        )
        if r.status_code == 200:
            with open(file_path, 'wb') as f:
                for chunk in r.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            file_size = file_path.stat().st_size
            print(f"    [å®Œæˆ] {file_size:,} bytes")
            time.sleep(3)  # ArXiv é™åˆ¶
        else:
            print(f"    [å¤±è´¥] HTTP {r.status_code}")
            return None
    except Exception as e:
        print(f"    [é”™è¯¯] {e}")
        return None

    return file_path


def analyze_paper(pdf_path: Path, prompt: str, client: GeminiClient) -> str:
    """åˆ†æå•ç¯‡è®ºæ–‡"""
    print(f"\n  åˆ†æä¸­: {pdf_path.name}")
    result = client.upload_and_analyze(str(pdf_path), prompt)
    return result


def save_analysis(paper_info: dict, analysis: str, output_dir: Path) -> Path:
    """ä¿å­˜åˆ†æç»“æœ"""
    import re
    arxiv_id = paper_info.get("arxiv_id", "unknown")
    safe_title = re.sub(r'[\\/*?:"<>|]', "", paper_info.get('title', 'unknown')).strip()[:50]
    filename = f"{arxiv_id}_{safe_title}_analysis.md"
    output_path = output_dir / filename

    # æ„å»ºå®Œæ•´è¾“å‡º
    content = f"# {paper_info.get('title', 'Unknown')}\n\n"
    content += f"**arXiv ID**: {arxiv_id}\n"
    content += f"**ç»„ç»‡**: {paper_info.get('org', 'Unknown')}\n"
    content += f"**GitHub Stars**: {paper_info.get('stars', 0)}\n"
    content += f"**Upvotes**: {paper_info.get('upvotes', 0)}\n"
    content += f"**å¾—åˆ†**: {paper_info.get('score', 0)}\n"
    content += f"**æ ‡ç­¾**: {paper_info.get('reasons', 'N/A')}\n\n"
    content += "---\n\n"
    content += analysis

    output_path.write_text(content, encoding='utf-8')
    print(f"  [ä¿å­˜] {filename}")
    return output_path


def run_pipeline(
    target_date: str,
    min_papers: int = 3,
    max_papers: int = 20,
    enable_topic_bonus: bool = False,
    dry_run: bool = False
):
    """
    è¿è¡Œå®Œæ•´ pipeline

    Args:
        target_date: ç›®æ ‡æ—¥æœŸ (YYYY-MM-DD)
        min_papers: æœ€å°‘ä¸‹è½½æ•°
        max_papers: æœ€å¤šä¸‹è½½æ•° (é˜²æ­¢ä¸‹è½½å¤ªå¤š)
        enable_topic_bonus: æ˜¯å¦å¯ç”¨å…´è¶£åŠ æˆ
        dry_run: åªæ˜¾ç¤ºä¸å®é™…æ‰§è¡Œ (æµ‹è¯•ç”¨)
    """
    print("=" * 80)
    print(f"Paper Analysis Pipeline - {target_date}")
    print("=" * 80)

    # 1. è·å–å¹¶æ’åºè®ºæ–‡
    print(f"\n[1/5] è·å–è®ºæ–‡åˆ—è¡¨...")
    url = f"https://huggingface.co/api/daily_papers?date={target_date}"
    resp = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})

    if resp.status_code != 200:
        print(f"é”™è¯¯: API è¯·æ±‚å¤±è´¥ ({resp.status_code})")
        return

    papers = resp.json()
    if not papers:
        print(f"æ— æ•°æ®: {target_date}")
        return

    print(f"  è·å–åˆ° {len(papers)} ç¯‡è®ºæ–‡")

    # 2. æ’åº
    print(f"\n[2/5] è®ºæ–‡æ’åº...")
    ranker = PaperRanker(enable_topic_bonus=enable_topic_bonus)
    ranked = ranker.rank_papers(papers)

    # æ‰¾åˆ°æœ€åä¸€ç¯‡ Frontier Lab è®ºæ–‡çš„ä½ç½®
    last_frontier_idx = 0
    for i, p in enumerate(ranked):
        reasons = p.get("rank_reasons", "")
        if "Super Lab" in reasons or "Frontier Lab" in reasons:
            last_frontier_idx = i

    # ç»Ÿè®¡ Frontier Lab æ•°é‡
    frontier_count = sum(1 for p in ranked[:last_frontier_idx+1]
                          if "Super Lab" in p.get("rank_reasons", "") or "Frontier Lab" in p.get("rank_reasons", ""))

    # ç¡®å®šä¸‹è½½æ•°é‡: ä»ç¬¬1ç¯‡åˆ°æœ€åä¸€ç¯‡ Frontier Lab
    # è‡³å°‘ min_papers ç¯‡ï¼Œæœ€å¤š max_papers ç¯‡
    download_count = max(min_papers, last_frontier_idx + 1)
    download_count = min(download_count, max_papers)

    papers_to_download = ranked[:download_count]
    print(f"  å°†ä¸‹è½½: ç¬¬1ç¯‡ â†’ ç¬¬{last_frontier_idx+1}ç¯‡ (å…± {download_count} ç¯‡)")
    print(f"  å…¶ä¸­ Frontier Labs: {frontier_count} ç¯‡")

    # æ˜¾ç¤ºå°†è¦ä¸‹è½½çš„è®ºæ–‡åˆ—è¡¨
    print(f"\n  å°†åˆ†æçš„è®ºæ–‡:")
    for i, p in enumerate(papers_to_download, 1):
        title = p.get("title", "")[:55]
        score = p.get("rank_score", 0)
        reasons = p.get("rank_reasons", "")
        marker = "ğŸ”¥" if ("Super Lab" in reasons or "Frontier Lab" in reasons) else "  "
        print(f"    {i:2d}. [{marker}] {score:6.2f} | {title}... | {reasons}")

    if dry_run:
        print("\n[DRY RUN] è·³è¿‡å®é™…ä¸‹è½½å’Œåˆ†æ")
        return

    # 3. å‡†å¤‡ç›®å½•
    project_root = Path(__file__).parent.parent
    download_dir = project_root / "HF_Paper_Downloads" / target_date
    download_dir.mkdir(parents=True, exist_ok=True)

    output_dir = project_root / "output" / target_date
    output_dir.mkdir(parents=True, exist_ok=True)

    # 4. ä¸‹è½½ PDF
    print(f"\n[3/5] ä¸‹è½½ PDF...")
    downloaded_files = []

    for i, p in enumerate(papers_to_download, 1):
        paper = p.get("paper", {})
        arxiv_id = paper.get("id", "")
        title = p.get("title", "")
        score = p.get("rank_score", 0)
        reasons = p.get("rank_reasons", "")

        print(f"\n  [{i}/{download_count}] Score: {score} | {title[:60]}...")
        print(f"     Tags: {reasons}")

        pdf_path = download_pdf(arxiv_id, title, download_dir)
        if pdf_path:
            downloaded_files.append({
                "pdf_path": pdf_path,
                "arxiv_id": arxiv_id,
                "title": title,
                "org": p.get("organization", {}).get("fullname", ""),
                "stars": paper.get("githubStars", 0),
                "upvotes": paper.get("upvotes", 0),
                "score": score,
                "reasons": reasons
            })

    if not downloaded_files:
        print("  æ²¡æœ‰æˆåŠŸä¸‹è½½ä»»ä½•è®ºæ–‡")
        return

    # 5. åˆ†æè®ºæ–‡
    print(f"\n[4/5] åˆ†æè®ºæ–‡...")
    client = GeminiClient()
    prompt = load_prompt()

    analysis_files = []

    for i, paper_info in enumerate(downloaded_files, 1):
        print(f"\n  è®ºæ–‡ {i}/{len(downloaded_files)}")

        try:
            analysis = analyze_paper(paper_info["pdf_path"], prompt, client)
            output_path = save_analysis(paper_info, analysis, output_dir)
            analysis_files.append(output_path)
        except Exception as e:
            print(f"  [é”™è¯¯] åˆ†æå¤±è´¥: {e}")

    # 6. æ±‡æ€»æŠ¥å‘Š
    print(f"\n[5/5] ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š...")
    summary_path = output_dir / "_summary.md"

    summary_content = f"# æ¯æ—¥è®ºæ–‡åˆ†ææŠ¥å‘Š - {target_date}\n\n"
    summary_content += f"## åˆ†ææ¦‚è§ˆ\n\n"
    summary_content += f"- **åˆ†ææ—¥æœŸ**: {target_date}\n"
    summary_content += f"- **è®ºæ–‡æ•°é‡**: {len(downloaded_files)}\n"
    summary_content += f"- **Frontier Labs**: {frontier_count}\n\n"
    summary_content += f"## è®ºæ–‡åˆ—è¡¨\n\n"

    for i, paper_info in enumerate(downloaded_files, 1):
        summary_content += f"{i}. **{paper_info['title'][:70]}...**\n"
        summary_content += f"   - ç»„ç»‡: {paper_info['org']}\n"
        summary_content += f"   - å¾—åˆ†: {paper_info['score']}\n"
        summary_content += f"   - æ ‡ç­¾: {paper_info['reasons']}\n"
        summary_content += f"   - åˆ†æ: [{paper_info['arxiv_id']}_analysis.md]({paper_info['arxiv_id']}_analysis.md)\n\n"

    summary_path.write_text(summary_content, encoding='utf-8')

    print("\n" + "=" * 80)
    print("Pipeline å®Œæˆ!")
    print(f"  ä¸‹è½½ç›®å½•: {download_dir}")
    print(f"  è¾“å‡ºç›®å½•: {output_dir}")
    print(f"  åˆ†ææ–‡ä»¶: {len(analysis_files)} ç¯‡")
    print("=" * 80)


def main():
    import argparse

    parser = argparse.ArgumentParser(description="Paper Analysis Pipeline")
    parser.add_argument("--date", default=None, help="ç›®æ ‡æ—¥æœŸ (YYYY-MM-DD)ï¼Œé»˜è®¤æ˜¨å¤©")
    parser.add_argument("--min-papers", type=int, default=3, help="æœ€å°‘ä¸‹è½½æ•° (é»˜è®¤3)")
    parser.add_argument("--max-papers", type=int, default=20, help="æœ€å¤šä¸‹è½½æ•° (é»˜è®¤20)")
    parser.add_argument("--topic-bonus", action="store_true", help="å¯ç”¨å…´è¶£åŠ æˆ")
    parser.add_argument("--dry-run", action="store_true", help="åªæ˜¾ç¤ºä¸å®é™…æ‰§è¡Œ")

    args = parser.parse_args()

    # é»˜è®¤æ˜¨å¤©
    if args.date:
        target_date = args.date
    else:
        target_date = (date.today() - timedelta(days=1)).strftime("%Y-%m-%d")

    run_pipeline(
        target_date=target_date,
        min_papers=args.min_papers,
        max_papers=args.max_papers,
        enable_topic_bonus=args.topic_bonus,
        dry_run=args.dry_run
    )


if __name__ == "__main__":
    main()
