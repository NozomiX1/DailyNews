# DailyNews - æ–‡ç« çˆ¬å–æ¨¡å—
import requests
import json
import time
import os
from datetime import datetime, timedelta
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))
import config
import markdown_parser

# ================= fakeid ç¼“å­˜ç®¡ç† =================

def load_fakeid_cache():
    """åŠ è½½ fakeid ç¼“å­˜"""
    if config.FAKEID_CACHE_FILE.exists():
        try:
            with open(config.FAKEID_CACHE_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            return {}
    return {}

def save_fakeid_cache(cache):
    """ä¿å­˜ fakeid ç¼“å­˜"""
    with open(config.FAKEID_CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)

def get_fakeid_with_cache(account_name):
    """
    è·å–å…¬ä¼—å· fakeidï¼Œä¼˜å…ˆä½¿ç”¨ç¼“å­˜
    """
    cache = load_fakeid_cache()

    # æ£€æŸ¥ç¼“å­˜
    if account_name in cache:
        print(f"âœ… ä»ç¼“å­˜è·å– fakeid: {account_name}")
        return cache[account_name]

    # ç¼“å­˜æœªå‘½ä¸­ï¼Œè°ƒç”¨ API
    print(f"ğŸ” æ­£åœ¨æŸ¥è¯¢å…¬ä¼—å·: {account_name}")
    fakeid = get_fakeid(account_name)

    if fakeid:
        # ä¿å­˜åˆ°ç¼“å­˜
        cache[account_name] = fakeid
        save_fakeid_cache(cache)
        print(f"ğŸ’¾ å·²ç¼“å­˜ fakeid: {account_name}")

    return fakeid

def get_fakeid(name):
    """
    æœç´¢å…¬ä¼—å·ï¼Œè·å–å…¶ fakeid
    API: /cgi-bin/searchbiz
    """
    url = f"{config.BASE_URL}/cgi-bin/searchbiz"
    params = {
        "action": "search_biz",
        "begin": "0",
        "count": "5",
        "query": name,
        "token": config.TOKEN,
        "lang": "zh_CN",
        "f": "json",
        "ajax": "1"
    }

    try:
        resp = requests.get(url, headers=config.HEADERS, params=params)
        data = resp.json()

        if data.get("base_resp", {}).get("ret") != 0:
            print(f"âŒ æœç´¢å¤±è´¥: {data}")
            return None

        for item in data.get("list", []):
            if item["nickname"] == name:
                print(f"âœ… æ‰¾åˆ°å…¬ä¼—å· [{name}], fakeid: {item['fakeid']}")
                return item["fakeid"]

        print(f"âŒ æœªæ‰¾åˆ°å…¬ä¼—å·: {name}")
        return None
    except Exception as e:
        print(f"âŒ get_fakeid å¼‚å¸¸: {e}")
        return None

# ================= æ–‡ç« çˆ¬å– =================

def get_published_articles(fakeid, page=0):
    """
    è·å–å·²å‘å¸ƒæ–‡ç« åˆ—è¡¨
    """
    url = f"{config.BASE_URL}/cgi-bin/appmsgpublish"

    params = {
        "sub": "list",
        "search_field": "null",
        "begin": str(page * 5),
        "count": "5",
        "query": "",
        "fakeid": fakeid,
        "type": "101_1",
        "free_publish_type": "1",
        "sub_action": "list_ex",
        "token": config.TOKEN,
        "lang": "zh_CN",
        "f": "json",
        "ajax": "1"
    }

    try:
        resp = requests.get(url, headers=config.HEADERS, params=params)
        data = resp.json()

        if data.get("base_resp", {}).get("ret") != 0:
            print(f"âŒ æ¥å£æŠ¥é”™: {data}")
            return []

        # è§£æå¤–å±‚ list
        publish_page = json.loads(data.get("publish_page", "{}"))
        publish_list = publish_page.get("publish_list", [])

        articles_result = []

        for i, item in enumerate(publish_list):
            try:
                # è§£åŒ…æ ¸å¿ƒæ•°æ®
                publish_info_str = item.get("publish_info", "{}")
                publish_info = json.loads(publish_info_str)

                # æå–æ—¶é—´ (ä¸‰çº§æŸ¥æ‰¾ç­–ç•¥)
                sent_time = 0

                # ç­–ç•¥ A: type=101 (ç¾¤å‘) -> sent_info.time
                if "sent_info" in publish_info and "time" in publish_info["sent_info"]:
                    sent_time = publish_info["sent_info"]["time"]

                # ç­–ç•¥ B: type=1 (å‘å¸ƒ) -> publish_info.create_time
                elif "publish_info" in publish_info and "create_time" in publish_info["publish_info"]:
                    sent_time = publish_info["publish_info"]["create_time"]

                # ç­–ç•¥ C: å…œåº• -> ä»ç¬¬ä¸€ç¯‡æ–‡ç« æ‹¿ create_time
                if sent_time == 0:
                    appmsgex = publish_info.get("appmsgex", [])
                    if appmsgex:
                        sent_time = appmsgex[0].get("create_time", 0)

                if sent_time == 0:
                    sent_time = time.time()

                # æå–æ–‡ç« åˆ—è¡¨
                appmsg_list = publish_info.get("appmsgex", [])
                if not appmsg_list:
                    appmsg_list = publish_info.get("appmsg_info", [])

                if not appmsg_list:
                    continue

                for index, msg in enumerate(appmsg_list):
                    title = msg.get("title")
                    link = msg.get("link")
                    if not link:
                        link = msg.get("content_url")

                    if title and link:
                        time_str = time.strftime("%Y-%m-%d %H:%M", time.localtime(sent_time))

                        articles_result.append({
                            "title": title,
                            "link": link,
                            "timestamp": sent_time,
                            "time_str": time_str,
                            "digest": msg.get("digest", ""),
                            "is_headline": index == 0
                        })

            except Exception as item_err:
                print(f"âŒ è§£æç¬¬ {i+1} æ¡å¤±è´¥: {item_err}")
                continue

        return articles_result

    except Exception as e:
        print(f"âŒ get_published_articles å¼‚å¸¸: {e}")
        return []


def fetch_articles_for_date(account_name, target_date):
    """
    çˆ¬å–æŒ‡å®šæ—¥æœŸçš„æ–‡ç« 
    """
    fakeid = get_fakeid_with_cache(account_name)
    if not fakeid:
        print(f"âŒ æ— æ³•è·å– {account_name} çš„ fakeidï¼Œè·³è¿‡")
        return []

    # è®¡ç®—æ—¶é—´çª—å£
    target_date = datetime.strptime(target_date, "%Y-%m-%d").date()
    today_0am = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
    target_0am = datetime.combine(target_date, datetime.min.time())
    next_day_0am = target_0am + timedelta(days=1)

    start_ts = target_0am.timestamp()
    end_ts = next_day_0am.timestamp()

    print(f"ğŸ“… çˆ¬å– [{account_name}] åœ¨ {target_date} çš„æ–‡ç« ")

    target_articles = []
    page = 0
    should_stop = False

    while not should_stop:
        batch = get_published_articles(fakeid, page=page)

        if not batch:
            break

        for art in batch:
            ts = art['timestamp']

            # è¿˜æ²¡åˆ°ç›®æ ‡æ—¥æœŸ
            if ts >= end_ts:
                continue

            # å·²ç»è¿‡ç›®æ ‡æ—¥æœŸ
            if ts < start_ts:
                should_stop = True
                break

            target_articles.append(art)

        if should_stop:
            break

        page += 1
        time.sleep(2)

    target_articles.sort(key=lambda x: x['timestamp'])
    print(f"âœ… [{account_name}] æ‰¾åˆ° {len(target_articles)} ç¯‡æ–‡ç« ")

    return target_articles


def save_article_markdown(account_name, index, article_data, target_date):
    """
    ä¸‹è½½æ–‡ç« å¹¶ä¿å­˜ä¸º Markdown
    """
    url = article_data['link']
    title = article_data['title']

    print(f"  ğŸ“¥ [{index+1}] {title}")

    # ä½¿ç”¨ markdown_parser ä¸‹è½½å¹¶è½¬æ¢
    md_content = markdown_parser.parse_wechat_to_md(url)

    if md_content:
        # ä¿å­˜æ–‡ä»¶
        date_dir = config.DATA_DIR / target_date
        date_dir.mkdir(parents=True, exist_ok=True)

        filename = f"{account_name}_{index+1:03d}.md"
        filepath = date_dir / filename

        with open(filepath, "w", encoding="utf-8") as f:
            f.write(md_content)

        print(f"    ğŸ’¾ å·²ä¿å­˜: {filename}")
        return True
    else:
        print(f"    âŒ ä¸‹è½½å¤±è´¥: {title}")
        return False


# ================= ä¸»å‡½æ•° =================

def main():
    """
    ä¸»å‡½æ•°ï¼šçˆ¬å–æ‰€æœ‰å…¬ä¼—å·ä»Šå¤©çš„æ–‡ç« 
    """
    # é»˜è®¤çˆ¬å–ä»Šå¤©çš„æ–‡ç« 
    today = datetime.now().strftime("%Y-%m-%d")

    print("=" * 50)
    print(f"ğŸš€ å¼€å§‹çˆ¬å–æ–‡ç« ï¼Œç›®æ ‡æ—¥æœŸ: {today}")
    print("=" * 50)

    all_articles = []

    for account_name in config.TARGET_ACCOUNTS:
        print(f"\n{'=' * 20} {account_name} {'=' * 20}")

        # çˆ¬å–æ–‡ç« åˆ—è¡¨
        articles = fetch_articles_for_date(account_name, today)

        # ä¸‹è½½å¹¶ä¿å­˜æ¯ç¯‡æ–‡ç« 
        for idx, article in enumerate(articles):
            save_article_markdown(account_name, idx, article, today)
            all_articles.append({
                "account": account_name,
                "index": idx + 1,
                "article": article
            })
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«

    print("\n" + "=" * 50)
    print(f"âœ… çˆ¬å–å®Œæˆï¼å…±ä¿å­˜ {len(all_articles)} ç¯‡æ–‡ç« ")
    print(f"ğŸ“ ä¿å­˜ä½ç½®: {config.DATA_DIR / today}")
    print("=" * 50)


if __name__ == "__main__":
    main()
