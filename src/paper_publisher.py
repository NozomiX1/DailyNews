#!/usr/bin/env python3
"""
å‘å¸ƒæ¯æ—¥è®ºæ–‡åˆ°å¾®ä¿¡å…¬ä¼—å·è‰ç¨¿ç®±
æ¯ç¯‡è®ºæ–‡å‘å¸ƒä¸ºä¸€ä¸ªç‹¬ç«‹çš„è‰ç¨¿
"""
import requests
import json
import re
import sys
from pathlib import Path
from datetime import datetime, timedelta

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))
import config


class PaperPublisher:
    """è®ºæ–‡å‘å¸ƒå™¨ - æ¯ç¯‡è®ºæ–‡ä¸€ä¸ªè‰ç¨¿"""

    def __init__(self):
        self.app_id = config.APP_ID
        self.app_secret = config.APP_SECRET
        self.token = self._get_access_token()

    def _get_access_token(self):
        """è·å– access_token"""
        url = f"https://api.weixin.qq.com/cgi-bin/token?grant_type=client_credential&appid={self.app_id}&secret={self.app_secret}"
        resp = requests.get(url, proxies=getattr(config, 'PROXIES', None)).json()
        if 'access_token' in resp:
            return resp['access_token']
        else:
            raise Exception(f"è·å– access_token å¤±è´¥: {resp}")

    def _parse_analysis_file(self, analysis_path):
        """è§£æå•ç¯‡è®ºæ–‡çš„åˆ†ææ–‡ä»¶"""
        with open(analysis_path, "r", encoding="utf-8") as f:
            content = f.read()

        # æå–æ ‡é¢˜ (ç¬¬ä¸€ä¸ª # åé¢çš„å†…å®¹)
        title_match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
        title = title_match.group(1).strip() if title_match else Path(analysis_path).stem

        # æå–è®ºæ–‡åŸæ ‡é¢˜ (ä»ç¬¬ä¸€è¡Œçš„ã€Šã€‹ä¸­æå–)
        paper_title_match = re.search(r'ã€Š(.+?)ã€‹', content.split('---')[0] if '---' in content else content)
        paper_title = paper_title_match.group(1) if paper_title_match else ''

        # æå–å…ƒæ•°æ® (arXiv ID, ç»„ç»‡, Stars, Upvotes, å¾—åˆ†, æ ‡ç­¾)
        arxiv_id = re.search(r'\*\*arXiv ID\*\*:\s*(.+)', content)
        org = re.search(r'\*\*ç»„ç»‡\*\*:\s*(.+)', content)
        stars = re.search(r'\*\*GitHub Stars\*\*:\s*(.+)', content)
        upvotes = re.search(r'\*\*Upvotes\*\*:\s*(.+)', content)
        score = re.search(r'\*\*å¾—åˆ†\*\*:\s*(.+)', content)
        tags = re.search(r'\*\*æ ‡ç­¾\*\*:\s*(.+)', content)

        # æå–æ­£æ–‡ (å»é™¤ --- ä¹‹åçš„å†…å®¹)
        parts = content.split('---', 1)
        body = parts[1].strip() if len(parts) > 1 else content

        # æå–ç¬¬ä¸€æ®µä½œä¸ºæ‘˜è¦ï¼ˆå»é™¤ç©ºè¡Œåç¬¬ä¸€ä¸ªéæ ‡é¢˜æ®µè½ï¼‰
        intro_match = re.search(r'^(?!#)(?!<)(.+)$', body, re.MULTILINE)
        intro = intro_match.group(1).strip() if intro_match else ''
        # å»é™¤ intro ä¸­çš„ markdown æ ¼å¼
        intro = re.sub(r'\*\*(.+?)\*\*', r'\1', intro)
        intro = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', intro)

        return {
            'title': title,
            'paper_title': paper_title,
            'intro': intro,
            'arxiv_id': arxiv_id.group(1).strip() if arxiv_id else '',
            'org': org.group(1).strip() if org else '',
            'stars': stars.group(1).strip() if stars else '',
            'upvotes': upvotes.group(1).strip() if upvotes else '',
            'score': score.group(1).strip() if score else '',
            'tags': tags.group(1).strip() if tags else '',
            'body': body
        }

    def _markdown_to_html(self, markdown_text):
        """å°† Markdown è½¬æ¢ä¸ºå¾®ä¿¡å…¬ä¼—å· HTML - ç›´æ¥è§£æå¹¶ç”Ÿæˆå¸¦æ ·å¼çš„ HTML"""
        lines = markdown_text.split('\n')
        html_lines = []
        skip_first_h1 = True  # è·³è¿‡ç¬¬ä¸€ä¸ª h1ï¼ˆå› ä¸ºå·²åœ¨æ ‡é¢˜å¤„æ˜¾ç¤ºï¼‰

        # åˆ é™¤ç¬¬ä¸€æ®µï¼ˆå·²ä½œä¸º intro æ˜¾ç¤ºï¼‰
        first_para_removed = False

        i = 0
        while i < len(lines):
            line = lines[i]
            stripped = line.strip()

            # è·³è¿‡ç¬¬ä¸€ä¸ª h1 æ ‡é¢˜
            if skip_first_h1 and re.match(r'^#\s+', line):
                skip_first_h1 = False
                i += 1
                continue

            # è·³è¿‡ç¬¬ä¸€æ®µï¼ˆå·²ä½œä¸º intro æ˜¾ç¤ºåœ¨å¡ç‰‡ä¸­ï¼‰
            if not first_para_removed and stripped and not re.match(r'^[#\*\-\d\s]', line):
                first_para_removed = True
                i += 1
                continue

            # å¤„ç†å››çº§æ ‡é¢˜
            match = re.match(r'^####\s+(.+)$', line)
            if match:
                content = match.group(1)
                content = re.sub(r'\*\*(.+?)\*\*', r'<strong style="color: #2c3e50; font-weight: 600;">\1</strong>', content)
                html_lines.append(f'<h4 style="font-size: 16px; font-weight: bold; color: #555; text-align: left; margin: 15px 0 10px;">{content}</h4>')
                i += 1
                continue

            # å¤„ç†ä¸‰çº§æ ‡é¢˜
            match = re.match(r'^###\s+(.+)$', line)
            if match:
                content = match.group(1)
                content = re.sub(r'\*\*(.+?)\*\*', r'<strong style="color: #2c3e50; font-weight: 600;">\1</strong>', content)
                html_lines.append(f'<h3 style="font-size: 18px; font-weight: bold; color: #34495e; text-align: left; margin: 20px 0 12px; padding-left: 10px; border-left: 4px solid #3498db;">{content}</h3>')
                i += 1
                continue

            # å¤„ç†äºŒçº§æ ‡é¢˜
            match = re.match(r'^##\s+(.+)$', line)
            if match:
                content = match.group(1)
                content = re.sub(r'\*\*(.+?)\*\*', r'<strong style="color: #2c3e50; font-weight: 600;">\1</strong>', content)
                html_lines.append(f'<h2 style="font-size: 20px; font-weight: bold; color: #2c3e50; text-align: center; margin: 30px 0 15px; padding: 10px 0; border-top: 1px solid #e0e0e0; border-bottom: 1px solid #e0e0e0;">{content}</h2>')
                i += 1
                continue

            # å¤„ç†ä¸€çº§æ ‡é¢˜ï¼ˆè·³è¿‡ç¬¬ä¸€ä¸ªä¹‹åçš„å…¶ä»– h1ï¼‰
            match = re.match(r'^#\s+(.+)$', line)
            if match:
                content = match.group(1)
                content = re.sub(r'\*\*(.+?)\*\*', r'<strong style="color: #2c3e50; font-weight: 600;">\1</strong>', content)
                html_lines.append(f'<h1 style="font-size: 22px; font-weight: bold; color: #1a1a1a; text-align: center; margin: 25px 0 20px; padding-bottom: 10px;">{content}</h1>')
                i += 1
                continue

            # å¤„ç†ç©ºè¡Œ
            if not stripped:
                if html_lines and not html_lines[-1].startswith('</'):
                    html_lines.append('<br>')
                i += 1
                continue

            # æ”¶é›†åˆ—è¡¨ï¼ˆå¤šè¡Œï¼‰
            list_items = []
            list_start_idx = i
            list_type = None  # 'ul' or 'ol'
            base_indent = None

            while i < len(lines):
                line = lines[i]
                stripped_i = line.strip()

                # ç©ºè¡Œç»“æŸåˆ—è¡¨
                if not stripped_i:
                    break

                # æ£€æµ‹åˆ—è¡¨é¡¹
                ul_match = re.match(r'^([\s]*)[\*\-]\s+', line)
                ol_match = re.match(r'^([\s]*)\d+\.\s+', line)

                # ç¡®å®šä½¿ç”¨å“ªä¸ªåŒ¹é…å¯¹è±¡
                match_obj = None
                if ul_match:
                    match_obj = ul_match
                elif ol_match:
                    match_obj = ol_match

                if match_obj:
                    indent = len(match_obj.group(1))

                    # ç¡®å®šåˆ—è¡¨ç±»å‹
                    if list_type is None:
                        list_type = 'ul' if ul_match else 'ol'
                        base_indent = indent

                    # æ£€æµ‹æ˜¯å¦æ˜¯ä¸åŒç±»å‹çš„åˆ—è¡¨ï¼ˆéœ€è¦é‡æ–°å¼€å§‹ï¼‰
                    current_is_ul = ul_match is not None
                    if (current_is_ul and list_type != 'ul') or (not current_is_ul and list_type == 'ul'):
                        if list_items:
                            break  # åˆ‡æ¢åˆ—è¡¨ç±»å‹

                    # ä½¿ç”¨ match å¯¹è±¡çš„ span æ¥æˆªå–å­—ç¬¦ä¸²
                    start, end = match_obj.span()
                    content = line[end:].strip()

                    # è·³è¿‡ç©ºå†…å®¹
                    if not content:
                        i += 1
                        continue

                    # å¤„ç†å†…è”æ ¼å¼
                    content = re.sub(r'\*\*(.+?)\*\*', r'<strong style="color: #2c3e50; font-weight: 600;">\1</strong>', content)
                    content = re.sub(r'\[([^\]]+)\]\(([^)]+)\)', r'<a href="\2" style="color: #3498db;">\1</a>', content)

                    # æ ¹æ®ç¼©è¿›åˆ¤æ–­æ˜¯å¦åµŒå¥—
                    is_nested = indent > base_indent
                    style = 'margin: 6px 0; line-height: 1.8; color: #333;' if is_nested else 'margin: 8px 0; line-height: 1.8; color: #333;'

                    if is_nested:
                        # åµŒå¥—åˆ—è¡¨é¡¹ï¼ˆä½¿ç”¨ä¸åŒæ ·å¼ï¼‰
                        list_items.append(f'<li style="{style} padding-left: 10px;">{content}</li>')
                    else:
                        list_items.append(f'<li style="{style}">{content}</li>')

                    i += 1
                else:
                    # éåˆ—è¡¨è¡Œï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯å‰ä¸€ä¸ªåˆ—è¡¨é¡¹çš„ç»­è¡Œ
                    if list_items and (line.startswith('    ') or line.startswith('\t')):
                        # ç»­è¡Œï¼Œè¿½åŠ åˆ°ä¸Šä¸€ä¸ªåˆ—è¡¨é¡¹
                        continuation = re.sub(r'\*\*(.+?)\*\*', r'<strong style="color: #2c3e50; font-weight: 600;">\1</strong>', stripped)
                        continuation = re.sub(r'\[([^\]]+)\]\(([^)]+)\)', r'<a href="\2" style="color: #3498db;">\1</a>', continuation)
                        # ç§»é™¤æœ€åä¸€ä¸ª </li> å¹¶æ·»åŠ ç»­è¡Œ
                        last_item = list_items[-1]
                        list_items[-1] = last_item.replace('</li>', f' {continuation}</li>')
                        i += 1
                    else:
                        break

            # è¾“å‡ºåˆ—è¡¨
            if list_items:
                if list_type == 'ul':
                    style = 'margin: 15px 0; padding-left: 20px;'
                else:
                    style = 'margin: 15px 0; padding-left: 25px;'
                html_lines.append(f'<{list_type} style="{style}">')
                html_lines.extend(list_items)
                html_lines.append(f'</{list_type}>')
                continue  # i å·²ç»åœ¨åˆ—è¡¨å¤„ç†ä¸­æ›´æ–°äº†

            # å¤„ç†æ™®é€šæ®µè½
            if stripped:
                # å¤„ç†ç²—ä½“
                line = re.sub(r'\*\*(.+?)\*\*', r'<strong style="color: #2c3e50; font-weight: 600;">\1</strong>', line)
                # å¤„ç†é“¾æ¥
                line = re.sub(r'\[([^\]]+)\]\(([^)]+)\)', r'<a href="\2" style="color: #3498db;">\1</a>', line)
                html_lines.append(f'<p style="font-size: 15px; color: #333; line-height: 1.9; margin-bottom: 10px; text-align: justify;">{line}</p>')

            i += 1

        return '\n'.join(html_lines)

    def generate_html(self, paper_data):
        """ç”Ÿæˆå•ç¯‡è®ºæ–‡çš„ HTML"""
        # å®¹å™¨æ ·å¼
        container = '<section style="font-family: -apple-system, BlinkMacSystemFont, \'Segoe UI\', Roboto, Helvetica, Arial, sans-serif; max-width: 677px; margin: 0 auto; padding: 20px 0;">'

        # æ ‡é¢˜å¤´éƒ¨
        title_html = f'''
<div style="text-align: center; margin-bottom: 25px;">
    <h1 style="font-size: 24px; font-weight: bold; color: #1a1a1a; margin: 0 0 15px; line-height: 1.4;">{paper_data['title']}</h1>
    {f'<p style="font-size: 14px; color: #666; margin: 5px 0 0;">ã€Š{paper_data["paper_title"]}ã€‹</p>' if paper_data.get('paper_title') else ''}
</div>
'''

        # å…ƒä¿¡æ¯å¡ç‰‡
        meta_html = '<div style="background: linear-gradient(135deg, #f5f7fa 0%, #e8ecf1 100%); padding: 15px; border-radius: 8px; margin-bottom: 25px; font-size: 14px; color: #555;">'

        if paper_data.get('arxiv_id'):
            arxiv_url = f"https://arxiv.org/abs/{paper_data['arxiv_id']}"
            meta_html += f'<div style="margin-bottom: 8px;">ğŸ“„ <strong>è®ºæ–‡ï¼š</strong><a href="{arxiv_url}" style="color: #3498db; text-decoration: none;">{paper_data["arxiv_id"]}</a></div>'
        if paper_data.get('org'):
            meta_html += f'<div style="margin-bottom: 8px;">ğŸ”¬ <strong>æœºæ„ï¼š</strong>{paper_data["org"]}</div>'
        if paper_data.get('tags'):
            meta_html += f'<div style="margin-bottom: 8px;">ğŸ·ï¸ <strong>æ ‡ç­¾ï¼š</strong>{paper_data["tags"]}</div>'

        # å¾—åˆ†å’Œäº’åŠ¨æ•°æ®
        stats_row = ''
        if paper_data.get('score'):
            stats_row += f'<span style="display: inline-block; margin-right: 15px;">ğŸ“Š {paper_data["score"]}</span>'
        if paper_data.get('upvotes'):
            stats_row += f'<span style="display: inline-block; margin-right: 15px;">ğŸ‘ {paper_data["upvotes"]}</span>'
        if paper_data.get('stars'):
            stats_row += f'<span style="display: inline-block;">â­ {paper_data["stars"]}</span>'
        if stats_row:
            meta_html += f'<div style="margin-top: 10px; padding-top: 10px; border-top: 1px solid #d0d7de;">{stats_row}</div>'

        meta_html += '</div>'

        # æ‘˜è¦æ®µè½
        intro_html = ''
        if paper_data.get('intro'):
            intro_html = f'<p style="font-size: 15px; color: #444; line-height: 1.8; margin-bottom: 20px; text-align: justify; padding: 12px; background: #f9f9f9; border-radius: 6px;">{paper_data["intro"]}</p>'

        # åˆ†éš”çº¿
        divider = '<hr style="border: none; border-top: 1px solid #e0e0e0; margin: 25px 0;">'

        # æ­£æ–‡
        body_html = self._markdown_to_html(paper_data['body'])

        # ç»“å°¾
        footer = '''
<div style="margin-top: 40px; padding-top: 20px; border-top: 2px solid #e0e0e0; text-align: center; color: #888; font-size: 13px;">
    <p style="margin: 0;">ğŸ“ ç”± AI ç²¾è¯»æ•´ç† | ä»…ä¾›å­¦ä¹ äº¤æµ</p>
</div>
</section>
'''

        return container + title_html + meta_html + intro_html + divider + body_html + footer

    def publish_single_paper(self, analysis_path):
        """å‘å¸ƒå•ç¯‡è®ºæ–‡åˆ°è‰ç¨¿ç®±"""
        paper_data = self._parse_analysis_file(analysis_path)

        print(f"ğŸ“„ æ­£åœ¨å‘å¸ƒ: {paper_data['title']}")

        # ç”Ÿæˆæ ‡é¢˜ (å»é™¤è¿‡é•¿æ ‡é¢˜)
        title = paper_data['title']
        if len(title) > 50:
            title = title[:47] + '...'

        # ç”Ÿæˆ HTML
        content_html = self.generate_html(paper_data)

        # åˆ›å»ºè‰ç¨¿
        draft_id = self._create_draft(title, content_html, config.COVER_MEDIA_ID)

        return draft_id, paper_data['title']

    def publish_all_papers(self, date_str):
        """å‘å¸ƒæŸä¸€å¤©çš„æ‰€æœ‰è®ºæ–‡"""
        project_root = Path(__file__).parent.parent
        output_dir = project_root / "output" / date_str

        if not output_dir.exists():
            raise Exception(f"è¾“å‡ºç›®å½•ä¸å­˜åœ¨: {output_dir}")

        # æ‰¾åˆ°æ‰€æœ‰åˆ†ææ–‡ä»¶ (æ’é™¤ _summary.md)
        analysis_files = [
            f for f in output_dir.glob("*_analysis.md")
            if not f.name.startswith('_')
        ]

        if not analysis_files:
            print(f"âŒ æ²¡æœ‰æ‰¾åˆ°åˆ†ææ–‡ä»¶: {output_dir}")
            return

        print(f"ğŸ“Š æ‰¾åˆ° {len(analysis_files)} ç¯‡è®ºæ–‡åˆ†æ")

        results = []
        for i, analysis_file in enumerate(analysis_files, 1):
            print(f"\n[{i}/{len(analysis_files)}] {analysis_file.name}")
            try:
                draft_id, title = self.publish_single_paper(analysis_file)
                results.append({'file': analysis_file.name, 'draft_id': draft_id, 'title': title, 'status': 'success'})
                print(f"  âœ… æˆåŠŸ - Media ID: {draft_id}")
            except Exception as e:
                results.append({'file': analysis_file.name, 'error': str(e), 'status': 'failed'})
                print(f"  âŒ å¤±è´¥ - {e}")

        return results

    def _create_draft(self, title, content, thumb_id):
        """åˆ›å»ºè‰ç¨¿"""
        url = f"https://api.weixin.qq.com/cgi-bin/draft/add?access_token={self.token}"
        data = {
            "articles": [{
                "title": title,
                "author": "Paper Analysis",
                "digest": f"{title[:50]}...",
                "content": content,
                "thumb_media_id": thumb_id
            }]
        }

        resp = requests.post(
            url,
            data=json.dumps(data, ensure_ascii=False).encode('utf-8'),
            headers={'Content-Type': 'application/json; charset=utf-8'},
            proxies=getattr(config, 'PROXIES', None)
        )

        result = resp.json()

        if 'media_id' not in result:
            raise Exception(f"è‰ç¨¿åˆ›å»ºå¤±è´¥: {result}")

        return result['media_id']


def main():
    import argparse

    parser = argparse.ArgumentParser(description="å°†è®ºæ–‡ç²¾è¯»å‘å¸ƒåˆ°å¾®ä¿¡å…¬ä¼—å·è‰ç¨¿ç®±")
    parser.add_argument("--date", default=None, help="æ—¥æœŸ (YYYY-MM-DD)ï¼Œé»˜è®¤æ˜¨å¤©")
    parser.add_argument("--file", default=None, help="å•ä¸ªåˆ†ææ–‡ä»¶è·¯å¾„")
    args = parser.parse_args()

    # ç¡®å®šæ—¥æœŸ
    if args.file:
        # å•ä¸ªæ–‡ä»¶æ¨¡å¼
        target_date = None
    else:
        if args.date:
            target_date = args.date
        else:
            # é»˜è®¤æ˜¨å¤©
            target_date = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")

    publisher = PaperPublisher()

    print("=" * 60)
    print("ğŸ“¤ æ¯æ—¥è®ºæ–‡ç²¾è¯»å‘å¸ƒåˆ°è‰ç¨¿ç®±")
    if target_date:
        print(f"ğŸ“… æ—¥æœŸ: {target_date}")
    if args.file:
        print(f"ğŸ“„ æ–‡ä»¶: {args.file}")
    print("=" * 60)

    try:
        if args.file:
            # å‘å¸ƒå•ä¸ªæ–‡ä»¶
            draft_id, title = publisher.publish_single_paper(args.file)
            print(f"\nâœ… è‰ç¨¿åˆ›å»ºæˆåŠŸï¼")
            print(f"ğŸ“‹ Media ID: {draft_id}")
            print(f"ğŸ“„ æ ‡é¢˜: {title}")
        else:
            # å‘å¸ƒæ‰€æœ‰è®ºæ–‡
            results = publisher.publish_all_papers(target_date)

            success_count = sum(1 for r in results if r['status'] == 'success')
            print(f"\n{'=' * 60}")
            print(f"âœ… å®Œæˆ: {success_count}/{len(results)} ç¯‡è®ºæ–‡å‘å¸ƒæˆåŠŸ")
            print(f"ğŸ‘‰ è¯·ç™»å½•å¾®ä¿¡å…¬ä¼—å·åå°æŸ¥çœ‹è‰ç¨¿ç®±")
            print("=" * 60)

    except Exception as e:
        print(f"\nâŒ å‘å¸ƒå¤±è´¥: {e}")


if __name__ == "__main__":
    main()
