# DailyNews - WeChat Article Fetcher
# Migrated from src/wechat_fetcher.py
import requests
import json
import time
from datetime import datetime, timedelta
from pathlib import Path
import sys

from .base import BaseFetcher
from ..utils import parse_wechat_to_md, retry_on_request_error
import config


class WechatFetcher(BaseFetcher):
    """WeChat Official Account article fetcher."""

    def __init__(self, data_dir: Path = None):
        super().__init__(data_dir)
        self.target_accounts = config.TARGET_ACCOUNTS
        self.fakeid_cache_file = config.FAKEID_CACHE_FILE

    # ================= fakeid ç¼“å­˜ç®¡ç† =================

    def _load_fakeid_cache(self):
        """åŠ è½½ fakeid ç¼“å­˜"""
        if self.fakeid_cache_file.exists():
            try:
                with open(self.fakeid_cache_file, "r", encoding="utf-8") as f:
                    return json.load(f)
            except:
                return {}
        return {}

    def _save_fakeid_cache(self, cache):
        """ä¿å­˜ fakeid ç¼“å­˜"""
        with open(self.fakeid_cache_file, "w", encoding="utf-8") as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)

    def _get_fakeid_with_cache(self, account_name):
        """è·å–å…¬ä¼—å· fakeidï¼Œä¼˜å…ˆä½¿ç”¨ç¼“å­˜"""
        cache = self._load_fakeid_cache()

        if account_name in cache:
            print(f"  âœ… ä»ç¼“å­˜è·å– fakeid: {account_name}")
            return cache[account_name]

        print(f"  ğŸ” æ­£åœ¨æŸ¥è¯¢å…¬ä¼—å·: {account_name}")
        fakeid = self._get_fakeid(account_name)

        if fakeid:
            cache[account_name] = fakeid
            self._save_fakeid_cache(cache)
            print(f"  ğŸ’¾ å·²ç¼“å­˜ fakeid: {account_name}")

        return fakeid

    @retry_on_request_error(max_retries=3)
    def _get_fakeid(self, name):
        """æœç´¢å…¬ä¼—å·ï¼Œè·å–å…¶ fakeid"""
        url = f"{config.BASE_URL}/cgi-bin/searchbiz"
        params = {
            "action": "search_biz",
            "begin": "0",
            "count": "5",
            "query": name,
            "token": config.TOKEN,
            "lang": "zh_CN",
            "f": "json",
            "ajax": "1"
        }

        try:
            resp = requests.get(url, headers=config.HEADERS, params=params)
            data = resp.json()

            if data.get("base_resp", {}).get("ret") != 0:
                print(f"  âŒ æœç´¢å¤±è´¥: {data}")
                return None

            for item in data.get("list", []):
                if item["nickname"] == name:
                    print(f"  âœ… æ‰¾åˆ°å…¬ä¼—å· [{name}], fakeid: {item['fakeid']}")
                    return item["fakeid"]

            print(f"  âŒ æœªæ‰¾åˆ°å…¬ä¼—å·: {name}")
            return None
        except Exception as e:
            print(f"  âŒ get_fakeid å¼‚å¸¸: {e}")
            return None

    # ================= æ–‡ç« çˆ¬å– =================

    @retry_on_request_error(max_retries=3)
    def _get_published_articles(self, fakeid, page=0):
        """è·å–å·²å‘å¸ƒæ–‡ç« åˆ—è¡¨"""
        url = f"{config.BASE_URL}/cgi-bin/appmsgpublish"

        params = {
            "sub": "list",
            "search_field": "null",
            "begin": str(page * 5),
            "count": "5",
            "query": "",
            "fakeid": fakeid,
            "type": "101_1",
            "free_publish_type": "1",
            "sub_action": "list_ex",
            "token": config.TOKEN,
            "lang": "zh_CN",
            "f": "json",
            "ajax": "1"
        }

        try:
            resp = requests.get(url, headers=config.HEADERS, params=params)
            data = resp.json()

            if data.get("base_resp", {}).get("ret") != 0:
                print(f"  âŒ æ¥å£æŠ¥é”™: {data}")
                return []

            publish_page = json.loads(data.get("publish_page", "{}"))
            publish_list = publish_page.get("publish_list", [])
            articles_result = []

            for i, item in enumerate(publish_list):
                try:
                    publish_info_str = item.get("publish_info", "{}")
                    publish_info = json.loads(publish_info_str)

                    sent_time = 0

                    if "sent_info" in publish_info and "time" in publish_info["sent_info"]:
                        sent_time = publish_info["sent_info"]["time"]
                    elif "publish_info" in publish_info and "create_time" in publish_info["publish_info"]:
                        sent_time = publish_info["publish_info"]["create_time"]

                    if sent_time == 0:
                        appmsgex = publish_info.get("appmsgex", [])
                        if appmsgex:
                            sent_time = appmsgex[0].get("create_time", 0)

                    if sent_time == 0:
                        sent_time = time.time()

                    appmsg_list = publish_info.get("appmsgex", [])
                    if not appmsg_list:
                        appmsg_list = publish_info.get("appmsg_info", [])

                    if not appmsg_list:
                        continue

                    for index, msg in enumerate(appmsg_list):
                        title = msg.get("title")
                        link = msg.get("link")
                        if not link:
                            link = msg.get("content_url")

                        if title and link:
                            time_str = time.strftime("%Y-%m-%d %H:%M", time.localtime(sent_time))

                            articles_result.append({
                                "title": title,
                                "link": link,
                                "timestamp": sent_time,
                                "time_str": time_str,
                                "digest": msg.get("digest", ""),
                                "is_headline": index == 0
                            })

                except Exception as item_err:
                    print(f"  âŒ è§£æç¬¬ {i+1} æ¡å¤±è´¥: {item_err}")
                    continue

            return articles_result

        except Exception as e:
            print(f"  âŒ get_published_articles å¼‚å¸¸: {e}")
            return []

    def _fetch_articles_for_date(self, account_name, target_date):
        """çˆ¬å–æŒ‡å®šæ—¥æœŸçš„æ–‡ç« """
        fakeid = self._get_fakeid_with_cache(account_name)
        if not fakeid:
            print(f"  âŒ æ— æ³•è·å– {account_name} çš„ fakeidï¼Œè·³è¿‡")
            return []

        target_date = datetime.strptime(target_date, "%Y-%m-%d").date()
        target_0am = datetime.combine(target_date, datetime.min.time())
        next_day_0am = target_0am + timedelta(days=1)

        start_ts = target_0am.timestamp()
        end_ts = next_day_0am.timestamp()

        print(f"  ğŸ“… çˆ¬å– [{account_name}] åœ¨ {target_date} çš„æ–‡ç« ")

        target_articles = []
        page = 0
        should_stop = False

        while not should_stop:
            batch = self._get_published_articles(fakeid, page=page)

            if not batch:
                break

            for art in batch:
                ts = art['timestamp']

                if ts >= end_ts:
                    continue

                if ts < start_ts:
                    should_stop = True
                    break

                target_articles.append(art)

            if should_stop:
                break

            page += 1
            time.sleep(2)

        target_articles.sort(key=lambda x: x['timestamp'])
        print(f"  âœ… [{account_name}] æ‰¾åˆ° {len(target_articles)} ç¯‡æ–‡ç« ")

        return target_articles

    def _save_article_markdown(self, account_name, index, article_data, target_date):
        """ä¸‹è½½æ–‡ç« å¹¶ä¿å­˜ä¸º Markdown"""
        url = article_data['link']
        title = article_data['title']

        print(f"    ğŸ“¥ [{index+1}] {title}")

        md_content = parse_wechat_to_md(url)

        if md_content:
            # æ–°è·¯å¾„: data/{date}/articles/
            date_dir = self.data_dir / target_date / "articles"
            date_dir.mkdir(parents=True, exist_ok=True)

            filename = f"{account_name}_{index+1:03d}.md"
            filepath = date_dir / filename

            with open(filepath, "w", encoding="utf-8") as f:
                f.write(md_content)

            print(f"      ğŸ’¾ å·²ä¿å­˜: {filename}")
            return {
                "account": account_name,
                "index": index + 1,
                "filepath": str(filepath),
                "title": title,
                "url": url,
                "time_str": article_data.get('time_str', ''),
                "timestamp": article_data.get('timestamp', 0)
            }
        else:
            print(f"      âŒ ä¸‹è½½å¤±è´¥: {title}")
            return None

    # ================= ä¸»æ¥å£ =================

    def fetch(self, date: str = None) -> list:
        """
        Fetch articles for a specific date.

        Args:
            date: Date string in YYYY-MM-DD format, defaults to today

        Returns:
            List of article dictionaries with content and metadata
        """
        if date is None:
            date = datetime.now().strftime("%Y-%m-%d")

        print(f"ğŸ“¡ å¼€å§‹çˆ¬å–å…¬ä¼—å·æ–‡ç« ï¼Œç›®æ ‡æ—¥æœŸ: {date}")

        all_articles = []

        for account_name in self.target_accounts:
            print(f"\n{'=' * 20} {account_name} {'=' * 20}")

            articles = self._fetch_articles_for_date(account_name, date)

            for idx, article in enumerate(articles):
                result = self._save_article_markdown(account_name, idx, article, date)
                if result:
                    # Read the saved content
                    try:
                        with open(result['filepath'], 'r', encoding='utf-8') as f:
                            result['content'] = f.read()
                    except:
                        result['content'] = ''
                    all_articles.append(result)
                time.sleep(1)

        print(f"\nâœ… çˆ¬å–å®Œæˆï¼å…±ä¿å­˜ {len(all_articles)} ç¯‡æ–‡ç« ")

        return all_articles

    def save_raw_data(self, items: list, date: str) -> Path:
        """Save raw article data (metadata only, content is saved separately)."""
        # æ–°è·¯å¾„: data/{date}/articles/
        date_dir = self.data_dir / date / "articles"
        date_dir.mkdir(parents=True, exist_ok=True)
        output_path = date_dir / f"{date}_metadata.json"
        import json
        with open(output_path, 'w', encoding='utf-8') as f:
            # Don't save content in metadata
            metadata = [{k: v for k, v in item.items() if k != 'content'} for item in items]
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        return output_path

    def load_from_json(self, date: str) -> list:
        """
        ä»æœ¬åœ° JSON åŠ è½½æ–‡ç«  metadata å¹¶è¯»å–å†…å®¹

        Args:
            date: æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)

        Returns:
            åŒ…å« content çš„æ–‡ç« åˆ—è¡¨
        """
        import json

        json_path = self.data_dir / date / "articles" / f"{date}_metadata.json"
        if json_path.exists():
            try:
                with open(json_path, 'r', encoding='utf-8') as f:
                    metadata_list = json.load(f)

                # è¯»å–æ¯ç¯‡æ–‡ç« çš„ Markdown å†…å®¹
                for meta in metadata_list:
                    filepath = Path(meta.get('filepath', ''))
                    if filepath.exists():
                        with open(filepath, 'r', encoding='utf-8') as f:
                            meta['content'] = f.read()
                    else:
                        meta['content'] = ''

                print(f"  âœ… ä» JSON åŠ è½½ {len(metadata_list)} ç¯‡æ–‡ç« ")
                return metadata_list
            except Exception as e:
                print(f"  âš ï¸ åŠ è½½ JSON å¤±è´¥: {e}")
        return []
