# DailyNews - Papers Fetcher
# Migrated from src/paper_fetch.py
import os
import requests
from datetime import date, timedelta
from pathlib import Path
from typing import List, Dict, Any
import json

from .base import BaseFetcher
from ..utils import PaperRanker, retry_on_request_error, retry_on_http_error
import config


class PapersFetcher(BaseFetcher):
    """HuggingFace Daily Papers fetcher."""

    def __init__(self, data_dir: Path = None):
        super().__init__(data_dir)
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }

    def _sanitize_filename(self, name: str) -> str:
        """æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦"""
        import re
        return re.sub(r'[\\/*?:"<>|]', "", name).strip()

    @retry_on_http_error(max_retries=3)
    def _download_pdf(self, pdf_url: str, file_path: Path) -> bool:
        """ä¸‹è½½å•ä¸ªPDFæ–‡ä»¶"""
        if not config.ENABLE_CACHE:
            return True  # Skip download but return success

        resp = requests.get(
            pdf_url,
            headers={"User-Agent": "Mozilla/5.0"},
            stream=True,
            timeout=60
        )
        resp.raise_for_status()

        with open(file_path, 'wb') as f:
            for chunk in resp.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
        return True

    @retry_on_request_error(max_retries=3)
    def fetch_papers_from_huggingface(self, target_date: str) -> List[Dict]:
        """
        ä» HuggingFace API è·å–è®ºæ–‡åˆ—è¡¨

        Args:
            target_date: ç›®æ ‡æ—¥æœŸ (YYYY-MM-DD)

        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        url = f"https://huggingface.co/api/daily_papers?date={target_date}"

        print(f"  ğŸ“¡ è·å–è®ºæ–‡åˆ—è¡¨: {target_date}")
        resp = requests.get(url, headers=self.headers, timeout=30)
        resp.raise_for_status()

        papers = resp.json()
        if not papers:
            raise ValueError(f"å½“æ—¥æ— æ•°æ®: {target_date}")

        print(f"  âœ… è·å–åˆ° {len(papers)} ç¯‡è®ºæ–‡")

        return papers

    def fetch(
        self,
        date: str = None,
        max_papers: int = 20,
        enable_topic_bonus: bool = False
    ) -> List[Dict]:
        """
        è·å–å¹¶æ’åè®ºæ–‡

        Args:
            date: ç›®æ ‡æ—¥æœŸ (YYYY-MM-DD)ï¼Œé»˜è®¤æ˜¨å¤©
            max_papers: æœ€å¤šè¿”å›è®ºæ–‡æ•°
            enable_topic_bonus: æ˜¯å¦å¯ç”¨å…´è¶£åŠ æˆ

        Returns:
            æ’ååçš„è®ºæ–‡åˆ—è¡¨
        """
        if date is None:
            date = (date.today() - timedelta(days=1)).strftime("%Y-%m-%d")

        print(f"ğŸ“¡ è·å– HuggingFace æ¯æ—¥è®ºæ–‡: {date}")

        papers = self.fetch_papers_from_huggingface(date)

        # æ’åº
        print(f"  ğŸ“Š è®ºæ–‡æ’åº...")
        ranker = PaperRanker(enable_topic_bonus=enable_topic_bonus)
        ranked = ranker.rank_papers(papers)

        for paper in ranked:
            paper['date'] = date

        print(f"  âœ… å®Œæˆï¼Œå…± {len(ranked)} ç¯‡")

        result = ranked[:max_papers]

        # Print data preview
        self._print_data_preview(result, "HuggingFace Papers")

        return result

    def _print_data_preview(self, items: List[Dict], title: str):
        """æ‰“å°ç¬¬ä¸€æ¡æ•°æ®é¢„è§ˆ"""
        if not items:
            return

        print(f"\nğŸ“‹ {title} - æ•°æ®é¢„è§ˆ (ç¬¬1æ¡):")
        print("-" * 50)

        # æ‰“å° JSON é¢„è§ˆ
        first_item = items[0]
        preview_json = json.dumps(
            first_item,
            ensure_ascii=False,
            indent=2
        )
        preview_lines = preview_json.split('\n')
        for line in preview_lines[:15]:  # å‰15è¡Œ
            print(line)
        if len(preview_lines) > 15:
            print("... (çœç•¥)")
        print("-" * 50)

    def save_raw_data(self, items: List[Dict], date: str) -> Path:
        """
        ä¿å­˜è®ºæ–‡æ¦œå•ä¸º JSON

        Args:
            items: è®ºæ–‡åˆ—è¡¨
            date: æ—¥æœŸå­—ç¬¦ä¸²

        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if not config.ENABLE_CACHE:
            print(f"      ğŸ“‹ æ— ç¼“å­˜æ¨¡å¼ï¼Œè·³è¿‡ä¿å­˜ papers JSON")
            return None

        # æ–°è·¯å¾„: data/{date}/papers/
        papers_dir = self.data_dir / date / "papers"
        papers_dir.mkdir(parents=True, exist_ok=True)

        output_path = papers_dir / f"{date}.json"

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(items, f, ensure_ascii=False, indent=2)

        print(f"  ğŸ’¾ å·²ä¿å­˜: {output_path}")
        return output_path

    def load_from_json(self, date: str, max_papers: int = None) -> List[Dict]:
        """
        ä»æœ¬åœ° JSON æ–‡ä»¶åŠ è½½è®ºæ–‡æ•°æ®ï¼ˆç”¨äºæ€»ç»“é˜¶æ®µï¼‰

        Args:
            date: æ—¥æœŸå­—ç¬¦ä¸²
            max_papers: æœ€å¤šåŠ è½½è®ºæ–‡æ•°ï¼ˆé»˜è®¤ Noneï¼ŒåŠ è½½å…¨éƒ¨ï¼‰

        Returns:
            è®ºæ–‡åˆ—è¡¨ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–åŠ è½½å¤±è´¥åˆ™è¿”å› None
        """
        import json

        json_path = self.data_dir / date / "papers" / f"{date}.json"
        if json_path.exists():
            try:
                with open(json_path, 'r', encoding='utf-8') as f:
                    papers = json.load(f)
                if max_papers:
                    papers = papers[:max_papers]
                    print(f"  âœ… ä» JSON åŠ è½½ {len(papers)} ç¯‡è®ºæ–‡ (é™åˆ¶ {max_papers} ç¯‡)")
                else:
                    print(f"  âœ… ä» JSON åŠ è½½ {len(papers)} ç¯‡è®ºæ–‡")
                return papers
            except Exception as e:
                print(f"  âš ï¸ åŠ è½½ JSON å¤±è´¥: {e}")
        return None

    def download_pdfs(self, items: List[Dict], date: str = None, min_papers: int = 3, max_papers: int = 12) -> Dict[str, int]:
        """
        ä¸‹è½½è®ºæ–‡ PDF

        Args:
            items: è®ºæ–‡åˆ—è¡¨
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œç”¨äºç¡®å®šä¿å­˜è·¯å¾„
            min_papers: æœ€å°‘ä¸‹è½½æ•°ï¼ˆé»˜è®¤3ï¼‰
            max_papers: æœ€å¤šä¸‹è½½æ•°ï¼ˆé»˜è®¤12ï¼‰

        Returns:
            ä¸‹è½½ç»Ÿè®¡å­—å…¸ {'success': æˆåŠŸæ•°, 'skipped': è·³è¿‡æ•°, 'failed': å¤±è´¥æ•°}
        """
        if not config.ENABLE_CACHE:
            print(f"      ğŸ“‹ æ— ç¼“å­˜æ¨¡å¼ï¼Œè·³è¿‡ä¸‹è½½ PDF")
            return {'success': 0, 'skipped': len(items), 'failed': 0}

        import time
        import re

        # æ–°è·¯å¾„: data/{date}/papers/pdf_downloads/
        if date is None:
            date = (date.today() - timedelta(days=1)).strftime('%Y-%m-%d')
        pdf_dir = self.data_dir / date / "papers" / "pdf_downloads"
        pdf_dir.mkdir(parents=True, exist_ok=True)

        # æ‰¾åˆ°æœ€åä¸€ç¯‡ Frontier Lab è®ºæ–‡çš„ä½ç½®
        last_frontier_idx = 0
        for i, paper in enumerate(items):
            reasons = paper.get("rank_reasons", "")
            if "Super Lab" in reasons or "Frontier Lab" in reasons:
                last_frontier_idx = i

        # ç¡®å®šä¸‹è½½æ•°é‡: ä»ç¬¬1ç¯‡åˆ°æœ€åä¸€ç¯‡ Frontier Lab
        download_count = max(min_papers, last_frontier_idx + 1)
        download_count = min(download_count, max_papers)

        items_to_download = items[:download_count]

        # ç»Ÿè®¡ Frontier Lab æ•°é‡
        frontier_count = sum(1 for p in items_to_download
                              if "Super Lab" in p.get("rank_reasons", "") or "Frontier Lab" in p.get("rank_reasons", ""))

        print(f"  ğŸ“¦ å¼€å§‹ä¸‹è½½ PDF (ç¬¬1ç¯‡ â†’ ç¬¬{last_frontier_idx+1}ç¯‡, å…± {download_count} ç¯‡, Frontier Labs: {frontier_count})...")

        stats = {'success': 0, 'skipped': 0, 'failed': 0}

        for i, paper in enumerate(items_to_download, 1):
            paper_detail = paper.get("paper", {})
            arxiv_id = paper_detail.get("id", "")
            title = paper.get("title", "")

            if not arxiv_id:
                continue

            # æ¸…ç†æ–‡ä»¶å
            safe_title = re.sub(r'[\\/*?:"<>|]', "", title).strip()
            filename = f"{arxiv_id}_{safe_title[:80]}.pdf"
            file_path = pdf_dir / filename

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if file_path.exists():
                stats['skipped'] += 1
                print(f"    [{i}/{len(items_to_download)}] âŠ™ {arxiv_id} - å·²å­˜åœ¨")
                continue

            # ä¸‹è½½ PDF
            pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
            try:
                print(f"    [{i}/{len(items_to_download)}] â¬‡ï¸ {arxiv_id}...")
                self._download_pdf(pdf_url, file_path)
                file_size = file_path.stat().st_size
                stats['success'] += 1
                print(f"       âœ“ {file_size:,} bytes")
                time.sleep(3)  # ArXiv é™åˆ¶
            except Exception as e:
                stats['failed'] += 1
                print(f"       âœ— é”™è¯¯: {e}")

        print(f"    PDF ä¸‹è½½å®Œæˆ: âœ“{stats['success']} âŠ™{stats['skipped']} âœ—{stats['failed']}")
        return stats
