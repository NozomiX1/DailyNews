"""
PaperRanker - ä¸º HuggingFace Daily Papers API è®¾è®¡çš„è¯„åˆ†ç³»ç»Ÿ

æ•°æ®æ¥æº: https://huggingface.co/api/daily_papers?date=YYYY-MM-DD
"""
import math
import re
from typing import Dict, List, Any, Optional


class PaperRanker:
    """HuggingFace è®ºæ–‡è¯„åˆ†å™¨"""

    # è¶…çº§å®éªŒå®¤ (æŒç»­å‘å¸ƒæ¨¡å‹ã€åˆ·æ¦œçš„é¡¶çº§å®éªŒå®¤) - +50 åˆ†
    SUPER_LABS = [
        r"\bOpenAI\b",           
        r"\bAnthropic\b",         
        r"Google DeepMind|DeepMind",  
        r"\bDeepSeek\b",          
        r"\bQwen\b|\bTongyi\b|\bé€šä¹‰\b|\bTongyi Qianwen\b|\bTongyi Lab\b",  # â† æ–°å¢ï¼štongyi å…¨è¦†ç›–ï¼ˆè¶…çº§é‡è¦ï¼‰
        r"\bMoonshot\b",          
        r"\bMistral\b",           
        r"\bMeta AI\b",           
        r"01\.AI|é›¶ä¸€ä¸‡ç‰©|Zhipu",  
        r"\bByteDance\b",         


        # 2025-2026 æ–°åˆ·æ¦œåŠ¿åŠ›ï¼ˆä¿æŒä¸å˜ï¼‰
        r"\bxAI\b",               
        r"\bMiniMax\b|\bæµ·èºAI\b", 
        r"\bStepFun\b|\bé˜¶è·ƒæ˜Ÿè¾°\b",
    ]

    # Frontier å®éªŒå®¤ (é¡¶çº§ç ”ç©¶æœºæ„) - +20 åˆ†
    FRONTIER_LABS = [
        r"DeepMind|Google", r"Meta AI|FAIR|Facebook",
        r"Anthropic", r"NVIDIA", r"Microsoft", r"Alibaba", r"Tencent",
        r"DeepSeek", r"Qwen", r"Mistral", r"Moonshot",
        r"01\.AI|Zhipu", r"ByteDance", r"Baichuan",
        r"Stanford", r"Berkeley", r"MIT", r"CMU",
        r"OpenAI", r"Tsinghua", r"Peking University|åŒ—äº¬å¤§å­¦",
        r"Huawei", r"Baidu", r"SenseTime", r"\bBaichuan\b",

        # æ–°å¢å…¬å¸ï¼ˆ2026ä»åœ¨é«˜äº§é«˜è´¨é‡è®ºæ–‡ï¼‰
        r"Alibaba|Tongyi|é€šä¹‰|Tongyi Qianwen|Tongyi Lab",  # â† åŒæ­¥å¢å¼º tongyi
        r"\bMeituan\b|\bç¾å›¢\b|\bLongCat\b|\blongcat\b|\bmeituan-longcat\b|LongCat Team|Meituan LongCat",
        r"\bAmazon\b|\bAWS\b", 
        r"\bApple\b", 
        r"\bIBM\b", 
        r"\bAdobe\b", 
        r"\biFlytek\b|\bç§‘å¤§è®¯é£\b", 
        r"\bXiaomi\b|\bå°ç±³\b", 
        r"\bMeituan\b|\bç¾å›¢\b",
        r"\bBAAI\b|\båŒ—äº¬æ™ºæº\b|\båŒ—äº¬äººå·¥æ™ºèƒ½ç ”ç©¶é™¢\b",  # åŒ—äº¬æ™ºæº
        r"\bShanghai AI Laboratory\b|\bä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤\b",
        r"\bPeng Cheng Laboratory\b|\bé¹åŸå®éªŒå®¤\b",
        r"\bZhejiang Lab\b",

        # æ–°å¢é¡¶å°–å¤§å­¦ï¼ˆUS News + arXivé«˜äº§æœºæ„ï¼‰
        r"\bHarvard\b|\bå“ˆä½›å¤§å­¦\b",
        r"\bPrinceton\b|\bæ™®æ—æ–¯é¡¿å¤§å­¦\b",
        r"\bCaltech\b|\båŠ å·ç†å·¥\b",
        r"\bUCLA\b|\bUCSD\b|\bUC San Diego\b",
        r"\bNYU\b|\bNew York University\b|\bçº½çº¦å¤§å­¦\b",
        r"\bColumbia\b|\bå“¥ä¼¦æ¯”äºšå¤§å­¦\b",
        r"\bUniversity of Washington\b|\bUW\b|\båç››é¡¿å¤§å­¦\b",
        r"\bUIUC\b|\bIllinois\b|\bä¼Šåˆ©è¯ºä¼Šå¤§å­¦\b",
        r"\bGeorgia Tech\b|\bä½æ²»äºšç†å·¥\b",
        r"\bUniversity of Toronto\b|\bå¤šä¼¦å¤šå¤§å­¦\b",
        r"\bMila\b|\bVector Institute\b",  # åŠ æ‹¿å¤§Mila/Vector
        r"\bOxford\b|\bç‰›æ´¥å¤§å­¦\b",
        r"\bCambridge\b|\bå‰‘æ¡¥å¤§å­¦\b",
        r"\bETH Zurich\b|\bè‹é»ä¸–è”é‚¦ç†å·¥\b",
        r"\bEPFL\b|\bæ´›æ¡‘è”é‚¦ç†å·¥\b",
        r"\bZhejiang University\b|\bæµ™æ±Ÿå¤§å­¦\b",
        r"\bShanghai Jiao Tong\b|\bSJTU\b|\bä¸Šæµ·äº¤é€šå¤§å­¦\b",
        r"\bUSTC\b|\bä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦\b|\bç§‘å¤§\b",
        r"\bFudan\b|\bå¤æ—¦å¤§å­¦\b",

        # å…¶ä»–å¼ºåŠ›ç ”ç©¶æœºæ„ï¼ˆå¸¸ä¸å¤§å‚åˆä½œå‘é¡¶ä¼šè®ºæ–‡ï¼‰
        r"\bAllen Institute for AI\b|\bAI2\b",
        r"\bHugging Face\b",      # HFè‡ªå·±çš„è®ºæ–‡è´¨é‡å¾ˆé«˜ï¼Œå»ºè®®åŠ 
        r"\bTII\b|\bTechnology Innovation Institute\b",  # Falconæ¨¡å‹
    ]

    # å…´è¶£å…³é”®è¯ (å‘½ä¸­ +10 åˆ†)
    INTEREST_KEYWORDS = [
        r"LLM", r"Large Language Model", r"Reasoning", r"Chain of Thought",
        r"Agent", r"RAG", r"Retrieval", r"Efficient", r"Quantization",
        r"Post-training", r"RLHF", r"Alignment", r"World Model",
        r"Multimodal", r"Vision", r"Diffusion"
    ]

    def __init__(
        self,
        *,
        upvotes_weight: float = 1.0,
        stars_weight: float = 1.5,
        comments_weight: float = 0.5,
        super_lab_bonus: float = 50.0,
        lab_bonus: float = 20.0,
        topic_bonus: float = 10.0,
        enable_topic_bonus: bool = False
    ):
        """
        åˆå§‹åŒ–è¯„åˆ†å™¨

        Args:
            upvotes_weight: upvotes æƒé‡
            stars_weight: stars æƒé‡
            comments_weight: comments æƒé‡
            super_lab_bonus: è¶…çº§å®éªŒå®¤åŠ æˆ (OpenAI/Anthropic/DeepMind/DeepSeek)
            lab_bonus: æ™®é€š Frontier Lab åŠ æˆ
            topic_bonus: å…´è¶£åŒ¹é…åŠ æˆ
            enable_topic_bonus: æ˜¯å¦å¯ç”¨å…´è¶£åŠ æˆ
        """
        self.upvotes_weight = upvotes_weight
        self.stars_weight = stars_weight
        self.comments_weight = comments_weight
        self.super_lab_bonus = super_lab_bonus
        self.lab_bonus = lab_bonus
        self.topic_bonus = topic_bonus if enable_topic_bonus else 0

    def _check_regex_list(self, text: str, regex_list: List[str]) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ¹é…æ­£åˆ™åˆ—è¡¨ä¸­çš„ä»»ä¸€æ¨¡å¼"""
        if not text:
            return False
        for pattern in regex_list:
            if re.search(pattern, text, re.IGNORECASE):
                return True
        return False

    def _extract_fields(self, api_item: Dict[str, Any]) -> Dict[str, Any]:
        """ä» API å“åº”ä¸­æå–è¯„åˆ†æ‰€éœ€çš„å­—æ®µ"""
        paper = api_item.get("paper", {})

        return {
            "title": api_item.get("title", ""),
            "org": api_item.get("organization", {}).get("fullname", ""),
            "upvotes": paper.get("upvotes", 0),
            "stars": paper.get("githubStars", 0),
            "comments": api_item.get("numComments", 0),
            "github_repo": paper.get("githubRepo", ""),
            "arxiv_id": paper.get("id", ""),
            "summary": paper.get("summary", "") or api_item.get("summary", ""),
            "ai_keywords": paper.get("ai_keywords", []),
            "ai_summary": paper.get("ai_summary", ""),
        }

    def calculate_score(self, api_item: Dict[str, Any]) -> Dict[str, Any]:
        """
        è®¡ç®—å•ç¯‡è®ºæ–‡çš„å¾—åˆ†

        Args:
            api_item: HuggingFace API è¿”å›çš„å•ç¯‡è®ºæ–‡æ•°æ®

        Returns:
            åŒ…å« score, is_golden, reasons ç­‰å­—æ®µçš„å­—å…¸
        """
        fields = self._extract_fields(api_item)

        score = 0.0
        reasons = []

        # --- åŸºç¡€æ•°å€¼æŒ‡æ ‡ (Log å½’ä¸€åŒ–) ---
        upvotes = fields["upvotes"]
        stars = fields["stars"]
        comments = fields["comments"]

        # Upvotes
        u_score = math.log1p(upvotes) * 5 * self.upvotes_weight
        score += u_score

        # Stars
        s_score = math.log1p(stars) * 4 * self.stars_weight
        score += s_score

        # Comments
        c_score = math.log1p(comments) * 2 * self.comments_weight
        score += c_score

        # --- å®éªŒå®¤åŠ æˆ ---
        org = fields["org"]
        is_super = self._check_regex_list(org, self.SUPER_LABS)
        is_frontier = self._check_regex_list(org, self.FRONTIER_LABS)

        if is_super:
            score += self.super_lab_bonus
            reasons.append("Super Lab")
        elif is_frontier:
            score += self.lab_bonus
            reasons.append("Frontier Lab")

        # --- å…´è¶£åŒ¹é…åŠ æˆ ---
        if self.topic_bonus > 0:
            title_match = self._check_regex_list(fields["title"], self.INTEREST_KEYWORDS)
            summary_match = self._check_regex_list(fields["summary"], self.INTEREST_KEYWORDS)

            if title_match or summary_match:
                score += self.topic_bonus
                reasons.append("Relevant")

        # --- é‡‘ç‰Œç›´é€šè½¦è§„åˆ™ ---
        is_golden = False

        # çˆ†æ¬¾ç›´é€š
        if stars > 2000 or upvotes > 500:
            is_golden = True
            reasons.append("Viral")

        # è¶…çº§å®éªŒå®¤ + ç›¸å…³é¢†åŸŸ
        if is_super:
            title_match = self._check_regex_list(fields["title"], self.INTEREST_KEYWORDS)
            if title_match:
                is_golden = True
                reasons.append("Must Read")

        return {
            "score": round(score, 2),
            "is_golden": is_golden,
            "reasons": ", ".join(reasons),
            "fields": fields,
        }

    def rank_papers(self, api_items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        å¯¹è®ºæ–‡åˆ—è¡¨è¿›è¡Œè¯„åˆ†å’Œæ’åº

        Args:
            api_items: HuggingFace API è¿”å›çš„è®ºæ–‡åˆ—è¡¨

        Returns:
            è¯„åˆ†å¹¶æ’åºåçš„è®ºæ–‡åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«åŸæ•°æ® + score, is_golden, reasons
        """
        results = []

        for item in api_items:
            result = self.calculate_score(item)
            # ä¿ç•™åŸå§‹æ•°æ®
            item["rank_score"] = result["score"]
            item["is_golden"] = result["is_golden"]
            item["rank_reasons"] = result["reasons"]
            results.append(item)

        # æ’åº: Golden ä¼˜å…ˆï¼Œç„¶åæŒ‰åˆ†æ•°é™åº
        results.sort(key=lambda x: (x["is_golden"], x["rank_score"]), reverse=True)

        return results


# --- ä¾¿æ·å‡½æ•° ---

def fetch_and_rank(date: str, max_papers: int = 10, enable_topic_bonus: bool = False) -> List[Dict]:
    """
    è·å–æŒ‡å®šæ—¥æœŸçš„è®ºæ–‡å¹¶æ’å

    Args:
        date: æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)
        max_papers: æœ€å¤šè¿”å›è®ºæ–‡æ•°
        enable_topic_bonus: æ˜¯å¦å¯ç”¨å…´è¶£åŠ æˆ

    Returns:
        æ’ååçš„è®ºæ–‡åˆ—è¡¨
    """
    import requests

    url = f"https://huggingface.co/api/daily_papers?date={date}"
    resp = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})

    if resp.status_code != 200:
        raise Exception(f"API è¯·æ±‚å¤±è´¥: {resp.status_code}")

    papers = resp.json()

    if not papers:
        return []

    ranker = PaperRanker(enable_topic_bonus=enable_topic_bonus)
    ranked = ranker.rank_papers(papers)

    return ranked[:max_papers]


def print_ranking(ranked_papers: List[Dict], show_details: bool = True):
    """
    æ‰“å°æ’åç»“æœ

    Args:
        ranked_papers: æ’ååçš„è®ºæ–‡åˆ—è¡¨
        show_details: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    """
    print("=" * 100)
    print("è®ºæ–‡æ’å")
    print("=" * 100)

    for i, p in enumerate(ranked_papers, 1):
        golden = "ğŸ†" if p.get("is_golden") else "  "
        title = p.get("title", "")[:55]
        score = p.get("rank_score", 0)
        reasons = p.get("rank_reasons", "")

        print(f'{i:2d}. [{golden}] {score:6.2f} | {title}...')

        if show_details:
            paper = p.get("paper", {})
            org = p.get("organization", {}).get("fullname", "")
            upvotes = paper.get("upvotes", 0)
            stars = paper.get("githubStars", 0)
            comments = p.get("numComments", 0)

            print(f"     Org: {org} | upvotes:{upvotes:3d} stars:{stars:4d} comments:{comments:2d}")
            if reasons:
                print(f"     Tags: {reasons}")
            print()


if __name__ == "__main__":
    import argparse
    from datetime import date, timedelta

    parser = argparse.ArgumentParser(description="è·å–å¹¶æ’å HuggingFace æ¯æ—¥è®ºæ–‡")
    parser.add_argument("--date", default=None, help="æ—¥æœŸ (YYYY-MM-DD)ï¼Œé»˜è®¤æ˜¨å¤©")
    parser.add_argument("--max-papers", type=int, default=10, help="æœ€å¤šè¿”å›è®ºæ–‡æ•°")
    parser.add_argument("--topic-bonus", action="store_true", help="å¯ç”¨å…´è¶£åŠ æˆ")

    args = parser.parse_args()

    # é»˜è®¤æ˜¨å¤©
    target_date = args.date or (date.today() - timedelta(days=1)).strftime("%Y-%m-%d")

    print(f"è·å– {target_date} çš„è®ºæ–‡...")

    ranked = fetch_and_rank(target_date, max_papers=args.max_papers, enable_topic_bonus=args.topic_bonus)
    print_ranking(ranked)
