# DailyNews - Markdown è§£ææ¨¡å—
# å°†å¾®ä¿¡å…¬ä¼—å·æ–‡ç«  HTML è½¬æ¢ä¸º Markdown
import requests
from bs4 import BeautifulSoup
from markdownify import markdownify as md
import re
from pathlib import Path
import sys
from typing import Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))
import config

from .retry import retry_on_request_error

# æ²¿ç”¨é…ç½®ä¸­çš„ Headers
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Cookie": config.COOKIE
}


@retry_on_request_error(max_retries=3)
def parse_wechat_to_md(url: str) -> Optional[str]:
    """
    ä¸‹è½½å¾®ä¿¡æ–‡ç« å¹¶è½¬æ¢ä¸º Markdown

    Args:
        url: å¾®ä¿¡æ–‡ç« é“¾æ¥

    Returns:
        Markdown æ ¼å¼çš„æ–‡ç« å†…å®¹ï¼Œå¤±è´¥è¿”å› None
    """
    print(f"    ğŸ“¥ æ­£åœ¨ä¸‹è½½: {url}")
    try:
        resp = requests.get(url, headers=HEADERS, timeout=30)
        if resp.status_code != 200:
            print(f"    âŒ ä¸‹è½½å¤±è´¥: {resp.status_code}")
            return None

        # æ£€æŸ¥æ˜¯å¦è·å–åˆ°æœ‰æ•ˆå†…å®¹
        if 'js_content' not in resp.text:
            print(f"    âŒ Cookie å¯èƒ½å·²è¿‡æœŸæˆ–æ— æ•ˆ")
            return None

        soup = BeautifulSoup(resp.text, 'html.parser')

        # ================= æå–æ­£æ–‡ =================
        # å…ƒæ•°æ®ï¼ˆtitle, account, date_str, urlï¼‰ç”±è°ƒç”¨æ–¹ä» article dict æˆ– JSON è·å–

        content_div = soup.find('div', {'id': 'js_content'})
        if not content_div:
            content_div = soup.find('div', {'class': 'rich_media_content'})

        if not content_div:
            return None

        # ä¿®å¤å›¾ç‰‡ï¼šdata-src -> src
        for img in content_div.find_all('img'):
            if 'data-src' in img.attrs:
                img['src'] = img['data-src']
            if 'style' in img.attrs:
                del img['style']

        # ç§»é™¤é‡å¤çš„ logo å›¾ç‰‡
        from collections import Counter
        img_urls = [img.get('src') or img.get('data-src', '')
                    for img in content_div.find_all('img')
                    if img.get('src') or img.get('data-src')]
        url_counts = Counter(img_urls)

        for img in content_div.find_all('img'):
            img_url = img.get('src') or img.get('data-src', '')
            if img_url and url_counts.get(img_url, 0) > 2:
                img.decompose()

        # ç§»é™¤å¹²æ‰°æ ‡ç­¾
        for tag in content_div(['script', 'style', 'iframe']):
            tag.decompose()

        # å¤„ç†å¾®ä¿¡ä»£ç å—æ ¼å¼
        for pre in content_div.find_all('pre', class_=re.compile(r'code-snippet__\w+')):
            for code in pre.find_all('code', recursive=False):
                code.insert_before('\n')

        # è½¬ Markdown
        body_md = md(str(content_div), heading_style="ATX", strip=['a', 'span'])

        # ================= æ ¼å¼æ¸…ç† =================
        # æ¸…ç†å¤šä½™ç©ºè¡Œ
        body_md = re.sub(r'\n{3,}', '\n\n', body_md)

        # ç§»é™¤ç©ºæ ‡é¢˜
        body_md = re.sub(r'^###\s*\n', '', body_md, flags=re.MULTILINE)
        body_md = re.sub(r'^###\s*$', '', body_md, flags=re.MULTILINE)

        # ä¿®å¤å›¾ç‰‡è¢«é”™è¯¯åŒ…è£¹åŠ ç²—
        body_md = re.sub(r'\*\*(!\[.*?\]\([^)]+\))\*\*', r'\1', body_md)

        # ä¿®å¤åŒé‡åŠ ç²—
        body_md = re.sub(r'\*\*(.+?)\*\*\*\*(.+?)\*\*', r'**\1\2**', body_md)

        # æ¸…ç†è¿ç»­çš„åå¼•å·å—æ ‡è®°
        body_md = re.sub(r'```\s*```\n', '```\n', body_md)
        body_md = re.sub(r'```\s*```\s*```', '```', body_md)

        # ç§»é™¤ä»£ç å—å‘¨å›´å¤šä½™çš„åå¼•å·è¡Œ
        body_md = re.sub(r'\n```\n```\n', '\n```\n', body_md)

        # å†æ¬¡æ¸…ç†å¤šä½™ç©ºè¡Œ
        body_md = re.sub(r'\n{3,}', '\n\n', body_md)

        # å…ƒæ•°æ®ï¼ˆtitle, account, date_str, urlï¼‰ç”±è°ƒç”¨æ–¹ä» article dict æˆ– JSON è·å–
        # æ­¤å¤„åªè¿”å›æ–‡ç« æ­£æ–‡å†…å®¹ï¼Œé¿å…ä¸ JSON metadata å’Œ LLM prompt é‡å¤
        return body_md

    except Exception as e:
        print(f"    âŒ è§£æå¼‚å¸¸: {e}")
        return None


# æµ‹è¯•å…¥å£
if __name__ == "__main__":
    # æµ‹è¯•å•ç¯‡æ–‡ç« è½¬æ¢
    test_url = "https://mp.weixin.qq.com/s/acMM1zgxUmzlrFk6O7p7iw"
    result = parse_wechat_to_md(test_url)

    if result:
        print(result[:2000])
        print("\nâœ… è½¬æ¢æˆåŠŸï¼")
