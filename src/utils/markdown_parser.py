# DailyNews - Markdown è§£ææ¨¡å—
# å°†å¾®ä¿¡å…¬ä¼—å·æ–‡ç«  HTML è½¬æ¢ä¸º Markdown
import requests
from bs4 import BeautifulSoup
from markdownify import markdownify as md
import re
from pathlib import Path
import sys
from typing import Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))
import config

from .retry import retry_on_request_error


class WeChatContentError(requests.exceptions.RequestException):
    """å¾®ä¿¡æ–‡ç« å†…å®¹è·å–å¤±è´¥å¼‚å¸¸ï¼ˆè§¦å‘é‡è¯•ï¼‰"""
    pass

# æ²¿ç”¨é…ç½®ä¸­çš„ Headers (lazy load)
def _get_headers():
    """Get headers with cookie, only loaded when needed."""
    return {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Cookie": config.get_cookie()
    }

# For backwards compatibility
HEADERS = None


@retry_on_request_error(max_retries=3, delay=2.0, backoff=2.0)
def parse_wechat_to_md(url: str) -> Optional[str]:
    """
    ä¸‹è½½å¾®ä¿¡æ–‡ç« å¹¶è½¬æ¢ä¸º Markdown

    Args:
        url: å¾®ä¿¡æ–‡ç« é“¾æ¥

    Returns:
        Markdown æ ¼å¼çš„æ–‡ç« å†…å®¹ï¼Œå¤±è´¥è¿”å› None
    """
    print(f"    ğŸ“¥ æ­£åœ¨ä¸‹è½½: {url}")
    try:
        resp = requests.get(url, headers=_get_headers(), timeout=30)
        if resp.status_code != 200:
            raise WeChatContentError(f"HTTP {resp.status_code}")

        # æ£€æŸ¥æ˜¯å¦è·å–åˆ°æœ‰æ•ˆå†…å®¹
        if 'js_content' not in resp.text:
            # æ£€æŸ¥æ˜¯å¦æ˜¯é€Ÿç‡é™åˆ¶ï¼ˆéœ€è¦é‡è¯•ï¼‰
            rate_limit_indicators = ['è®¿é—®è¿‡äºé¢‘ç¹', 'è¯·åœ¨å¾®ä¿¡å®¢æˆ·ç«¯', 'anti-spider', 'antispider']
            is_rate_limit = any(indicator in resp.text for indicator in rate_limit_indicators)

            if is_rate_limit:
                raise WeChatContentError("å¯èƒ½é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œç¨åé‡è¯•")
            else:
                # å†…å®¹ç¡®å®ä¸å­˜åœ¨ï¼ˆæ–‡ç« åˆ é™¤/è¿è§„ç­‰ï¼‰ï¼Œä¸é‡è¯•
                print(f"      âš ï¸ æ–‡ç« å†…å®¹ä¸å¯ç”¨ï¼ˆå¯èƒ½å·²åˆ é™¤æˆ–éœ€ç‰¹æ®Šæƒé™ï¼‰")
                return None

        soup = BeautifulSoup(resp.text, 'html.parser')

        # ================= æå–æ­£æ–‡ =================
        # å…ƒæ•°æ®ï¼ˆtitle, account, date_str, urlï¼‰ç”±è°ƒç”¨æ–¹ä» article dict æˆ– JSON è·å–

        content_div = soup.find('div', {'id': 'js_content'})
        if not content_div:
            content_div = soup.find('div', {'class': 'rich_media_content'})

        if not content_div:
            raise WeChatContentError("æœªæ‰¾åˆ°æ–‡ç« å†…å®¹åŒºåŸŸ")

        # ä¿®å¤å›¾ç‰‡ï¼šdata-src -> src
        for img in content_div.find_all('img'):
            if 'data-src' in img.attrs:
                img['src'] = img['data-src']
            if 'style' in img.attrs:
                del img['style']

        # ç§»é™¤é‡å¤çš„ logo å›¾ç‰‡
        from collections import Counter
        all_imgs = content_div.find_all('img')

        # å…ˆæ”¶é›†æ‰€æœ‰å›¾ç‰‡ URLï¼ˆåœ¨ä¿®æ”¹ DOM ä¹‹å‰ï¼‰
        img_urls = []
        for img in all_imgs:
            if img.attrs:  # æ£€æŸ¥ attrs ä¸æ˜¯ None
                src = img.get('src') or img.get('data-src', '')
                if src:
                    img_urls.append(src)

        url_counts = Counter(img_urls)

        # å…ˆæ”¶é›†è¦åˆ é™¤çš„å›¾ç‰‡å…ƒç´ ï¼ˆé¿å…è¿­ä»£æ—¶ä¿®æ”¹ DOMï¼‰
        imgs_to_remove = []
        for img in all_imgs:
            if img.attrs:  # æ£€æŸ¥ attrs ä¸æ˜¯ None
                img_url = img.get('src') or img.get('data-src', '')
                if img_url and url_counts.get(img_url, 0) > 2:
                    imgs_to_remove.append(img)

        # ç»Ÿä¸€åˆ é™¤
        for img in imgs_to_remove:
            img.decompose()

        # ç§»é™¤å¹²æ‰°æ ‡ç­¾
        for tag in content_div(['script', 'style', 'iframe']):
            tag.decompose()

        # å¤„ç†å¾®ä¿¡ä»£ç å—æ ¼å¼
        for pre in content_div.find_all('pre', class_=re.compile(r'code-snippet__\w+')):
            for code in pre.find_all('code', recursive=False):
                code.insert_before('\n')

        # è½¬ Markdown
        body_md = md(str(content_div), heading_style="ATX", strip=['a', 'span'])

        # ================= æ ¼å¼æ¸…ç† =================
        # æ¸…ç†å¤šä½™ç©ºè¡Œ
        body_md = re.sub(r'\n{3,}', '\n\n', body_md)

        # ç§»é™¤ç©ºæ ‡é¢˜
        body_md = re.sub(r'^###\s*\n', '', body_md, flags=re.MULTILINE)
        body_md = re.sub(r'^###\s*$', '', body_md, flags=re.MULTILINE)

        # ä¿®å¤å›¾ç‰‡è¢«é”™è¯¯åŒ…è£¹åŠ ç²—
        body_md = re.sub(r'\*\*(!\[.*?\]\([^)]+\))\*\*', r'\1', body_md)

        # ä¿®å¤åŒé‡åŠ ç²—
        body_md = re.sub(r'\*\*(.+?)\*\*\*\*(.+?)\*\*', r'**\1\2**', body_md)

        # æ¸…ç†è¿ç»­çš„åå¼•å·å—æ ‡è®°
        body_md = re.sub(r'```\s*```\n', '```\n', body_md)
        body_md = re.sub(r'```\s*```\s*```', '```', body_md)

        # ç§»é™¤ä»£ç å—å‘¨å›´å¤šä½™çš„åå¼•å·è¡Œ
        body_md = re.sub(r'\n```\n```\n', '\n```\n', body_md)

        # å†æ¬¡æ¸…ç†å¤šä½™ç©ºè¡Œ
        body_md = re.sub(r'\n{3,}', '\n\n', body_md)

        # å…ƒæ•°æ®ï¼ˆtitle, account, date_str, urlï¼‰ç”±è°ƒç”¨æ–¹ä» article dict æˆ– JSON è·å–
        # æ­¤å¤„åªè¿”å›æ–‡ç« æ­£æ–‡å†…å®¹ï¼Œé¿å…ä¸ JSON metadata å’Œ LLM prompt é‡å¤
        return body_md

    except Exception as e:
        print(f"    âŒ è§£æå¼‚å¸¸: {e}")
        return None


# æµ‹è¯•å…¥å£
if __name__ == "__main__":
    # æµ‹è¯•å•ç¯‡æ–‡ç« è½¬æ¢
    test_url = "https://mp.weixin.qq.com/s/acMM1zgxUmzlrFk6O7p7iw"
    result = parse_wechat_to_md(test_url)

    if result:
        print(result[:2000])
        print("\nâœ… è½¬æ¢æˆåŠŸï¼")
