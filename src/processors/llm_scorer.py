# LLM Batch Processor
# Batch scoring, deduplication, and ad filtering using LLM
import json
import re
from typing import List, Dict, Any


class LLMBatchProcessor:
    """LLM æ‰¹é‡æ‰“åˆ†ã€å»é‡ã€å¹¿å‘Šè¿‡æ»¤"""

    MAX_INPUT_TOKENS = 100_000
    TARGET_BATCH_SIZE = 30

    def __init__(self, client):
        """
        Initialize the batch processor.

        Args:
            client: LLM client instance (e.g., GeminiClient)
        """
        from prompts.scoring import ScoringPrompt
        self.client = client
        self.prompt = ScoringPrompt()

    def process(self, articles: List[Dict]) -> List[Dict]:
        """
        æ‰¹é‡å¤„ç†æ–‡ç« 

        Args:
            articles: List of article dictionaries with title, summary, tags, etc.

        Returns:
            Processed list with scores, deduplication info, ads filtered
        """
        if not articles:
            return []

        print(f"  ğŸ¤– LLM æ‰¹é‡å¤„ç†: {len(articles)} ç¯‡æ–‡ç« ")

        # åˆ†æ‰¹å¤„ç†
        batches = self._split_into_batches(articles)
        all_processed = []
        total_ads = 0
        total_duplicates = 0

        for i, batch in enumerate(batches, 1):
            print(f"    [{i}/{len(batches)}] å¤„ç†æ‰¹æ¬¡: {len(batch)} ç¯‡...")
            result = self._process_batch(batch)
            all_processed.extend(result['articles'])

            # ç»Ÿè®¡
            total_ads += result.get('removed_ads', 0)
            total_duplicates += result.get('duplicate_groups', 0)

        # è·¨æ‰¹æ¬¡å»é‡ï¼ˆå¦‚æœæœ‰å¤šæ‰¹æ¬¡ï¼‰
        if len(batches) > 1:
            print(f"    ğŸ”— è·¨æ‰¹æ¬¡å»é‡...")
            all_processed = self._cross_batch_deduplicate(all_processed)

        print(f"  âœ… LLM å¤„ç†å®Œæˆ: ä¿ç•™ {len(all_processed)}/{len(articles)} ç¯‡")
        if total_ads > 0:
            print(f"     ğŸš« è¿‡æ»¤å¹¿å‘Š: {total_ads} ç¯‡")
        if total_duplicates > 0:
            print(f"     ğŸ”„ å»é‡åˆ†ç»„: {total_duplicates} ç»„")

        return all_processed

    def _split_into_batches(self, articles: List[Dict]) -> List[List[Dict]]:
        """
        æŒ‰ token é™åˆ¶åˆ†æ‰¹

        Args:
            articles: List of articles to split

        Returns:
            List of batches
        """
        batch_size = self.TARGET_BATCH_SIZE
        batches = []

        for i in range(0, len(articles), batch_size):
            batch = articles[i:i + batch_size]
            if batch:
                batches.append(batch)

        return batches

    def _process_batch(self, batch: List[Dict]) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªæ‰¹æ¬¡

        Args:
            batch: List of articles in this batch

        Returns:
            Result dict with articles, removed_ads, duplicate_groups
        """
        prompt = self.prompt.format_prompt_with_articles(batch)
        response = self.client.generate_content(prompt)

        try:
            parsed = self._parse_response(response.text, batch)
            return parsed
        except Exception as e:
            print(f"      âš ï¸ LLM è§£æå¤±è´¥: {e}ï¼Œä½¿ç”¨åŸæ•°æ®")
            # é™çº§ï¼šè¿”å›æ‰€æœ‰æ–‡ç« ï¼Œæ ‡è®°ä¸ºæœªå¤„ç†
            return {
                'articles': batch,
                'removed_ads': 0,
                'duplicate_groups': 0
            }

    def _parse_response(self, response_text: str, original: List[Dict]) -> Dict[str, Any]:
        """
        è§£æ LLM å“åº”

        Args:
            response_text: Raw LLM response
            original: Original articles list (for reference by id)

        Returns:
            Parsed result dict
        """
        result = self._extract_json_from_response(response_text)

        processed = []
        removed_ads = 0
        duplicate_groups = 0
        seen_groups = set()

        articles_data = result.get('articles', [])

        for item in articles_data:
            article_id = item.get('id')
            if article_id is None or article_id >= len(original):
                continue

            is_ad = item.get('is_ad', False)
            keep = item.get('keep', True)
            duplicate_group = item.get('duplicate_group')

            # ç»Ÿè®¡
            if is_ad:
                removed_ads += 1
            if duplicate_group is not None and duplicate_group not in seen_groups:
                seen_groups.add(duplicate_group)
                duplicate_groups += 1

            # è·³è¿‡å¹¿å‘Šå’Œè¢«è¿‡æ»¤çš„æ–‡ç« 
            if not keep:
                continue

            # æ›´æ–°æ–‡ç« æ•°æ®
            article = original[article_id].copy()
            article.update({
                'score': item.get('score', 3),
                'duplicate_group': duplicate_group,
                'keep_reason': item.get('keep_reason', ''),
                'llm_processed': True
            })
            processed.append(article)

        return {
            'articles': processed,
            'removed_ads': removed_ads,
            'duplicate_groups': duplicate_groups
        }

    def _extract_json_from_response(self, text: str) -> Dict:
        """
        Extract JSON from LLM response that may contain extra text.

        Args:
            text: Raw LLM response

        Returns:
            Parsed JSON dictionary
        """
        # Try direct parse first
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            pass

        # Try to extract JSON from markdown code blocks
        # Look for ```json ... ```
        json_match = re.search(r'```json\s*(.*?)\s*```', text, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except json.JSONDecodeError:
                pass

        # Look for ``` ... ```
        json_match = re.search(r'```\s*(.*?)\s*```', text, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except json.JSONDecodeError:
                pass

        # Look for { ... }
        json_match = re.search(r'\{.*\}', text, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(0))
            except json.JSONDecodeError:
                pass

        raise ValueError(f"æ— æ³•ä»å“åº”ä¸­æå–æœ‰æ•ˆ JSON: {text[:200]}...")

    def _cross_batch_deduplicate(self, articles: List[Dict]) -> List[Dict]:
        """
        è·¨æ‰¹æ¬¡å»é‡ï¼šåŸºäº duplicate_group å’Œæ ‡é¢˜ç›¸ä¼¼åº¦

        Args:
            articles: List of processed articles from multiple batches

        Returns:
            Deduplicated list
        """
        if not articles:
            return articles

        # æŒ‰ duplicate_group åˆ†ç»„
        groups = {}
        ungrouped = []

        for article in articles:
            group_id = article.get('duplicate_group')
            if group_id is not None:
                if group_id not in groups:
                    groups[group_id] = []
                groups[group_id].append(article)
            else:
                ungrouped.append(article)

        # æ¯ç»„åªä¿ç•™æœ€é«˜åˆ†çš„
        result = []
        for group_id, group_articles in groups.items():
            # æŒ‰åˆ†æ•°é™åºæ’åº
            sorted_articles = sorted(
                group_articles,
                key=lambda x: x.get('score', 0),
                reverse=True
            )
            # ä¿ç•™æœ€é«˜åˆ†çš„
            best = sorted_articles[0]
            best['keep_reason'] = f"ç»„{group_id}æœ€ä¼˜ï¼ˆåˆ†æ•°{best['score']}ï¼‰"
            result.append(best)

        # æ·»åŠ æœªåˆ†ç»„çš„
        result.extend(ungrouped)

        return result
