#!/usr/bin/env python3
"""
å‘å¸ƒ GitHub Trending åˆ°å¾®ä¿¡å…¬ä¼—å·è‰ç¨¿ç®±
"""
import requests
import json
import re
import sys
from pathlib import Path
from datetime import datetime

sys.path.insert(0, str(Path(__file__).parent.parent))
import config


class GitHubTrendingPublisher:
    """GitHub Trending å‘å¸ƒå™¨"""

    def __init__(self):
        self.app_id = config.APP_ID
        self.app_secret = config.APP_SECRET
        self.token = self._get_access_token()

    def _get_access_token(self):
        """è·å– access_token"""
        url = f"https://api.weixin.qq.com/cgi-bin/token?grant_type=client_credential&appid={self.app_id}&secret={self.app_secret}"
        resp = requests.get(url, proxies=getattr(config, 'PROXIES', None)).json()
        if 'access_token' in resp:
            return resp['access_token']
        else:
            raise Exception(f"è·å– access_token å¤±è´¥: {resp}")

    def _parse_github_trending(self, report_path):
        """è§£æ github_trending.md æ–‡ä»¶"""
        with open(report_path, "r", encoding="utf-8") as f:
            content = f.read()

        items = []

        # æŒ‰æ–‡ç« åˆ†å‰² (æ¯ç¯‡ä»¥ ### å¼€å¤´)
        articles = re.split(r'\n###\s+(\d+)\.\s+', content)

        for i in range(1, len(articles), 2):
            if i + 1 >= len(articles):
                break

            idx = articles[i]
            article = articles[i + 1]

            if not article.strip():
                continue

            # æå–é¡¹ç›®å (ç¬¬ä¸€è¡Œ)
            title_match = re.search(r'^([^\n]+)', article)
            title = title_match.group(1).strip() if title_match else f"é¡¹ç›® {idx}"

            # æå–è¯­è¨€ã€starsã€ä»Šæ—¥
            lang_match = re.search(r'\*\*è¯­è¨€\*\*: ([^\n|]+)', article)
            language = lang_match.group(1).strip() if lang_match else "æœªçŸ¥"

            stars_match = re.search(r'\*\*Stars\*\*: ([\d,]+)', article)
            stars = stars_match.group(1).strip() if stars_match else ""

            today_match = re.search(r'\*\*ä»Šæ—¥\*\*: \+([\d,]+)', article)
            today_stars = today_match.group(1).strip() if today_match else ""

            # æå–é“¾æ¥
            url_match = re.search(r'\*\*é“¾æ¥\*\*: (.+?)\n', article)
            url = url_match.group(1).strip() if url_match else ""

            # æå–æ‘˜è¦
            summary_match = re.search(r'\*\*æ‘˜è¦\*\*: (.+?)(?:\n---|\n\n###|\Z)', article, re.DOTALL)
            summary = summary_match.group(1).strip() if summary_match else ""
            summary = re.sub(r'<br>', '\n', summary)

            if title:
                items.append({
                    'title': title,
                    'language': language,
                    'stars': stars,
                    'today_stars': today_stars,
                    'url': url,
                    'summary': summary
                })

        return items

    def generate_html(self, items):
        """ç”Ÿæˆ HTML å†…å®¹"""
        html_parts = ['<section style="font-family: -apple-system, BlinkMacSystemFont, Arial, sans-serif;">']
        html_parts.append('<section style="margin-top: 20px;"></section>')

        for idx, item in enumerate(items, 1):
            # æ ‡é¢˜
            title_html = f'<h3 style="margin-top: 30px; margin-bottom: 5px; font-size: 18px; font-weight: bold; color: #000;">{idx}. {item["title"]}</h3>'

            # å…ƒä¿¡æ¯
            meta_parts = []
            if item.get('language'):
                meta_parts.append(f'ğŸ’» è¯­è¨€ï¼š{item["language"]}')
            if item.get('stars'):
                meta_parts.append(f'â­ Starsï¼š{item["stars"]}')
            if item.get('today_stars'):
                meta_parts.append(f'ğŸ”¥ ä»Šæ—¥ï¼š+{item["today_stars"]}')

            meta_first_line = 'ã€€'.join(meta_parts)

            meta_html = f'<div style="font-size: 13px; color: #888; margin-bottom: 10px; background: #f9f9f9; padding: 8px; border-radius: 4px;">{meta_first_line}<br><span style="display: inline-block; margin-top: 4px; color: #576b95; word-break: break-all;">ğŸ”— é“¾æ¥ï¼š{item["url"]}</span></div>'

            summary_text = item["summary"].replace("\n", "<br>")
            summary_html = f'<p style="font-size: 16px; color: #333; line-height: 1.6; text-align: justify; margin-bottom: 25px;">{summary_text}</p>'

            divider = '<hr style="border: 0; border-top: 1px dashed #ddd; margin: 20px 0;" />' if idx < len(items) else ""
            html_parts.append(title_html + meta_html + summary_html + divider)

        html_parts.append("</section>")
        return "".join(html_parts)

    def publish_to_draft(self, report_path, title=None):
        """å‘å¸ƒåˆ°è‰ç¨¿ç®±"""
        items = self._parse_github_trending(report_path)

        if not items:
            raise Exception("âŒ æŠ¥å‘Šä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•é¡¹ç›®")

        print(f"ğŸ“Š è§£æåˆ° {len(items)} ä¸ªé¡¹ç›®")

        # ç”Ÿæˆæ ‡é¢˜
        if not title:
            title = f"GitHub çƒ­é—¨é¡¹ç›® | {datetime.now().strftime('%Y-%m-%d')}"

        # ç”Ÿæˆ HTML
        content_html = self.generate_html(items)

        # åˆ›å»ºè‰ç¨¿
        draft_id = self._create_draft(title, content_html, config.COVER_MEDIA_ID)

        return draft_id

    def _create_draft(self, title, content, thumb_id):
        """åˆ›å»ºè‰ç¨¿"""
        url = f"https://api.weixin.qq.com/cgi-bin/draft/add?access_token={self.token}"
        data = {
            "articles": [{
                "title": title,
                "author": "GitHub Report",
                "digest": "ä»Šæ—¥ GitHub çƒ­é—¨é¡¹ç›®ç²¾é€‰...",
                "content": content,
                "thumb_media_id": thumb_id
            }]
        }

        resp = requests.post(
            url,
            data=json.dumps(data, ensure_ascii=False).encode('utf-8'),
            headers={'Content-Type': 'application/json; charset=utf-8'},
            proxies=getattr(config, 'PROXIES', None)
        )

        result = resp.json()

        if 'media_id' not in result:
            raise Exception(f"âŒ è‰ç¨¿åˆ›å»ºå¤±è´¥: {result}")

        return result['media_id']


def main():
    import argparse

    parser = argparse.ArgumentParser(description="å°† GitHub Trending å‘å¸ƒåˆ°å¾®ä¿¡å…¬ä¼—å·è‰ç¨¿ç®±")
    parser.add_argument("--date", default=None, help="æ—¥æœŸ (YYYY-MM-DD)ï¼Œé»˜è®¤ä»Šå¤©")
    args = parser.parse_args()

    # ç¡®å®šæ—¥æœŸ
    target_date = args.date or datetime.now().strftime("%Y-%m-%d")

    # æŠ¥å‘Šè·¯å¾„
    report_path = Path(__file__).parent.parent / "output" / target_date / "github_trending.md"

    if not report_path.exists():
        print(f"âŒ æŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨: {report_path}")
        return

    print("=" * 50)
    print(f"ğŸ“¤ æ­£åœ¨å‘å¸ƒ GitHub Trending åˆ°è‰ç¨¿ç®±")
    print(f"ğŸ“„ æŠ¥å‘Šæ–‡ä»¶: {report_path}")
    print("=" * 50)

    try:
        publisher = GitHubTrendingPublisher()
        draft_id = publisher.publish_to_draft(report_path)

        print(f"\nâœ… è‰ç¨¿åˆ›å»ºæˆåŠŸï¼")
        print(f"ğŸ“‹ Media ID: {draft_id}")
        print(f"\nğŸ‘‰ è¯·ç™»å½•å¾®ä¿¡å…¬ä¼—å·åå°æŸ¥çœ‹è‰ç¨¿ç®±")
        print("=" * 50)

    except Exception as e:
        print(f"\nâŒ å‘å¸ƒå¤±è´¥: {e}")


if __name__ == "__main__":
    main()
