# 每日论文汇总 - 2026-02-02

**论文数量**: 7

---

###  1. DreamActor-M2：通过时空上下文学习实现通用角色图像动画生成 (DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning)

**论文链接**: [https://arxiv.org/abs/2601.21716](https://arxiv.org/abs/2601.21716)
**组织**: ByteDance
**得分**: 60.42
**标签**: Super Lab
**Upvotes**: 6 | **Stars**: 0

**摘要**: 针对现有角色动画中身份保持与动作一致性难以兼顾以及过度依赖人体骨架先验的问题，字节跳动提出了 DreamActor-M2 框架。该研究将动作驱动建模为时空上下文学习问题，通过将外观与动作线索融合至统一潜空间，利用基础模型的生成先验实现联合推理。同时，引入自引导数据合成流水线以实现端到端的 RGB 驱动动画，显著增强了对非人型角色及复杂场景的泛化能力。实验证明该方法在视觉保真度和跨领域迁移上均达到了当前领先水平。

**亮点**:
  - 将动作建模重构为上下文学习，摆脱了对骨架等显式姿态先验的依赖，极大提升了对非人型角色的泛化性。
  - 引入自引导数据合成流水线和 AW Bench 评测集，解决了身份保持与动作一致性之间的平衡难题。

---

###  2. ASTRA：自主智能体轨迹与强化学习竞技场的自动化合成 (ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas)

**论文链接**: [https://arxiv.org/abs/2601.21558](https://arxiv.org/abs/2601.21558)
**组织**: Unknown
**得分**: 45.91
**标签**: 
**Upvotes**: 39 | **Stars**: 80

**摘要**: ASTRA 是一个用于训练工具增强型语言模型智能体的全自动端到端框架，旨在解决现有方法依赖人工干预和不可验证环境的问题。该框架通过结合基于工具调用图的轨迹合成技术与可验证的环境合成技术，实现了大规模数据生成与确定性的多轮强化学习。研究提出了一套统一的训练方法，将有监督微调（SFT）与在线强化学习（RL）相结合，在多个智能体基准测试中取得了 SOTA 性能，并接近闭源系统水平。

**亮点**:
  - 提出了全自动化的数据合成与可验证环境构建流水线，无需人工干预即可生成高质量训练数据。
  - 利用工具调用图的静态拓扑结构合成轨迹，增强了模型在复杂场景下的工具使用和迁移能力。
  - 实现了 SFT 与在线 RL 的统一训练架构，通过轨迹级奖励有效平衡了任务完成度与交互效率。

---

###  3. PaperBanana：面向 AI 科学家的学术插图自动生成系统 (PaperBanana: Automating Academic Illustration for AI Scientists)

**论文链接**: [https://arxiv.org/abs/2601.23265](https://arxiv.org/abs/2601.23265)
**组织**: Google
**得分**: 33.52
**标签**: Frontier Lab
**Upvotes**: 11 | **Stars**: 0

**摘要**: 本研究针对 AI 自动化科研流程中高质量学术插图制作耗时的问题，提出了 PaperBanana 代理框架。该框架结合视觉语言模型（VLM）与图像生成技术，通过多代理协作实现从参考检索、内容规划到迭代自我优化生成的过程。在基于 NeurIPS 2025 论文构建的 PaperBananaBench 基准测试中，该方法在准确度、可读性和审美等方面均显著优于现有基准，为学术插图的自动化生成开辟了新路径。

**亮点**:
  - 提出了首个专门用于自动生成出版级学术插图的 Agentic 代理框架
  - 构建了包含 292 个真实科研场景测试用例的基准数据集 PaperBananaBench
  - 通过多代理协作与自我批判机制，实现了从逻辑示意图到统计图表的高质量生成

---

###  4. TTCS：面向自我进化的测试时课程合成 (TTCS: Test-Time Curriculum Synthesis for Self-Evolving)

**论文链接**: [https://arxiv.org/abs/2601.22628](https://arxiv.org/abs/2601.22628)
**组织**: Unknown
**得分**: 30.59
**标签**: 
**Upvotes**: 22 | **Stars**: 9

**摘要**: 本文提出了 TTCS 框架，通过在测试阶段协同演化“题目合成器”和“推理求解器”，解决了大语言模型在复杂推理任务中因样本稀缺和难度跨度大导致的训练不稳定问题。该方法利用合成器生成与求解器当前能力匹配的渐进式课程题目，并通过自我一致性奖励进行模型更新。实验证明，TTCS 在多个数学和通用领域基准测试中显著增强了模型的推理能力，展示了动态构建测试时课程的有效性。

**亮点**:
  - 提出协同演化框架（TTCS），通过合成变体题目实现动态课程学习
  - 利用自我一致性反馈闭环，确保证合成题目难度与模型当前能力对齐
  - 显著提升了 LLM 在具有挑战性的数学推理任务上的表现及跨领域泛化能力

---

###  5. Best-of-N 采样下大语言模型对抗风险的统计评估 (Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling)

**论文链接**: [https://arxiv.org/abs/2601.22636](https://arxiv.org/abs/2601.22636)
**组织**: Microsoft
**得分**: 29.65
**标签**: Frontier Lab
**Upvotes**: 5 | **Stars**: 0

**摘要**: 本文针对传统评估方法低估 LLM 对抗性风险的问题，提出了 SABER 评估框架，旨在建模大规模并行采样（Best-of-N）下的越狱脆弱性。该方法利用 Beta 分布对样本级成功率进行建模，并推导出一种分析型缩放定律，能够从少量样本（如 n=100）中准确外推大规模采样下的攻击成功率。研究发现，该方法将 ASR@1000 的预测误差降低了 86.2%，揭示了看似稳健的模型在并行压力下可能出现剧烈的非线性风险放大。

**亮点**:
  - 提出 SABER 框架，实现了从低预算采样到大规模对抗风险的可靠外推
  - 利用 Beta 分布和缩放定律将 ASR 估计误差大幅降低 86.2%
  - 揭示了模型在并行采样攻击下风险非线性扩大的异质性特征

---

###  6. 金鹅 (Golden Goose)：从不可验证的互联网文本中合成无限 RLVR 任务的简单技巧 (Golden Goose: A Simple Trick to Synthesize Unlimited RLVR Tasks from Unverifiable Internet Text)

**论文链接**: [https://arxiv.org/abs/2601.22975](https://arxiv.org/abs/2601.22975)
**组织**: NVIDIA
**得分**: 27.62
**标签**: Frontier Lab
**Upvotes**: 3 | **Stars**: 0

**摘要**: 本文针对强化学习中可验证奖励 (RLVR) 数据短缺的问题，提出了名为 Golden Goose 的简单技巧，旨在从不可验证的互联网文本中合成无限的推理任务。该方法通过将文本转换为多选题形式的“填空”任务，利用 LLM 掩盖关键推理步骤并生成干扰项，从而使科学教科书等丰富语料库可用于 RLVR。实验证明，合成的 GooseReason-0.7M 数据集能有效提升模型性能，使 1.5B 和 4B 模型在 15 个基准测试中达到 SOTA，并在网络安全等缺乏标注数据的领域展现出强大的跨领域泛化能力。

**亮点**:
  - 提出一种将非结构化、不可验证文本转化为可验证 RLVR 任务的通用框架。
  - 构建了包含 70 万个任务的大规模推理数据集 GooseReason-0.7M，涵盖数学、编程及科学领域。
  - 在网络安全领域证明了该方法的实战价值，训练出的 4B 模型超越了 7B 规模的领域专用模型。

---

###  7. 持续学习型 GUI 智能体 (Continual GUI Agents)

**论文链接**: [https://arxiv.org/abs/2601.20732](https://arxiv.org/abs/2601.20732)
**组织**: Tsinghua University
**得分**: 27.62
**标签**: Frontier Lab
**Upvotes**: 3 | **Stars**: 0

**摘要**: 本研究提出了“持续学习型 GUI 智能体”这一新任务，旨在解决 GUI 环境在领域和分辨率动态变化时，静态训练模型性能下降的问题。研究发现现有方法在分布偏移时难以维持稳定的定位能力，为此提出了 GUI-AiF 强化微调框架。该框架通过引入锚点奖励（APR-iF）和区域奖励（ARR-iF），引导智能体在动态环境中精准对齐交互点与区域，有效缓解了对静态坐标或比例的过度拟合。实验证明 GUI-AiF 在多项基准上优于现有 SOTA 模型，是首个针对 GUI 智能体的持续学习框架。

**亮点**:
  - 定义了 GUI 智能体在动态领域和分辨率下的持续学习（Continual Learning）新任务
  - 提出 GUI-AiF 强化微调框架，利用双重锚定奖励机制解决定位漂移问题
  - 填补了 GUI 智能体在动态环境下长效性能维护的技术空白

---
